这辈子所有的任务清单都会写在这个文件里面。写多了就分库分表，yes！

今日任务   2024-06-26 更新于 2024-06-27/00:30
  1。mysql:看5篇文章，还要复习mybatis里是如何与mysql交互的。                                          没有搞定，要多实践，多调优。
  2.redis:复习各种数据结构，先学这个。                                                              搞定，还要多复习几遍
  3.java:继续复习八股，练习注解的使用，一定要练熟。练透                                                   完成
          Java里juc并发编程，学透AQS部分，可以自己写一个AQS同步器，弄清楚thread源码                  先看aqs同步器 ，看懂aqs同步器。没有看Thread的源码，按照训练营来。       
          JAVA里的设计模式，学习两个设计模式，尤其是单例模式                                        学了单例模式，工厂模式，抽象工厂模式 
          JAVA集合：搞清楚list，treeset                                                                          没有做
  4.算法：2道算法题，复习一道LRU，还要联系多线程编程，看Thread源码                                  刷了三道题，看了一道单调栈的题。
  5.操作系统：先学前边三个。                        
  6.计算机网络：没想好，先学前边三个，主要是要弄会三个网络模型，

其实今天，再来十几个小时，还是可以学完的。所以要早点休息，早点起床。


今日任务 
  1.mysql: 重点了锁部分的文章。继续看3~5篇文章。     今天就看五篇吧，尤其是要弄明白explain,还有mysql背后的原理，高可用这些不是我们该管的 没怎么看，看了子查询部分，复习了查询方法和explain
  2.redis: 昨天看了数据结构，今天接着看                         今天接着看一些命令，底层原理
  3.java: 并发编程看完了aqs，解这看别的。看3篇文章.   搞明白了threadlocal，map的并发编程。别的啥也没搞懂，下午再看看map,和其他容器；哦对还看了condition对象，LockSupport.HashMap源码没有太看明白，得接着看，要看明白的源码就是java集合，java并发编程。这是7月份必须完全看明白的源码
          设计模式：学习两个设计模式                            
          java基础：继续看javaguide的八股文。             学习了java数组，java集合等         
          集合：理解List集合的源码，写成文章。
          java io好好学。理解nio；           今天必须好好理解java io;完全理解透彻。今天的重点，今天算是理解了nio,bio，以及常用的bio包装类。
  4.算法：刷两道算法，复习以前的算法题         连续的子数组和  刷了两道。自己ac了两道，还有两道是抄的，其中一道太简单了；
  5.操作系统：主要要看io.这个对理解很重要。    
  6.计算机网络：                                  看看tcp/ip 一篇就行。


今日任务
  1.mysql:继续复习锁这一章节。搞清楚explain。         
  2.redis:看文章                                    看了哨兵机制，主从机制。
  3.java:
        java基础：复习知识点，                            
        jvm:虚拟机看50节课                        学习了一些
        java并发编程：接着刷题，每天都要刷题和看源码，尤其是锁       搞清楚了concrrenhashmap,linkedblockingqueue,以及一些阻塞队列。
        java集合：搞清楚hashmap,                   
  4.算法：看零茶山的一课，全部看完。刷两道题。                    刷了两道，感觉一天刷个两小时就行，睡觉吧。明天三点半起来学习。
  5.计算机网络：学习tcp/ip
总结：最近没怎么写项目，但我也觉得，没必要重复这些老项目，没一点意思，有这时间还可以多看看呢.加油陈乾珲，弟弟也不跟你说话的。晚安吧。我买了个长毛毯。看了海贼王路飞去伟大航路一集，太好看了，上次看还是12年左右吧，哈哈哈哈。我人生
最自主的一年半，二年级下学期到初三下学期。之后就是行尸走肉了。哈哈哈。好好学习呀！加油加油陈乾珲，以后就是自己和自己聊天。

  这是写下的日子，这辈子所有的任务清单都会写在这个文件里面。写多了就分库分表，yes！
今日任务：2024-06-28   刚把爹！
  java:
    java并发编程:看明白阻塞队列                 看完了阻塞队列和线程池，延时线程池和futuretask还是没有看明白。
    java集合：继续看源码：                
    java基础：慢慢看八股  
    jvm：看类加载器，这方面一天三个小时吧。也不花太多时间 毕竟东西太多了。
    java设计模式：复习一两个设计模式。
  mysql:看锁这一章。                          开始看锁。基本看完了锁，但是掌握的还不是很透彻，需要接着看。我知道了一点，那就是加锁是给扫描区间里的记录加锁。
  redis:循环看学习指引的文章。                  
  计算机网络：循环看文章，3篇                  看了tcp/ip连接。还要接着看
  操作系统：循环看文章，3篇

  我要专注于每一分钟，没必要思考以后的事情。对于我来说，过好每一个小时自然就能过好每一个月。每一年。其实我也发现了，每天还是要复习，因此以后起来的第一个任务就是复习。感觉明天可以
接着看锁这一部分。以及java的线程池框架。还有redis的数据结构部分。这十天就看redis的数据结构部分。Java基础的话。也是每天轮着来，三个小时吧。算法要多刷。多刷，多刷！jvm就按明天的来。

今日任务：

  redis: 数据结构又过一遍。                  明天可以看看redis的网络模型
  mysql：看了锁这一章，明白了加锁的流程，但是精确匹配还没搞明白，以及锁是怎么退化的
  jvm:就看训练营的那些吧，会就行了。      没有看 
  java设计模式：看一两个；               没有看，看一个吧
  java多线程编程：接着看线程池部分的。   线程池应该说阻塞队列这块不行，还有futuretask这块也不行，得多看几遍。看看阻塞队列吧
  算法：刷了几道简单的滑动窗口。明天接着刷滑动窗口。还要听一听灵神是怎么做的。

    线程池：
            为什么线程池的线程不会回收，很简单，线城池的线程是worker对象，他的run方法是死循环，永远不会回收。除非关闭线程池。
            线程池包括一个list对象存放线程，一个阻塞队列存放任务。因此线程池应该有一个主方法。用来进行判断。阻塞队列里的元素类型必须为runnable,或者Callable,后者相比前者可以返回
            线程的结果。
            线程池的参数：
                          1.核心线程数
                          2.最大线程数，阻塞队列，存活时间，时间单位，拒绝策略。这一切都围绕着线程池的处理流程来展开
            核心流程：当任务被提交时，是通过线程池的execute方法提交的。而不是直接放到阻塞队列里。如果线程池里的线程数并不到核心线程数，那就创建一个线程来处理他。如果达到了。
                      就放到阻塞队列里。如果阻塞队列满了，在线程数没有达到最大线程数的情况下,创建一个线程来执行他。该线程在存活一定时间后会被销毁。如果线程数达到最大线程数。
                      那么任务就会被丢弃。
            提交任务的方法：execute（runnable）,此时没有返回结果，而submit（callable）是有返回结果的。
            关闭线程池：    shutdown,shutdownNow,他们都是调用interrupt方法中断线程，区别在于，shutdown只会中断没有运行的线程。
            查看线程池是否终止：isTerminated();
            线程执行结果的获取：Future或者FutureTask对象来获取执行结果，通过object.get()方法来获取。后者可以对线程进行操控。这里的线程池采用的是JAVA传统的线程方式。
            FutureTask可以用来协调各个线程的工作.
            根据线程池的参数不同，会分为很多不同的线程池。关键在于阻塞队列的选择，拒绝策略，此外可以对线程池进行继承，实现自己的线程池。达到不同的效果。比如延时线程池。固定线程池
          不同的线程池，并发效果是不一样的。比如cachethreadpool的并发能力最大。因为它可以创建无数的线程。
              线程池中线程的个数根据任务类型，cpu数量来定，io密集型应该使用2*Ncpu;cpu密集型应该采用Ncpu+1来做。
            java本身提供了四种线程池，前三种根据线程池线程数量区分，最后一种可以执行延时任务。
  线程池的创建采用了Execute框架。

今日任务：  从今天开始都要写记录。
    mysql:接着看文章，看两三篇吧，看一看mysql的逻辑架构，和引擎；redolog,undolog,          看了redolog,undolog，binlog
    redis:看网络模型那一章，                                                               复习了持久化
    java并发编程：好像都看完了。得复习一下，拔高以下，看看应用                             又看了看线程池，更理解了但是没有看futuretask
    java设计模式：每天两个设计模式                                                        以后每天起来先看设计模式
    java基础：接着看java基础                                                              没有看
    jvm：学习                                                                            学习了类加载机制，双亲委派机制等
    算法：刷双指针，每天刷一些sql题。                                                      刷了两三道滑动窗口的题。
    计算机网络：看tcp/ip一节                                                               看了tcp/ip的拥塞，滑动窗口，以及rpc协议，websocket格式
    操作系统：看小林coding                    没咋看。

    mysql加锁：mysql无论是什么语句，在确定了扫描区间后就会进行加锁。正常的枷锁流程就是确定扫描区间然后逐个加next-key锁。但在一些情况下锁会退化。我们要注意这些特殊情况。
    redis持久化：redis的持久化通过aof,rdb来实现。在这其中要注意写时复制技术。是实现高性能持久化的基础。
    java并发编程：难点在于能不能自己手写一个高效的线程池，应对不同的并发请求。只要涉及到生产者消费者模式的。都可以使用ree锁，其他就用syn锁。书写简单。
    mysql日志：  怎么理解Undo，redo，里面存的是数据，所以其实就是数据页的备份，只不过redolog寸的是多个版本的数据的备份，redolog和undo寸的都是和事务相关的数据。rollback时把undolog拿出来刷盘，恢复时把redolog拿出来刷盘。那怎么主从复制呢，用Binlog，事务是否提交以binlog为主，怎么判断事务是否需要恢复，
  就是看binlog和redolog里的事务id，事务id可以在Binlog里面找到，就恢复，找不到就去undolog里进行回滚。
  ·  一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id：

          通过 trx_id 可以知道该记录是被哪个事务修改的；
          通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链；
    那么undolog如何持久化，undolog本身也是数据页，undolog通过redolog持久化。
  什么是redolog？
    redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。
    当事务提交的时候redoLog一定会刷盘的。事务提交时binlog也会刷盘的。redolog可以实现顺序磁盘写。这一点比随机Io效率要高得多。此外redolog也会有其他的刷盘措施。
  
  我觉得我应该做做项目了。基本都看明白了。以后要猛攻网络，网络是架构的基础。





我感觉主要是白天的运动量太小了。然后天天看短视频，其实我很早就开始睡不着了，主要是独居引起的精神兴奋。然后其实我白天并没有很充实的过着。因为我起的太晚了。就是这样，寒假在家我也睡得很好。长期睡不着就会导致昼夜颠倒。嗯嗯，也不是加油，我觉得我需要做出抉择。技术大咖还是混混。只会扣手机的人
我陈乾珲，于2024年7月三日凌晨1点27分宣布，从今开始不再看贴吧，海贼王，哔哩哔哩，只刷算法，看小林，学Java，跑步，睡觉。除此以外什么也不干。有时候就听听音乐。作为消遣，对我也不看什么房子，什么都不看。只看java，戒掉短视频。




今日任务： 2024-07-03
  java：复习了几种设计模式，学习jvm：待会吃完饭回来学。
  计算机网络：要写文章才行，每天看完要理解一下，这个估计得一个月。慢慢看吧，每天主要看tcp/ip；我们还要看看协议怎么拟定。
  java并发编程：看了cas原子类。以及并发编程常用工具。
  mysql：
      1.看buffer pool 
      mysql的buffer pool用三种链表来管理buffer pool的内存空间。通过后台线程定期的刷新脏页。通过redolog来保证数据宕机也可以恢复。以及mysql的lru算法的实现。
      buffer pool 的内存大小是128mb,可以通过参数调节。
      2.看一看mysql运行时的内存结构，搞明白各个分区都是干什么的。相比于其他技术，mysql是目前进展最好的。要继续努力！
      3.mysql的行记录模式，搞清楚是怎么存储的，一行的最大能占用多少字节。溢出列怎么处理。
      4.看一看buffer pool里的change buffer
        写操作时，如果更新页不在内存里，会将数据写在change buffer里，然后定期的merge，异步的写入到磁盘里，当然，如果访问了数据所在页，也会自动触发merge。处理读大于写的任务时，普通索引加上change buffer可以很好的满足大量
  写入的要求。
      5.select的执行流程
          连接器（连接池，每一个连接都对应一个线程）   解析器（语法，词法解析），优化器（建立执行计划），执行器（与执行引擎交互，单位是记录。执行引擎根据扫描区间拿到记录后返回给执行器，执行器判断是否符合查询条件。如果查询条件就是索引，那么判断会直接在引擎层执行。） 执行引擎
      6.mysql内存结构，看一看吧。
      7.看懂了两阶段提交，和order by的优化方式。
感悟：这些唯一两个狠抓的就是mysql和redis，每天循环的看，好好看！今天mysql没什么长进，我觉得可以一遍遍看吧。


    
今日任务：
    mysql：继续杂七杂八的看，但我觉得还是应该接着搞懂mysql的逻辑结构，再看看B+树。 目前看了为什么存储不能超过2000w,因为层高三层较为合适。四层会有磁盘Io。看了B+树的储存。我觉得我可以看看别的，总结一下sql语句的执行流程。主要就是各个sql语句的执行流程。
          一个知识点就是Mysql的各个查询都是怎么工作的。
                  select ,order by ,group by,distinct,union,子查询，join这几个的流程，会采用什么算法，怎么优化。
            order by 记住filesort_buffer，
            join: nlj,bnl，bka,hash join四种方式，Bka本质上优化的是回表以及对驱动表的扫描次数。被驱动表根据索引获得的id是随机的。那么就可以使用bka，mrr对id进行排序，这样就可以优化了。因此Join查询最好就是bka,其次是inlj，bnl，最差是snlj，记住选择小表作为驱动表。
                  inlj：前提是查询被驱动表用到了索引。那么流程：
                              1.扫描驱动表的记录，获得符合查询条件的记录，
                              2.根据被驱动表的索引来查询。获得被驱动的主键id，回表查询得到记录
                              3.判断记录是否符合要求，符合就组成结果集。返回一条。
                  缺点：访问被驱动表的时候是随机io。
                        假设驱动表记录是100次，那么被驱动表也会被扫描100次。扫描次数是200次。
                  snlj：被驱动表没有索引，那么就会触发被驱动表的全表扫描。过程和inlj一样
                  缺点：假设被驱动表记录100次，被驱动表记录数是10000次，那么扫描次数是100+100*10000次；
                  BNL：使用Join buffer,将驱动表加载进内存里，然后把被驱动表加载进内存里，从被驱动表里取出一条记录。与内存里的驱动表进行比对；
                        假设驱动表内存记录数是100行，被驱动表10000行，那么扫描次数就是100+10000，但是判断次数是100*10000，很占用cpu资源。
                        此外，如果join_buffer撑不下驱动表，被驱动表就要被多次扫描。扫描次数是N+λ*N*M;判断次数仍然是N*M，即100*10000；
                  bak:  bak其实就是snlj，但是bak的前提是访问被驱动表的时候用到了索引。 对于inlj的随机io问题，bak算法会将查询索引得到的被驱动表id值放在mrr里，进行排序，然后顺序Io。
                        1.将驱动表的记录加载进join_buffer,将join_buffer里的记录与被驱动表的索引进行匹配。得到一系列的被驱动表id，
                        2.将被驱动表id放到mrr缓存里。mrr对id进行排序，之后顺序回表，返回结果集。
                  bak算法即优化了inlj的随机io,而且用到了join_buffer，减少了对驱动表的io次数。效果是最好的。假设驱动表有100行，被驱动表扫描后有100行，那么扫描次数就是100+100，而且是顺序io，批量io。有助于增加join语句的执行速率
                        bak缺点：如果被驱动表加大。那么就要维护一个很大的索引树，为了优化。可以创立临时表。对临时表加索引。这样更可以优化了。
                  hash join: 将连接表与被驱动表的记录拿到java里，使用hashmap作为中间点。进行组合。
                  
            group by :
                  1.根据group by的依据，构建临时表。将select里的数据挑出来作为临时表的字段，临时表的主键就是group by的依据
                  2.全部拿出来后，再放到sort_buffer里进行排序。执行聚合函数。返回结果集。
                  怎么优化group by:可以省掉排序，去重的步骤，那就是给group by 创建索引。直接扫描索引就行了。还不用回表。
      
            distinct:
                    1.将distinct字段拿出来作为主键。放到临时表里面，之后sort_buffer里去重。
            union:
                  1.将第一个子查询的全部字段作为主键，
                  2.创建临时表，执行union后的子查询时，在将查询结果放到临时表前，先要判断是否有主键冲突，如果有就会舍弃该条结果。
                  3.查询完毕后，将临时表作为结果集，返回，并删除临时表
            union all :
                  相比于union不会创建临时表，会直接返回每一个子查询的记录。
            
            子查询：
                    
        redis:
            多线程网络模型看的不咋地，得多看几遍。看了过期删除策略和内存淘汰策略。
            看一看二进制协议，看了redis的resp协议，之后再看多线程模型吧，得一遍一遍看，
            我感觉,redis和mysql一天各自最少三小时。
            redis:   
                      缓存，缓存属于临界资源，要修改缓存的话，必须使用分布式锁。而使用分布式锁，就要考虑到多节点的问题，npc问题
                      怎么分片，哈希槽以及缓存一致性问题。
        redis:感觉实操不行，找个时间实操一波。加深印象。
        java设计模式：
              备忘录模式：用于恢复发起人的状态。可以恢复任意时刻的状态。
                  源发器（Originator）：需要保存和恢复状态的对象。它创建一个备忘录对象，用于存储当前对象的状态，也可以使用备忘录对象恢复自身的状态。
                  备忘录（Memento）：存储源发器对象的状态。备忘录对象可以包括一个或多个状态属性，源发器可以根据需要保存和恢复状态。
                  管理者（Caretaker）：负责保存备忘录对象，但不能修改备忘录对象的内容。它可以存储多个备忘录对象，并决定何时将备忘录恢复给源发器。

               访问者模式：
  
         java并发编程：
                  多线程要多看。

            以后每天早上四点起来，看mysql+redis看到11点，之后干别的事情。这俩太重要了.


      今日任务 2024/07/05
          mysql：
            看explain查询计划。分库分表，数据库连接池，别的好像没看
          redis:感觉可以看看高可用部分。然后自己我觉得也没必要非得写个项目，写项目太花时间了，看了集群。这部分挺难理解的。可以实操一下；
          并发编程：aqs
          总结：以后每天都要看这三个，这三个是重点中的重点。
          java设计模式：复习java的设计模式，
          java:背java的八股文了，每天三四个小时。这样两点前解决这四个。
          算法：刷了单调栈，感觉不能贪多。慢慢刷吧.我觉得每天看看灵神的视频是耗时最少的了。不要看多，没用
          
          
          今日任务：2024/07/06
        java：
                明确以后就是上午数据库+并发编程，下午Java+jvm，晚上计算机网络+算法+操作系统
                学完序列化，反射，注解三大块。学习Jvm里的字节码部分

        jdk内置的序列化时两个流。
              jdk序列化的时候不会序列化静态变量，trasient变量，只会序列化成员变量。注意也不会序列化方法这些。
              jdk序列化的时候包括两部分，头部，对象，头部生命版本号，魔数，对象部写成员属性。还包括开始结束符，类名，签名，属性，属性值。
              以及一些特殊分隔符。
            序列化Id:
    如果可序列化类没有显式声明 serialVersionUID，则序列化运行时将基于该类的各个方面计算该类的默认
    serialVersionUID 值。尽管这样，还是建议在每一个序列化的类中显式指定 serialVersionUID 的值。
    因为不同的 jdk 编译很可能会生成不同的 serialVersionUID 默认值，从而导致在反序列化时抛出 InvalidClassExceptions 异常。
    serialVersionUID 字段必须是 static final long 类型。
            序列化要点：
            1.父类如果是se,子类就都可以序列化。而子类是，父类不是，父类不会被序列化，数据丢失
            2.属性是对象类型，属性必须实现序列化，否则会报错
            3.反序列化时，如果对象属性有修改，增加，删减，那么修改部分数据丢失，不会报错
            4.反序列化时Id被修改会序列化失败。
            5.序列化会忽略trasient的属性
      java还指定了Externalizable接口实现自定义的序列化，但他的协议依然是Jdk自身的协议，jdk序列化是无法跨语言，且非常的笨重，不适合在网络中
进行传输。因此应该使用二进制序列化，以及json序列化。对于http应用，使用json序列化，对于微服务相互调用就应该使用二进制序列化，当我们进行传输的时候
      首先应该考虑的就是序列化的协议。一般基于某种编码的字符串是最合适的
      json序列化：  可以用FastJson,Jackson,Gson来进行json序列化，spring默认Jackson进行序列化。从性能上看Fastjson > Jackson > Gson

  序列化要考虑到 安全性（是否存在漏洞，加密。当然使用https时网络报文层面会进行加密，但自定义的rpc是没有加密的），兼容性（跨平台，跨语言），性能（时间，空间），易用性。
    java反射：
        反射广泛应用于注解，框架，动态代理里面。
        反射首先要获取Class文件，class文件的获取方式有四种，常用的是三种，第四种不会对Class对象进行初始化。Class文件本质是Class对象
    通过CLass文件可以获取Method,FIELD对象。通过Method,field对象可以获取Annotation对象，之后可以获取注解的值，进行赋值操作。对私有的成员，方法进行调用时
首先要修改可达性，修改为true;
      反射的缺点：存在安全性问题，反射的性能稍差。
      反射可以去修改成员属性的值，调用成员方法，生成对象。  
      反射的本质就是将JAVA类里的各种成分都映射成为对象，method映射成Method对象，成员就映射成FIELD对象。
      instanceof 关键字比较的就是class对象是否一致。
      动态代理：
      jdk自带的：        实现InvocationHandler的接口必须是代理模式，里面有原目标对象的属性，之后重写invoke方法。然后调用Proxy.newInstance()方法创建代理对象。这样创建的class对象，在使用结束后就会被销毁，减轻内存的负担，但是动态代理只能基于接口进行，对于一些类对象，想实现动态代理只能用cglib动态代理
      cglib:   spring里的aop使用cglib来生成动态对象。代码上和jdk差别不大，但是底层逻辑不一样。在CGLIB中，方法的调用并不是通过反射来完成的，而是直接对方法进行调用
注解：
    定义注解时要用到元注解
    @Documented
    @Target({ElementType.FIELD, ElementType.PARAMETER})   //可以出现在属性，参数上
    @Retention(RetentionPolicy.RUNTIME)                   //可以出现在运行时。即可以通过反射获取他。
    public @interface RegexValid {}
注解属性只能使用 public 或默认访问级别（即不指定访问级别修饰符）修饰。

注解属性的数据类型有限制要求。支持的数据类型如下：

所有基本数据类型（byte、char、short、int、long、float、double、boolean）
String 类型
Class 类
enum 类型
Annotation 类型
以上所有类型的数组
注解属性必须有确定的值，建议指定默认值。注解属性只能通过指定默认值或使用注解时指定属性值，相较之下，指定默认值的方式更为可靠。注解属性如果是引用类型，不可以为 null。这个约束使得注解处理器很难判断注解属性是默认值，或是使用注解时所指定的属性值。为此，我们设置默认值时，一般会定义一些特殊的值，例如空字符串或者负数。

如果注解中只有一个属性值，最好将其命名为 value。因为，指定属性名为 value，在使用注解时，指定 value 的值可以不指定属性名称。
使用注解：
      Method,Field属性都有isAnnotationPresent,和getAnnotation(Class class)属性来获取方法，属性，参数上的注解。进而获取值。
枚举：
          在 enum 中，提供了一些基本方法：

          values()：返回 enum 实例的数组，而且该数组中的元素严格保持在 enum 中声明时的顺序。
          name()：返回实例名。
          ordinal()：返回实例声明时的次序，从 0 开始。
          getDeclaringClass()：返回实例所属的 enum 类型。
          equals() ：判断是否为同一个对象。
          可以使用 == 来比较enum实例。

          此外，java.lang.Enum实现了Comparable和 Serializable 接口，所以也提供 compareTo() 方法。

          枚举也可以实现方法，枚举自身也可以定义方法。也可以调用方法。
          枚举适合实现单例模式。而且是最佳方法。
  netty：
        使用Channel，ByteBuffer读取磁盘文件.
       try(FileChannel channel=new FileInputStream("E:\\project0\\Redis-笔记资料\\03-高级篇\\资料\\juc_learn\\juc1\\eslearn\\src\\test\\resources\\application.yaml").getChannel()) {
            ByteBuffer buffer = ByteBuffer.allocate(10);
           while(channel.read(buffer)!=-1)//read代表的是方向，从磁盘到内存，write代表的是从内存写到磁盘。
           {
               buffer.flip();//切换到buffer的读模式
               while (buffer.hasRemaining())
               {
                   byte b = buffer.get();
                   System.out.print((char)b);
               }
               buffer.clear();//清空buffer，切换为写模式
           }
          Channel是FileInputStream读写文件的关键。ByteBuffer是内存里的区域，用来存储Channel读写的字节。
  mysql:
        子查询执行流程。以及一条普通sql语句的执行流程
  感觉并发编程得一遍一遍地看，一遍一遍的做，刷题。现在首先学会各个api的使用，把leetcode的十道题全部刷完。然后多多看源码，写总结。但我感觉，网络Io加上多线程才是网络编程。redis我觉得明天可以看看Lua,学会搭建redis集群
  mysql的话，我感觉要一遍一遍的看。现在重点看看事务机制。


mysql
    slow_query_log long_query_time 
  set global long_query_time=1 
  log_output=FILE


redis最关键的部分就是数据结构和应用场景，以及高可用部分了。高可用还涉及到了故障恢复。我在想该怎么解释这个东西。
redis学一下lua  学习完毕。
然后看计算机网络。我觉得计网可以先跳过Tcp.看别的，现在感觉关键就是滑动窗口没看明白，把他看明白。今天的任务，然后写一些介绍。
    RTO:重传时间间隔。包发送后达到指定时间没有收到ack时，就会重新发包
    RTT:发送一个包到收到对应的ack,花费的时间。
  窗口：
      窗口分为发送方和接收方，应该说双方都有两个窗口
   
            Tcp报文段里的Window字段，用于告诉接收端自己还有多少缓冲区可以接受数据，发送端会根据这个来发送数据，这样不会导致流量过多。发送方发送的数据不能超过接收方的窗口大小。但是发送方再发送时也不是把大小为窗口的数据发送的数据全部发送过去
  而是一批一批的发送，之后，接收方会返回ack，发送方根据ack移动窗口，确定还可以发送多少数据。
  发送方：
        SND.WND 表示发送窗口的大小（等于对方的Window）
        SND.UNA  指向以发送但未收到ack的第一个字节的序列号。 SND.NXT 可用窗口大小，指向未发送但在可发送范围的第一个字节序列号
        可用窗口大小=SUD.WND-(SUD.NXT-SUD.UNA)
  接收方：
        RCV.WND:接收窗口的大小，等于自己的Window
        RCV.NXT:指向期望从发送方发送来的字节序列号
      发送方会在窗口大小内持续发送窗口，但只有在ack时才会向右延伸窗口，这样就不会导致一次发的太多而造成流量的浪费。  比如收到了ack以后，ack返回的时序列号，SND.UNA和SND.WND会加上这个值。一旦SND.NXT一旦在这个范围内。经过运算，可发送
窗口的大小如果不为0，就会发包，然后移动SND.NXT。直到超过这个大小。
        滑动窗口在操作系统的缓冲区中，缓冲区的大小=滑动窗口的大小+进程未读数据的大小  就是说如果缓冲区为500字节，窗口一开始是500个字节，发送了140个字节，只读了40个字节，那么窗口大小就是400字节了。因此进程阻塞会引起网络传输阻塞。
因此，窗口有可能变成0，引发死锁。这里就出现了窗口探测程序，防止死锁的发生。
  如果窗口大小甚至小于tcp/ip头，那就划不来了。
  利  用窗口可以进行流量控制，拥塞控制
    拥塞窗口 cwnd是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。

我们在前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。
拥塞窗口 cwnd 变化的规则：
只要网络中没有出现拥塞，cwnd 就会增大；
但网络中出现了拥塞，cwnd 就减少；

了解了滑动窗口，拥塞控制，之后应该看看应用层的协议了。这才是根本。

socket函数：
    1.服务端调用Bind函数监听ip,端口，客户端调用connect函数绑定一个Ip，port。服务端之所以也这样做，是因为计算机有多个ip地址。
    2.服务端绑定好后，会调用listen()函数进行监听。此时客户端可以发起连接了
    3.客户端函数调用connect，连接服务端，连接成功后，连接会放在连接队列里。
    （一单connect,listen返回，就代表连接成功。在内核里，会给每个socket建立两个队列，连接队列和半连接队列。队列里的成员是其他客户端的socket）
    4.服务端会调用accept()拿到已经完成的连接进行处理，没有就会阻塞。连接完成后，accept会返回另外一个socket进行处理，所以要注意，监听socket,处理的socket不是一个。连接完成后，双方会通过read,write开始传输数据。类似于文件流
    感受：其实socket就是应用层了。read,write里面会做数据的校验和重发，其实和java是一样的，哈哈。    
在内核里，socket是一个文件，每个进程都有一个task_struct,这个数据结构里，有一个数组包含了该进程所有的文件描述符，文件描述符就是一个整数，是数组的下标，通过文件描述符可以访问到文件的位置，所以socket本身与其说是文件，
    不如说是数据结构。而socket里，包含了一个文件流。而socket的指针，指向了内核中的socket结构，
    socket结构里有两个队列，发送队列，接收队列。队列的成员就是缓存，一块内存区域。保存了对面发送来的tcp包。每一个数据都能看到完整的tcp包。
    所以到此我们可以知道端口的作用了，他只是根据端口来绑定一个socket，我们可以根据prot来确定一个socket而已。实际上在硬件层面根本没有这回事。只有Ip地址，mac地址。通过端口找到socket,通过socket找到连接队列，队列本质上是数组，或者连表
    创建数据结构进行队列的入队出队操作即可。入队的就是tcp包。
总结：socket本质上就是两个连接队列以及半连接队列。位于内核里，通过port来区分不同的socket,通过socket找到连接队列，将网络包去掉ip部分将完整tcp部分放进队列里。发送时将包交给ip即可。这就是所谓的缓冲区。
udp连接：
    udp连接使用sendto,recvfrom来发送，接收包。

我们可以看到，这种基础socket是一对一的。无法多对一。
  因此往往采用io多路复用的方式来维护多个socket,具体实现有select,epoll两种方式，select的复杂度是On,epoll是OlgN，因为采用了红黑树。这样就可以由一个线程处理多个socket连接。没看明白，就这样吧。
dns：
    1.重定向,即改变映射的值
    2.负载均衡，一个域名配置多个ip,然后返回最合适的ip。
cdn：dns的装饰模式
    看用户的 IP 地址，查表得知地理位置，找相对最近的边缘节点；
    看用户所在的运营商网络，找相同网络的边缘节点；
    检查边缘节点的负载情况，找负载较轻的节点；
    其他，比如节点的“健康状况”、服务能力、带宽、响应时间等
    cdn的缓存代理，多级缓存，层次缓存。
如何实现cdn:答案就是nginx+openresty
本质上通过Nginx发送请求，结合vue生成html返回，也很好。
看看jvm

感觉，还是要保证时间，明天要好好看redis了，高可用部分！场景部分！我觉得一天积累一个场景，实现一下。然后天天看高可用！数据结构一天复习一个就行。多线程也是，要多练，明天开始刷题！

今日任务 2024/07/08

今天很不好，手机掉到马桶里了，坏了，他是一个很好的手机，2020年3月的一天中午，妈妈问我要不要手机，就给我买了一个，他真的很好。永远不会发热，不会内存紧张，不会自动关机，我拿着它，其实也玩了很多，不像原来那个，huawei,
我觉得，手机既然作为一个消费产品，我也没必要买太好的。以后我的手机就不超过1000元，我没必要要那么好的。钱应该花在健康，知识上。而不是这些最终伤害自己的地方。钱不花要比花好，我还买了个健身自行车，以后天天骑车，每天两个小时骑，一定可以减肥的


下午开始看redis的高可用部分。全部搞定！熟悉各部分的原理，然后复习上午看过的redis场景；

redis的高可用包括主从复制，哨兵，集群三种方式，集群可以解决海量数据高并发写高可用的问题，哨兵可以解决高并发读高可用的问题，因此数据量很大时就应该选择集群方式了。集群和哨兵都是以主从复制为基础的。
主从复制包括全量复制，增量复制，缓冲区；
  流程：
      1.replicaof ipA  ipB,确认主节点的Ip地址，发起连接。
      2.建立连接收，从服务器发送psync ？ -1申请数据同步。主节点看到-1后开始生成rdb文件，全量同步
      3.同步期间的，直到从服务器发送回数据，主服务器执行的命令都在replication buffer 备份，从服务器返回备份成功后，主服务器会将buffer里的命令全部发送给从服务器。
      4.之后主服务器每执行一条命令，都会将命令发送给从服务器。从服务器每执行一条命令，都会把自己的slave_repl_offset加上命令的字节数，而主服务器每执行一条命令，也会给自己的master_repl_offset加上命令的字节数。
      5.在这期间，如果出现了延迟，主服务器会把命令写在replication buffer里，循环写，主从再次连接后，从服务器再次发送psync runid offset，主服务器根据offset（就是slave_repl_offset）,和replication buffer里的指针replication offset（就是master_repl_offset），计算两个Offset的差值，
      如果超过了replication buffer的大小，就会触发全量复制，如果不超过，就将replication buffer里对应的字节发送给从服务器，也就是增量复制。
      6.因此，为了放置全量复制多次发生，可以调大repl_backlog_buffer的大小，调节公式为：second*write_size_per_Second，second是秒(从服务器断线后重新连接上主服务器所需的平均时间)，后者是主服务器每秒平均产生的写命令数据量的大小。
      具体参数是repl-backlog-size 1mb
  总结：以上就是redis主从复制的流程，可以看到从节点过多也会影响主节点的性能，尤其是网络不好时触发的全量复制，因此在使用主从模式时应该控制从节点的数量，此外，主从复制可以应对从节点挂掉的情况，无法处理主节点挂掉的情况，因此就出现了哨兵机制，哨兵机制里依然是一主多从多哨兵，无法解决海量数据
  高并发写的问题，因此又引入了集群模式。
      如何应对主从不一致？
      redis无法做到强一致性，在主从模式下都做不到强一致性，为了强一致性，往往会在前端采取措施，比如支付后会有一个支付成功页面，点击返回才能返回。这就是通过业务来保证一致性。
      redis里的info replication命令可以查看主节点接收写命令的信息进度和从节点复制写命令的进度，我们可以开发一个监控程序，得到master_repl_offset,slave_repl_offset的差值，差值过大就不会连接相关的从节点。
      redis如何判断某个节点是否正常工作？
      通过ping-pong的心跳检测机制，如果一半以上的节点去ping一个节点都没有pong,就认为这个节点挂掉了
      主节点每10s发送ping命令，判断从节点的存活性
      从节点每一秒 发送 replconf ack offset命令，给主节点上报复制偏移量
      主从切换如何减少数据丢失？
      主节点执行完命令后就断电，会导致命令没有发送给从节点（此时aof还没有刷盘），主节点数据丢失，从节点没有接收到写命令，同样没有数据。
      这个主要会发生在增量复制和全量复制期间，命令无法立刻发送给从服务器。
      min-slaves-max-lag：lag值指的是从节点上一次发送 replconf ack <replication_offset>相对于现在的时间差，主节点发送info replication后每一个从节点都有一个lag值，当所有从节点的lag值超过min-slave-max-lag的值时，主节点就拒绝写入请求，而且还有一个值，min-slaves-to-write，
      指的是从服务器的数量，当数量小于min-slaves-to-write时，主节点也会拒绝写请求
      从服务器每隔1s会向主服务器发送replconf ack <replication_offset>命令，来检测主节点的网络情况，以及自己的复制进度是否有差距。主节点拿到replication_offset后，于自己的master_repolk_offset进行比较，如果有差别，就会把repl_back_log里的差异部分发送给从节点
      主节点每10s也会给从节点发送ping命令，检测从节点的网络连接情况，通过ping,replconf ack <replication_offset>来相互确认对方。
      通过这两个配置数据可以解决哨兵机制时，切换主从节点导致的数据损失。
哨兵
    哨兵主要负责 监控，选主，通知。
    监控：
        每隔1s给所有的主从节点发送Ping命令，当主从节点接收到ping命令后，回复pong,这样来确定各个节点是否运行，如果没有在规定时间内回复，哨兵将标记节点为主观下线。该规定时间由down-after-milliseconds参数设定，单位是ms，
        除了主管下线，还有客观下线，客观下线用于主节点。当一个哨兵发现主节点主管下线后，便会向其他哨兵发起命令，其他哨兵进行赞成或拒绝投票，当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」，判断下线后，哨兵就开始在多个从节点中
    挑选一个从节点作为主节点。那么谁来挑选主节点，多个哨兵会进行投票，每个哨兵一个投票机会，要想成为leader必须拿到半数以上的赞成票，票数还要大于quorum值，而如何成为候选者？那就是判断主节点为客观下线的那个哨兵。6
      执行过程：
            1.发现主管下线，进行投票，超过quroum就是客观下线，确定客观下线的哨兵成为候选者，候选者开始投票，达到quroum以及拿到半数以上的赞成票就成为leader,leader执行主从切换
            2.选出新主节点。
                          1.筛选掉网络不好的从节点，这主要是哨兵的心跳检测，如果断连超过十次，就认为网络状态不好
                          2，根据优先级，复制进度（offset），id号进行最多三轮排查。选择新的主节点。redis中有一个slave-priority，值越小优先级越高；优先级相同时，根据复制进度，最多的作为主节点；复制进度也相同时，根据从服务器的id号。
            3.选出主节点后，哨兵leader向从节点发送 slaveof no one，命令，将从节点变为主节点，发送该命令后，leader每秒向该节点发送info命令，观察该节点的角色信息。当变成master时，就成了主节点。之后哨兵节点向所有从节点发送slaveof ip port，来实现主节点转移。一切做完后，哨兵将新节点的信息返回给
            客户端。
      核心：
            1.ping监听各个节点的状态，主节点客观下线后，通过投票选出leader,leader根据优先级，offset,id来选择主节点，选好后向该节点发送slaveof no one，同时每秒发送一次info指令，观察其角色，转变为master后向其他节点发送slaveof ip port。转换完成。
    哨兵无法解决高并发写的问题。以及海量数据的存储问题，因此redis的最终状态就是集群。  
集群
    得实践一下
redis场景：  网易游戏直播间弹幕系统
      1.一般思路：数据库里维护一个弹幕表，根据直播id,用户id，时间戳可以确认一个弹幕。不存在传递依赖。发弹幕时，直接写进数据库，看弹幕就是轮询，从数据库拿信息
        缺点：扛不住高并发
      2.加入redis:
              先写redis,redis写mq,mq异步写到数据库，查询依然是轮询，看弹幕依然是轮询 ，redis采用的数据结构是zset，按时间排序来拉取（注意List虽然也可以排序，但List无法按照时间来拉取，因此如果我们要根据客观因素，时间，评分来拉取指定范围的数据时，只能用zset）
                mq，redis的消息会丢失，无法保证持久化。可以使用mq/监听redis日志来实现延迟写。
                缺点：mq,redis挂了，消息会丢失。而redis一挂就会导致缓存击穿。打崩数据库。（注意先写redis，再写mysql都有这些问题）
                如果短时间突然有大量的弹幕，可以使用mq来削峰。缓存在mq的硬盘文件里
                对于弹幕读请求，可以使用本地缓存，缓存近5s的数据，过期了就由服务自身回源redis,（数据量太大时会导致gc）
网络模型：
      今天看一看网络模型，感觉一切到最后就是网络模型呀！
      
  看一看抽奖项目；
      1.抽奖的用户要进行鉴权，满足级别的才可以。参与抽奖的用户有一个过滤。可以在网关或者拦截器进行操作。
      2.抽奖的形式：秒杀活动/每天有一定次数的抽奖
            前者存在高并发，后者就是保证奖品不能早早的抢光。
      3.抽奖的概率：要进行控制
      4.风险控制：防止攻击。保证奖品不超发。中奖概率要均衡。
  中奖的流程：
          在此之前有个过滤层，需要对用户进行鉴权，认证。基本逻辑                              基本逻辑-鉴权逻辑
                为了防止用户出现一些多刷行为，对用户进行一些额外逻辑处理，限制他的中奖次数。      业务逻辑-风险控制逻辑
                          java层有一个函数判断是否中奖，中奖就执行数据库操作。创建订单。否则就返回    业务逻辑-核心逻辑  
          由此可见，该项目需要采用责任链模式，代理模式进行处理。以及业务的扩展。
感悟：设计模式yyds呀！代码就是按照设计模式来写的。
设计模式：
      1.命令模式：
              1.抽象命令： 一个抽象接口，定义了执行命令的统一方法
              2.具体命令：  具体命令，拥有接收者对象
              3.接收者：    执行实际命令的类，命令对象调用接受者的方法来执行命令，也就是说，执行命令里写的就是接收者的方法，来执行命令。
              4.调用者：    持有命令对象。比如waiter
              总结：具体命令里包含了接收者。具体命令的实现方法写的就是接受者的方法。然后调用者包含了一个Command对象，调用者执行命令的逻辑里写的就是具体命令的方法。
      举例：服务员点餐，点完报给前台，前台报给厨师，厨师做完交给前台，前台交给服务员，服务员交给顾客。所以是顾客点餐。顾客点餐叫来服务员，服务员接受菜单后交给厨师。在这里一个方法的执行结果交给另一个执行。
              抽象命令：command
              具体命令：点菜，做饭，端饭
              接收者：接收者进行点菜，做饭，端饭  真正命令的执行者是接收者，具体命令包含了接收者而已。但有时候，接收者需要继续分配命令，接收者自己又是调用者。什么是调用者，包含一个command对象，什么是接收者，接收者被放在具体命令对象里。我们可以看到，当一个人要执行命令时，他就是接收者
              ，他要被放在具体命令里，他的逻辑要放在具体命令实现的抽象方法里，当一个对象是调用者，他应该持有一个命令对象，在他的方法里调用命令对象的执行命令方法。
          命令模式见于：一个业务逻辑需要一系列的对象来执行方法，调用方法。那就需要命令模式了。
      2.责任链模式：
                  抽象处理器，
                  具体处理器：
            按照某个条件进行链式调用，就是责任链模式，常见于sevlret的过滤器。
    3.观察者模式：
            常见于发布订阅。
今日感悟：感觉redis看的也差不多了。剩下就是要实操，每天连连redis的api，看看redis的场景题。主要是要实操才行。mysql其实也差不多了，剩下就是也得实操，还得背。我觉得可以看一些进阶内容了。还有，每天还是要看怎样运行的和设计与实现。并发编程也是要实操，加上看源码。网络这块，感觉每天看看netty算了
，一天看个四五节。对于理解网络编程是好的。jvm我觉得不要急。每天看一些。重点是java八股，赶快看完，还有一些场景题目，加密算法。


先学设计模式吧，看看设计模式。
访问者模式：
        抽象访问者：定义了对自身数据结构中各个元素的操作，是接口
        具体访问者：实现了访问者接口中定义的操作
        抽象元素：定义了接受访问者的接口，通常是一个接口或抽象类，其中定义了一个接受访问者的方法，被访问者对象作为方法的参数。
        具体元素：实现了抽象元素接口，它是数据结构中的具体的元素，用于接收具体的访问者并执行相应的操作。每个具体元素都有自己的业务逻辑，并在接收访问者时将具体的操作委托给访问者进行处理。
        对象结构：是一个包含元素角色的容器，提供让访问者对象遍历容器中的所有元素的方法，通常由 List、Set、Map 等聚合类实现。
      对象结构提供了让访问者访问容器每一个元素的方法。元素本身的抽象方法里，参数是访问者，而在里面其实是访问者将元素作为参数进行调用。
        对元素的操作是在访问者里定义的，在具体访问者里实现的。抽象元素里的方法接收抽象访问者作为参数，也就是说，其实元素本身是有操作的，但是里面只是调用了参数的方法，也就相当于把具体操作交给了访问者来实现。
      举例：参观博物馆的时候，游客和考古学家对每一件展品所做的操作是不同的。即对元素的操作是不一样的。那就需要把对展品的操作拿出来，交给这些游客来实现。 
      访问者模式就是将对数据结构的操作与数据结构分离。这样就可以将对数据结构的操作多样化。尤其是这个数据结构并不是数据型数据结构时。
责任链模式：交给一批来处理，包括抽象处理器和具体处理器两个元素
命令模式：存在相互调用的时候用命令模式。
今天应该学习java多线程的应用了。狂刷一天的题。搞明白场景。
Semaphore：
      semaphore底层就是aqs队列，acquire()方法就是将state的值减去一个值，relase()方法是将state值加上一个值。本质他也是用到了state队列。acquire方法则是会加入阻塞队列，
感觉这个月，要加强并发编程的练习。不能光看，还要在写的过程中了解原理。
最朴素的办法就是lock,和syn+volatile，用这两个解决问题。不要去用高阶的，容易死锁。感觉不会的就是并发编程，要多练！数据库这块还行，要实操!       
今天的任务：搞明白并发编程！
    synchronized:
            可以调用wait,notify将当前线程加入阻塞队列，或者唤醒阻塞队列里的线程，返回的前提是获得了锁。调用wait方法时，会将当前线程加入阻塞队列，notify方法会将线程加入同步队列，在同步队列里争抢锁成功后，就可以继续运行。
    线程之前的通信可以使用管道输入输出流：PipedReader,PipedWriter.
jdk本身提供的synchronized,wait,notify,notifyAll,PipedStream,join，volatile提供了基本的线程运行逻辑。volatile是一种轻量级的锁。wait,join都提供了超时模式，syn基本都是阻塞式的获取锁。
如果想恢复线程的执行，可以用LockSupport
lock相比于syn可以多次释放获得锁，且lock提供了condition,非常好；condition必须和lock相关联。此外，lock可以中断的，非阻塞的获取锁。
在java里，实现自定义的同步器，比如锁，阻塞队列，都要采用aqs同步器。
原子更新类：
      原子更新类都无法解决aba问题，可以更新引用，int,boolean,Object类。他们的使用不是很熟悉，但可以用来计算商品的总价格等。不过当商品服务部署了多个服务时，就不太行了。
多线程工具类：
      Semaphore,CountDownLatch,Exchanger,CycliBarrier一共四个。
  CountDownLatch:用于协调主线程和子线程的工作。   适合于传输大文件。再结合操作系统的零拷贝效率会很高。
  CycliBarrier:    cyclibarrirer可以协调所有的线程一起工作。当一个线程到达await方法后，线程就会阻塞，知道所有的线程都到达了await,且变量的值减为0，所有的线程才会被全部唤醒，开始工作。常见的例子就是打游戏，只有所有的玩家都进入游戏，
                  游戏才能开始。
  Semaphore:    信号量，用来控制并发线程的数量
  Exchanger:   用于两个线程之间交互信息。exchange相比于PipedReader的单向传输,使用exchanger的两个线程可以相互传输信息，

  

    CountDownLatch源码：
            使用了aqs同步器，
          public void countDown() { sync.releaseShared(1); }  将state减少1
          public void await() throws InterruptedException {sync.acquireSharedInterruptibly(1);}  state值为0的时候就返回。因为doAcquire返回的条件就是state==0;

          //释放锁
          protected boolean tryReleaseShared(int releases) {
          
            // Decrement count; signal when transition to zero
            for (;;) {
                int c = getState();
                if (c == 0)
                    return false;
                int nextc = c-1;
                if (compareAndSetState(c, nextc))
                    return nextc == 0;
            }
        }
    }
          public final void acquireSharedInterruptibly(int arg)
            throws InterruptedException {
        if (Thread.interrupted())
            throw new InterruptedException();
        if (tryAcquireShared(arg) < 0)
            doAcquireSharedInterruptibly(arg);
    }

 protected int tryAcquireShared(int acquires) {
            return (getState() == 0) ? 1 : -1;
        }
//获取aqs锁，插入到同步队列里，如果之前的节点是头节点，就把自己设置成头节点。然后返回。
 private void doAcquireSharedInterruptibly(int arg)
        throws InterruptedException {
        final Node node = addWaiter(Node.SHARED);
        boolean failed = true;
        try {
            for (;;) {
                final Node p = node.predecessor();
                if (p == head) {
                    int r = tryAcquireShared(arg);//不等于0的时候就代表state还有值。那就继续阻塞，
                    if (r >= 0) {
                        setHeadAndPropagate(node, r);
                        p.next = null; // help GC
                        failed = false;
                        return;
                    }
                }
                if (shouldParkAfterFailedAcquire(p, node) &&
                    parkAndCheckInterrupt())
                    throw new InterruptedException();
            }
        } finally {
            if (failed)
                cancelAcquire(node);
        }
    }
总结：4中多线程工具类的核心依然是aqs，可见aqs是并发编程的核心。除了aqs,还有volatile,synchronized,cas,unsafe，如何基于aqs实现公平锁，非公锁，可重入锁。
    aqs的核心就是阻塞队列，诀窍就是成为头结点的线程获得锁。而释放锁的核心就是要唤醒head.next节点对应的线程。采用的是unpark() ,此外还有可重入锁的设置，都是一样的。Condition是一个封装，它封装了ConditionObject,该类里面有一个阻塞队列。
 public class ConditionObject implements Condition, java.io.Serializable {
        private static final long serialVersionUID = 1173984872572414699L;
        /** First node of condition queue. */    阻塞队列第一个节点
        private transient Node firstWaiter;
        /** Last node of condition queue. */  阻塞队列最后一个节点
        private transient Node lastWaiter;
        public ConditionObject() { }
公平锁，非公平锁的获取.该怎么设置？
阻塞队列：
      插入方法，移除方法，检查方法，这三个方法根据队列为空，满的时候采取什么操作。有以下几种：抛出异常，返回特殊值，阻塞，超时退出。为了避免命名冲突，三种方法应该重载。
阻塞队列的实现也是使用了aqs;阻塞的实现使用了Locksupport.park来实现。
    
 延时阻塞队列：队列里的元素要实现Delayed接口里的getDelay()方法，此外还要有一个AtomicLong 的sequence变量，还要实现compareTo接口，指定比较规则，此外还有一个Time相关的元素，指定时间
            当消费者从队列里拿元素时，拿出后调用getDelay方法计算timer与当前时间的差值，如果小于0，说明timer在当前元素之前，那么就可以获取该元素，否则就阻塞当前线程，当然如果现在有一个线程也在获取头部元素，且在first里标记了，那就阻塞当前线程。线程执行完后调用signal，唤醒其他线程。
适合高并发的队列：SynchronousQueue
  线程之间传递消息的方式：共享内存，以及直接传输两种。Syn和LinkedTransferQueue都是采用了直接传输。
      

一般来说，我们可以设计阻塞队列来实现自己的线程池。实现更高效的线程池
DelayQueue:
      使用了PriorityQueue来实现延时阻塞队列。
计算机网络：
      好好看看。

总结：今天复习了并发编程的知识点。感觉还是缺少实操。弟弟今天说了一个坏消息。我们两个都没有退路，但我们两个前面都不是绝路。如果我俩一直坚持学习，就没事，反之如果贪玩，就会变成绝路，死路。今天看了看并发编程，以后还是要多看MySQL，redis,每天早上起来就看这俩。现在也要包括高可用了。好好看，好好学。今后要注意实操。
        
mysql:今天可以看看高可用.看看高可用该怎么做。redis要看看redisson锁。

mysql:
    读写分离是Mysql应对高并发的第一招，

    mysql的优化参数：
          innodb_buffer_pool_size:   buffer pool的内存大小，可以调节成内存的80%，这样可以减少因为内存不足而刷盘。
          innodb_buffer_pool_instances :buffer pool的实例个数，为了减少并发冲突，当buffer pool大小大于1GB时，innodb会自动将缓冲区划分为多个实例空间。  
          在高并发情况下，这两个参数很有用
          工作线程缓冲区：
            sort_buffer_size: 排序缓冲区，对于优化distinct,group by ,order by 语句很有必要  当排序的结果集大小大于sort_buffer-size时，就需要存到临盘里辅助排序。因此要适当的将该值调大一些，
            join_buffer_size:  对于优化连接查询很有必要。使用bnl算法时，会将驱动表放进join_buffer里，join_buffer放不下驱动表时就要多次扫描被驱动表。
            read_buffer_size:
            read_rnd_buffer_size:用于mrr优化。优化使用索引的语句。
        参数大小如何调整：  对于这些区域，最好根据机器内存来设置为一到两倍MB，啥意思呢？比如4GB的内存，建议将其调整为4/8MB、8GB的内存，建议将其调整为8/16MB.....，但这些区域的大小最好控制在64MB以下，因为线程每次执行完一条SQL后，就会将这些区域释放，所以再调大也没有必要了
          关于排序，还有一个参数：
            max_length_for_sort_data:这个参数关乎着MySQL排序的方式，如果单行的最大长度小于该值，则会将所有要排序的字段值载入内存排序，但如果大于该值时，则只会将排序字段和id放进buffer里，全部排序后，按照id值回表获得完整的数据，并返回。前一种排序称为全字段排序，后一种成为rowid排序。
        调整临时表空间：  
          tmp_table_size:  临时表最大内存大小，超过该值时，临时表会暂存到磁盘里。
            同时还可以调整tmp_table_size、max_heap_table_size两个参数，这两个参数主要是限制临时表可用的内存空间，当创建的临时表空间占用超过tmp_table_size时，就会将其他新创建的临时表转到磁盘中创建，这显然是十分违背临时表的设计初衷，毕竟创建临时表的目的就是用来加快查询速度，结果又最后又把临时表放到磁盘中去了，这反而还多了一步开销。
          调整线程存活时间，和最大连接数：
            max_connections:最大连接数
            wait_timneout:  空闲连接在连接池里的存活时间，默认是8h.
            interactive_timeout:
        join语句该怎么优化？
              算法：bnl, inlj,snlj,bka,其中bnl是对snlj的优化
        二级索引查询该怎么优化：
             read_rnd_buffer_size ：将查询获得的id放到read_rnd_buffer里，排序后回表，将随机io变成顺序io,称之为mrr优化。开启语句是： set optimizer_switch="mrr_cost_based=off"  set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on';
            在join语句的inlj里，就可以采用mrr优化。格式是： set optimizer_switch="mrr_cost_based=off"
            对inlj的查询，也可以用bka进行进一步优化，这样可以对被驱动表进行顺序io。开启方式就是 set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on';此外还要开启join_buffer_pool，mrr排序的字段就是join_buffer_pool里的能容纳的驱动表行的大小，越大，就越可以减少随机io。
            对于snlj，只能由bnl进行查询。
      mrr优化：    
        Multi-Range Read
            Multi-Range Read简称MRR，其目的是尽量使用顺序读盘。
            对于SQLselect * from t1 where a>=1 and a<=100;
            其执行流程是：
            根据索引a，定位到满足条件的记录，将id值放入read_rnd_buffer中。
            将read_rnd_buffer中的id进行递增排序。
            排序后的id数组，依次到主键id索引中查记录，并作为结果返回。
            使用MRR的原因是随着a值的增加，id的值会变成随机的，随机访问的性能较差，但是多少数据是按照递增顺序插入得到的，所以如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。
            read_rnd_buffer的大小是由read_rnd_buffer_size参数控制的。如果步骤1中，read_rnd_buffer放满了，就会先执行完步骤2和3，然后清空read_rnd_buffer。之后继续找索引a的下个记录，并继续循环。这里的过程有点像Block Nested-Loop Join的join_buffer执行过程。
      总结：
            可以看到，mysql的调优主要是内存大小的调优，分为对buffer pool的调优以及针对各个查询的调优，比如sort_buffer_pool,read_rnd_buffer_pool,join_buffer_pool，sort_buffer_pool与group by,order by,distinct有关。

      除了对mysql优化，还有可以对orm框架进行优化，特点就是对orm的连接池进行优化。
        mrr优化，bka优化本质就是代理模式。我们只要开启参数就好。


    主从复制：优化读多写少的场景
          主从复制里的延时问题，
          如果某些业务的一致性要求，延时性要求很高，就必须强制读主库，比如写库后必须实时读数据时，（金融支付后查询支付情况）就要强制读主库。对于一致性必须是强一致性的业务，就要采用读主库的方式。在读主库时，可以采用分库的形式，应对高并发的情况。前边的系统也可以限制操作。比如减少短时间内多次支付。
          主从复制如何避免丢数据：
            可以设置同步方式为半同步复制和全同步复制来避免数据丢失。但这样会导致性能的下降。
          什么时候主从集群？
            应该先考虑sql优化，索引优化，redis优化，最后在考虑读写分离，分库分表。
          大事务：
            大事务的Binlog往往很大，回对主从复制造成影响，影响mysql性能。
          还是要实操
        主从复制的类型：异步复制，同步复制，无损半同步复制，有损半同步复制。建议选择无损半同步复制。此外还有多源复制。多源复制是为了方便统计分析。延迟复制是为了放置误删导致的数据丢失。
        总结：
            1.异步复制用于非核心业务，并发量高
            2.无损半同步复制用于核心业务场景，保证数据的强一致性。
            3.多源复制可以将多个master数据汇总到一个数据库实例进行统计分析
            4.延迟复制可以放置误操作带来的后果。
      
        分库分表：  优化写多读少的场景
              核心：
            分库分表的方式    水平分库分表，垂直分库分表
            分库分表的sharingKey  :即分库分表的依据，由于shardingKey只有一个，当使用别的查询条件查询时就无法获得shardingKey了，那就要将shardingKey映射到其余的字段上，比如根据用户id进行分表，使用订单id查询时，可以将用户id放到订单id最后几位，实现shardingKey.
            分库分表的分片算法： 范围分片，哈希分片，查表法  范围分片容易产生热点问题，哈希分片会影响数据库的伸缩能力。哈希分片也可能出现热点。
            分库分表后的一些查询问题：如count(),就要引入es，为了引入es,还要引入mycat，进行缓存实时更新。
            分库分表后的主键问题：“
                  UUID:首先 UUID 作为数据库主键太长了，会导致比较大的存储开销，另外一个，UUID 是无序的，如果使用 UUID 作为主键，会降低数据库的写入性能。  MySQL InnoDB 引擎支持索引，底层数据结构是 B+ 树，如果主键为自增 ID 的话，那么 MySQL 可以按照磁盘的顺序去写入；如果主键是非自增 ID，在写入时需要增加很多额外的数据移动，将每次插入的数据放到合适的位置上，导致出现页分裂，降低数据写入的性能。
                  雪花算法：Snowflake 算法可以作为一个单独的服务，部署在多台机器上，产生的 ID 是趋势递增的，不需要依赖数据库等第三方系统，并且性能非常高，理论上 409 万的 QPS 是一个非常可观的数字，可以满足大部分业务场景，
                            其中的机器 ID 部分，可以根据业务特点来分配，比较灵活。如果服务器在同步 NTP 时出现不一致，出现时钟回拨，那么 SnowFlake 在计算中可能出现重复 ID。除了 NTP 同步，闰秒也会导致服务器出现时钟回拨，不过时钟回拨是小概率事件，在并发比较低的情况下一般可以忽略
                  此外还有一些分布式Id的解决方案：
    redis看redisson锁：
          redisson在获取锁的时候，如果之前没有获取，就设置锁，锁的次数是1，锁的过期时间，如果锁存在，并且持有者是当前线程，那么增加锁的次数，重置锁的有效期。如果锁已经被别的线程占有，那么返回锁的剩余有效时间。
          redisson在释放锁的时候，如果锁不是自己的，就返回，是，就将锁的次数减1，如果剪完后，锁的次数为0，就删除锁。并使用publish将删除的消息广播给订阅频道的线程。
          tryLock():
                  1.首先会使用lua脚本获取锁，获取锁失败的时候就会返回该锁的剩余有效时间。
                  2.返回后先查看是否已经超过超时时间，如果是就返回false;如果不是，用一个对象来订阅这个锁，这个锁在释放时会publish，
                  3.代码的意思就是在time时间内等待锁的释放，如果time时间内未释放，就会返回false;抢锁失败，如果成功，就可以继续抢锁
                  4.首先计算时间是否过时，过时就返回false;不过时的话，就重新抢锁。但如果抢锁再次失败了，就再次阻塞，这里阻塞的时间根据锁的剩余有效期和自己的朝时期来定，调用aqs进行休眠。
                  5.休眠完成后，继续判断是否到了自己的有效期，到了就退出，不到就计算抢锁。因此用的是循环抢锁。
        watchDog:
     总结：今天刷了一些题，我感觉还是应该按照宫水三叶的刷题单来，而且应该每天中午一道题，一天多个时间段思考它。关于mysql,redis接下来依然要接着读文章，复习之前的知识。开始循环巩固。java也要继续看设计模式，多线程。尤其是多线程，感觉自己多线程这块很薄弱。主要是没有看过实际项目，挺不好的。、
    计算机网络，操作系统，每天看个一两篇文章吧。加油！

今日任务 2024/07/11
    感觉以后早上起来先刷题，看灵神的题解。刷个两个小时。然后是数据库，多线程。

https://leetcode.cn/problems/longest-even-odd-subarray-with-threshold/solutions/2528771/jiao-ni-yi-ci-xing-ba-dai-ma-xie-dui-on-zuspx/  刷题

mysql：不知道该刊什么了，复习吧
redis: 继续看文章！
        关于缓存的问题：关键就是缓存重建（重建时要获取分布式锁，还有未抢到锁的线程怎么办，一种是自旋，查询，还有就是返回旧值。），布隆过滤器，缓存击穿，就用布隆过滤器+设置有效期较短的值，缓存穿透使用重建缓存，缓存雪崩考虑设置Key的有效期。
        缓存的大key怎么解决：采用压缩算法，更压缩的序列化。选择好的数据结构。
        缓存的方式：  更新完数据库要不要写缓存？答案是要么删除缓存（常见做法），要么更新缓存（银行系统常见，但要注意时序性的问题，说白了也是重建缓存，多线程写的问题，可以采用分布式锁+版本号来解决），要么先更新缓存在更新数据库（并发量很大，不是很核心的业务）。
        redLock: 放置主从模式下主节点崩溃导致多个线程都抢到了锁。redLock保证redis加锁时需要争取一半以上的机器同意，才可以加锁，这样即使崩溃了几个机器，也可以保证锁依然存在。
              NPC:  N：network,网络延迟，获取锁后返回时，网络延迟过大，导致锁的释放，可以用锁的过期时间来处理
                    P：进程暂停，发生了GC，GC后锁已经过时了，导致多个进程获得了同一把锁。redLock无法解决。
                    C：时钟漂移，redis的服务器发生了时钟漂移，即key过期了，被删除，使得锁瞬间过期。这点redLock也无法解决。
        综上redLock相较于单一的redisson，（redisson解决了服务崩溃锁无法释放，不可重入的问题。）解决了分布式锁主节点崩溃时多机抢锁的问题，但没有完全解决，在P,C状态下，redLock无法解决。

        redis限流：限制用户访问。
              计数器：
              滑动窗口：
        感觉限流还挺难的。感觉redis这边就是细水长流了，现在要多看看网络，操作系统,jvm的文章。但是redis还是搞的不太懂，我觉得要实操！一边看一边实操。
                缓存相关问题： 已经解决。
                集群相关问题：比较初级
                分布式锁：感觉也可以了，就一个redisson就行
                持久化：也还行，复习一下参数
                场景：不行，其实就是这块不行，这块要学会实操！明天开始实操一下。
          mysql：每天复习一下就行了。还有也是看看orm的框架了，学会实操
          算法：感觉每天刷着，看零茶山的课。不会就过。一天看一个新题就行
          八股：背一背计算机网络了，java基础了。

    感悟：感觉只能早起，五点起来干活，每天可以多8个小时的学习时间。基本可以学完数据库相关的。

  redis:完全掌握基本命令和数据结构，主从集群复制！才能去看场景题。
    redis：根据需求，和内存，选择正确的数据结构。· 
          redis的数据结构会根据类型大小做出不同的实现；redis的value对应的是redisObject结构
        redisObject{
                unsigned  type;  通过 type 查看
                unsigned  encoding; 通过object encoding查看
                void* ptr;   指向底层的数据结构
        }
          type 只有5个：string,set hset,list zset.
          编码类型就是底层的数据结构实现：int,embstr,raw,  linkedlist,ziplist,intset,HT,skiplist分别是整数，简单字符串，动态字符串，双向链表，压缩列表（存在级联更新，但内存小）整数集合（set里面全是整数的时候）HT （hset的底层）skiplist(zset的底层)
      字符串对象：  type位string
              如果字符串长度是39字节，那么用embstr保存这个值，特点是。redisObject和sds将分配一个连续的空间；如果是大于39空间，那么就是用raw格式，特点是sds,和redisObject不在一块了。释放内存时需要释放两次，如果字符串保存的是整数，在Int范围内，也是用int来保存。如果是小数，就用embstr或raw来保存
            命令：set,get,append,incrbyfloat,incrby,decrby,strlen,sterange getrange
      set对象：    编码为int set,hashtable
              如果全是整数，就用int set来保存，int set是升序的，查找时使用二分查找的方式。集合元素对象保存的所有元素都是整数值且数量不超过512个用int set,超过就用hashtable.
                调节参数：
                        set-max-ziplist-entries:
          命令：sadd,scard,sismember,smembers,srandmember,spop,srem,sinter,sunion,sdiff
      list:    编码为ziplist,linkedlist
              元素的字符串长度小于64字节，元素数量小于512；
              调节参数：  list-max-ziplist-value:
                          list-max-ziplist-entries:
              命令： lpush,rpush,lpop,rpop,lindex,llen,linsert,lrem,ltrim,lset
              可以修改指定索引处的值，可以在某个节点前后插入值，可以修剪链表的长度，可以返回指定索引处的值
      hash对象：ziplist,linkedlist
                键，值字符串的长度都小于64字节，键值对数量小于512个
                调节参数： hash-max-ziplist-value
                          hash-max-ziplist-entries
            命令：hset,hget,hexists,hdel,hlen,hgetall
            解析：只能对hash进行增加删除，统计长度的操作；
      zset对象：ziplist,skiplist
                元素数量小于128个，长度小于64字节
                调节参数：
                          zset-max-ziplist-value
                          zset-max-ziplist-entries
                命令：zadd,zcard,zcount,zrange,zrevrange,zrank,zrevrank,zrem,zscore
      就是这些个命令，所有的场景都是用这些个命令来实现的。
        剩下几种数据结构：
                    bitmap,geo hyperlog    bitmap用于海量数据打卡，签名等，geo用于地理纬度查询，hyperlog用于粗略估计总量
              bitmap:    setbit key offset value
                          getbit key offset  
              hyperlog：pfadd key value  pfsountc key  
              geo: geoadd  geosearch fgeoradius\
                              geoadd city 121.47 31.23 "上海" 116.41 39.90 "北京"
                              geosearch key 
                             geopos city 北京
                             geodist city 北京 上海 km
                            georadius city 121.48 31.23 10 km withdist withcoord withhash count 10 desc
                             georadiusbymember city "上海" 10 km withdist withcoord withhash count 10 desc 
        redis数据类型就这么多。所有场景都根据他们来实现。
        redis本身数据结构：
          redisServer{
            redisDb* db;
          }
          redisDb{
          dict*dict;
          }
          dict{
        dictht ht[2];
          }
          dictht{
          dictEntry * *table;
          unsigned long size;
          unsigned long sizemask;
          unsigned long used;
          }
        dictEntry{
          void* key;
          union{
          void* val;
          uint64_t u64;
          }
        dictEntry *next;
        }
        //骑手抢单
              有一个单，扫数据库订单表，找到，设置。自己的名字。修改配送状态。可以使用拉模式。骑手的app定时查询数据库，获得到后，使用乐观锁修改。这样性能不好；
            写完数据库后，同步到redis,这里会有时序的问题。骑手自己的程序轮询，但这样访问量很大，因此应该采用推模式。有一个系统定时轮询。

      接着看redis的高性能，高可用，今天彻底结束。
        主从复制，哨兵，集群，现在主要看集群。
          集群： cluster meet ip port， 会将ip port对应的节点加入到nodes所在的集群里
          启动集群时需要更改配置文件里的cluster-enabled，改为yes，否则无法使用集群模式。集群根据槽来确定数据是否要存入自己的节点，reids采用hash槽的形式相对改善了
          集群的伸缩能力，因为他会自动的进行数据迁移。
          槽的使用：redis服务器接收到key后，使用hash算法转换为hash值，然后hash%16384得到数据所在的槽，接着查询自己的char slots[16384]。看是否为1，1说明节点在自己的
          处理范围内，为0就要查询clusterNodes *slots[16384]，找到索引对应的主节点值，再次查询。接着返回结果。
          redis今天就到这里吧，看看别的，http吧。http感觉也没啥用呀，就把那些文章看完就行。感觉redis的集群，还是很难判断的。
      现总结主从复制和哨兵，我们要明白他们会出什么问题，他俩都是应对读多写少的请求的。集群是应对海量数据写多读多的请求的。
    
    http:
          http将数据类型分为八类，八类下边由很多子类格式：type/subtype。如text/html,就表示是文本类型里的html文件。image/png就表示是图片类型里的png格式的图片。
        对于未知数据类型，采用application,如application/octet-stream就表示是二进制数据。此外对于大文件，还需要进行压缩；指出采用了何种压缩算法。即encoding type;
        常见的有gzip,deflate,br等
            http里的字段： accept:  accept-encoding: content-type:
          语言类型： accept-language:   content-language:     accept-charset:  语言的编码格式放在content-type里。
          Connection:表示了希望之后的连接该怎么处理，keep-alive表示之后应该继续保持连接；close表示关闭连接。
          服务器主动断开连接的方式： keepalive_timeout,keepalive_requests，前者表示一段时间内没有数据收发就主动断开连接，后者表示长连接上的最大请求次数。 这是在nging里边的设置；
          其实tcp本身就是可以主动断开连接的。但在http里服务器只能在自己的配置文件里面进行设置。
          重定向：http服务器的响应报文的headers里出现了Location:的字段，它的内容就是客户端应该重定向的uri,这个uri既可以是相对uri,也可以是绝对uri,相对uri省略了scheme,host:port,只有
        path,query。
              区别：站内条转需要使用相对uri,站外跳转需要使用绝对uri
          重定向的问题：性能损耗和循环跳转。
        Cookie:    
                请求头里由Cookie,响应头里有set-cookie;set-cookie的字段是：key=value; 请求头里的字段是 cookie:key=value结构。 cookie由浏览器存储，服务器和客户端浏览器个有一份
        cookie存在生存周期，他的有效期由expires,max-age两个属性来设置。“Expires”俗称“过期时间”，用的是绝对时间点，可以理解为“截止日期”（deadline）。“Max-Age”用的是相对时间，单位是秒
浏览器用收到报文的时间点再加上 Max-Age，就可以得到失效的绝对时间。Expires 和 Max-Age 可以同时出现，两者的失效时间可以一致，也可以不一致，但浏览器会优先采用 Max-Age 计算失效期。
        其次cookie有作用域，domain,path属性制定了cookie所属的域名和路径，浏览器在发送cookie前会从uri里取出host,port对比cookie的属性，不满足条件时，就不会再请求头里发送cookie.
        此外cookie还有安全性，httponly告诉浏览器cookie只能通过http协议传输，secure表示cookie只能用https加密传输。
        http的缓存：cache-control字段，属性有max-age;max-age=30表示相应的数据将缓存30s。时间的计算起点是响应报文的创建时刻即离开服务器的时刻。
                    此外还有 no_cache字段，表示可以缓存但使用前必须去服务器验证是否过期（即还要发送http请求），no_store表示不允许缓存。 must_revalidate表示过期后如果还想用就要去服务器
      验证。
              对于缓存，客户端浏览器也可以在请求里添加缓存相关的请求，比如cache-control:no-cache,表示不希望缓存。而查看是否使用缓存需要根据请求头里的status-code，如果后面有from disk cache；
      就表示是缓存。
              条件缓存：  if-modified-since,if-none-match  ,需要使用上一次服务器响应时的数据 last-modified,和ETag,
      代理服务器：服务器会有一个代理，缓存静态文件，常用的就是nginx,代理可以解决跨域的问题。以及安全地问题。还有负载均衡的问题。常用的字段就是Via字段。通常情况下服务端需要知道用户的ip地址，
      因此http里面出现了 X-forwarded-for ,x-real-ip两个字段。后者就是客户的真实ip，此外还有origin,via字段
        感悟：还需要了解加密算法，但我觉得一天时间花在数据库上是没问题的。
      如何伪装redis的客户端，执行info replication;可以减少主从节点的数据不一致情况，
      redis主从复制的缺点，没有复杂均衡和轮询的中间件，可能出现数据不一致的情况。哨兵机制，如果哨兵挂了怎么办

    感悟：明天早上继续看集群。要彻底看明白，然后过几天实操一波，继续实验mysql的加锁。我觉得明天可以把集群结束了。然后每天复习。java就可以复习一些基础和框架就行了。除此之外呢，还有一些，但主要就是这个了
  ，也就是redis，剩下还有并发编程。但可以接着练练redis的数据结构。不同数据结构主要怕是
    今日任务： 2024/07/13 
    redis:看明白集群。
    创建集群：     redis-cli --cluster
    访问节点：      redis-cli -c -p port 
    查看集群：     redis-cli -p cluster nodes
    添加集群节点： redis-cli --cluster add-node new_host:new_port existing_host: existing_port --cluster-slave  --cluster-master-id <arg>  默认是主节点
    插槽分配：      redis-cli --cluster reshard ip:port         x(移动的插槽的数量)      id(接受插槽的id)        source(槽的来源id)  done(结束)     yes（是否要移动插槽）
    集群的数据迁移类似哨兵，不过是自动完成的。
    故障转移命令:cluster failover  在从节点执行，执行后会直接将从节点变成主节点，主节点变成从节点，数据迁移工作
    ask错误：发生在槽迁移期间的错误，是一种临时方案
    moved错误：发生在正常运行期间
    选举新节点：当某个主节点状态时fail时，发现fail的主节点会在集群里广播，收到消息的fail节点的从节点，就会进行广播，要求处理槽的节点给自己投票。票数超过N/2+1时，该从节点就成为主节点。
    PING:集群里的节点每隔1s就从集群中随机挑出5个节点，对这五个节点中最长时间没有发送过ping消息的节点发送ping消息。以此检验该节点是否在线。当然一个节点一个可以直接广播pong消息，来更新时间。
    aqs看源码
        REENTRABNTLOCK:
              非公平锁：只是用cas,因此锁的性能要好一些，此外，非公平锁是非阻塞的。
              公平锁：加入了同步队列，
  jvm:
      查看字节码文件： javap -verbose PCRegisterTest.class
      设置栈的大小 -Xss1m  -Xss256k 虚拟机的大小是1m;  设置前需要先添加 ADD VMoptions
      每个方法都对应着一个栈帧。栈帧里面包括局部变量表，操作数栈，动态链接，方法返回地址。
        局部变量表是一个数字数组，最基本的存储单元是Slot(变量槽)，局部变量表存储的是各种基本数据类型，对象引用，returnAddress类型，局部变量表所需的栈帧大小是
        局部变量表里，32位以内的类型占一个slot,64位占2个slot,
        编译器已知的，保存在maximum local variables数据项里。也可以称为locals，stacks代表了栈的深度；
        LineNumberTable: 行号指的是代码里的行号，起始PC指的是该行代码在字节码里的起始行号。
        LocalVariableTable: start pc:变量作用域的起始字节码，length:该变量在多少行字节码内是有效的。两个参数一起限定了变量的作用域。
        在栈帧中，与性能调优最为密切的就是局部变量表，局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中变量直接或间接引用的对象都不会被回收。

    设置堆内存：  -Xms20m -=Xmx20m 最小内存20m,最大内存20m
          内存监视：jvisualvm.exe，位于jdk的文件内。
          堆内存里新生代和老年代的分配比例： -XX:NewRatio=2 表示新生代占1，老年代占2，新生代整个为1/3；它代表的是老年代是新生代的多少倍。
                          新生代里伊甸园区和两个幸存者区的比例：默认为8：1：1，参数为 -XX:SurvivorRatio=8，表示伊甸园区占比为80%

    感悟：每天都要看设计模式，写设计模式！这个太重要了。

    总结：明天要接着看Mysql,redis，redis尤其要做好命令的记忆和熟悉，然后多练。多看配置文件。并发编程继续看源码，看aqs和多线程的源码，mysql要加强实操。jvm发狠的看，看完jvm基本就
    大功告成了。redis要多看场景的使用，redsiion的源码。

今日任务
    jvm：继续听课，看书，尤其是记住参数，感觉其实也不难，就是毕竟比较多
          方法区大小设置参数：-XX:MetaspaceSize  -XX:MaxMetaspaceSize,在Windows里前者是21M，后者是-1表示没有大小限制，在运行时，-XX:MetaspaceSize会随着gc反复变化
          赋值：-XX:MetaspaceSize=100m
          查看运行时方法区的大小：jinfo -flag MetaspaceSize pid
          方法区主要包括类信息和运行时常量池，静态变量，即时编译器编译后的代码缓存，field信息，Method信息；使用cglib,proxy时，会产生大量的临时类，即动态生成的类，在没有用的时候应该予以回收。  
          类信息包括：类型完整的有效名称，直接父类的完整有效名，修饰符，这个类型直接接口的一个有序列表。
          field信息：field名称，类型，修饰符，field信息的存储按照生命顺序
          methdo信息：名称，返回类型，修饰符，参数的数量类型，字节码，操作数栈，局部变量表大小等。
  感悟:今天就当放假吧，早期比什么都重要！以后要接着看并发编程，数据库，设计模式，Jvm，加油！

今日任务：
    jvm:听方法区的知识
    java并发编程：看unsafe魔法类；
    java设计模式：复习两个设计模式
    数据库：
            mysql:看分布式id的方案；
            redis：看数据结构和场景。
今日任务：
      1.juc：继续学习；包括设计模式，查漏补缺。看了看unsafe类，没啥感觉，还不如看看阻塞队列，异步执行。
      2.io：看小林哥推荐的
      3.juc:
            Future:future接口时阻塞式的，也可以叫做观察者模式，当被观察者状态发生变化时，主动通知观察者，回调，AIO皆是基于此创建的。被观察者要包含观察者属性。
      CompletableFuture可以进行异步任务的编排组合，而获取结果的get方法依然是阻塞调用的。因此他并不是观察者模式。我们可以用外观模式来形容completableFuture接口
      那么该怎么做呢，就是说可以任意安排顺序，这又可以叫做桥接模式。我们可以在里面形容为观察者模式，即一个线程的状态改变后，就通知另一个线程，或者可以用中介者模式
      感觉复杂的相互协同工作，应该是中介者模式。同时每一个执行方法的方法体都有一个任务，应该是模板方法模式。
          1.如何传递结果。传递结果时，需要用到抽象工厂模式。
          2.
          /**
           *      subMit，execute
           *      Async的区别就是执行方法的线程，Async时可以自己制定或者ComeplatableFuture内定线程，否则就是主线程
           *      thenApply Async    Function  有参数有返回值
           *      thenAccept Async  Consumer      有参数，无返回值
           *      thenRun  Async     Runnable         无参数，无返回值
           *      thenCompose Async          对传入的参数有类型限制
           *
           *      Consumer可以接收前一个任务返回的结果，但本身没有返回值，Function则可以接收参数，也可以返回，Runnable就是纯的执行逻辑。，Callable则是没有参数，有返回值。BiConsumer可以接受两个参数，但是没有返回值。
           *
           *      任务组合：
           *      thenCombine Async   对任务的结果进行整合。也是异步的。
           *
           *      runAfterEither,  两个任务任意一个执行完了就开始执行第三段逻辑
           *      runAfterBoth,  两个任务任意都执行完了就开始执行第三段逻辑
           *      allOF(...).join():所有任务都执行完了才会往下执行，
           *      anyOF(...).join(),任意一个任务执行完了，就会往下走
           *
           *      getNow();可以指定默认结果；
           *      whenComplete(): 给每个任务注册回调逻辑
           *      execeptionally() :注册出现异常时的逻辑；参数是一个异常。
           *      complete():直接让任务完成。
           *      cancel:取消任务。如果任务已经结束了，那就没有用，如果正在执行，未执行，会在获取结果时抛出异常。
           */
              看一看completableFuture的源码，弄明白了他采取了策略模式和观察者模式，明天可以继续看，晚上看些什么好呢？晚上看java io吧，
              今天可以看一看kafka的源码！
            今天晚上看java的io，学会使用kafka；
            java io:
              操作系统的io：操作系统在进行Io时，堆内存进行了划分，在硬盘和内存见还有一层内存，他和硬盘间的读取是以块为单位的（使用dma）。在Io时，数据从硬盘以块的形式放到这个内存里，再以另一种速度放到
            用户空间中。bio的read,writ都是一次读写一个字节，buffer则是一次一个缓冲区。bio使用的是cpu里的寄存器进行读写操作，即数据先到cpu,再到内存，因此一次只能是32字节。
            此外当需要大批量操作时，可以使用dma，dma以缓冲区为单位进行数据拷贝。将数据直接拷贝到用户空间里。这样一可以减少cpu的使用，二可以加快数据传输的速度。此外操作系统还提供了map接口；将缓冲内存的
            地址直接提供给用户空间，用户修改后直接刷入磁盘，这显然更快，但也非常危险。因此dma也是nio高效的基础。在传输数据的时候，还可以使用dma进行零拷贝。全程都以块为核心进行传输。
              此外，对于java来说，数据拷贝流程是：disk-内核缓冲区-native buffer-heap buffer，可以看到相同的数据传输了三次；使用dma后，还是要拷贝三次，但这时是以dma，以块为单位进行拷贝
              ，但是当使用map时，不需要拷贝，直接修改kernel buffer里的数据。使用零拷贝的时候，也是通过两次dma，将kernel buffer里的数据直接放到硬盘里。这也是为什么buffer比directbuffer慢，
            directbuffer比map()慢，而零拷贝是最快的。
                因此nio相比于bio，更多的使用了操作系统提供的api,而异步Io，则在api的基础上增加了设计模式里的命令模式，来进行回调操作。
                        在java里，通道就是对dma。cpu传输数据的抽象。
                  只分为5种,在单线程模式下是一样的，但是在多线程模式下，这是很有意义的，可以用多个线程相互协作。我们在思考io的时候，就看他属于哪一种，除此之外
            还有它对系统调用的使用次数，因为每一次使用都会导致一次上下文切换。
                  1.io类型。 2.系统调用次数（对应上下文切换和系统调用次数，系统调用次数还受到该Io是字节类型的还是块类型的传输数据方式）3.内存拷贝次数。4io的两端
            bio：
                bio往往比较低效，首先他会阻塞主线程，其次他是一个字节一个字节的读写。因此系统调用次数很多，但bio可以根据两端分为:
                字节流：一次读写一个字节，内存io(不用序列化，ByteArrayInputStream同一个线程内通信,PipedOutPutStream两个不同的线程通信,)，网络io（socket不同的线程，进程通信）文件（需要序列化，FileInputStream,BufferedInpurtStream，ObjectInputStream，DataOutPutStream）,
                字符流:  一次系统调用读写一个字符。
            NIO:
                    NIO采用缓冲区，选择器，通道三种改良措施，通道可以双写，且通道不必拘泥于两端，Buffer的存在可以将数据直接读取的缓冲区内，BIO中是先调用read()方法,再将read方法的结果写到目的地中，因此Buffer可以减少内存拷贝的次数。
            选择器可以同时监听多个Channel,因此BIO主要用于网络io里。此外Nio还是用了零拷贝的技术。
              在nio里，可以划分的只有Channel,它分为四类：
               FileChannel：从文件中读写数据；
                DatagramChannel：通过 UDP 读写网络中数据；
                SocketChannel：通过 TCP 读写网络中数据；
                ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。   
              Buffer: Buffer的存在可以减少数据拷贝次数，比如DirectBuffer直接将数据从内核空间复制到设备，除此之外，Channel还实现了基于块的拷贝，这样也可以加快拷贝速度。DirectBuffer相当于直接使用内核空间，它是将
              用户空间与内核空间进行了映射。
              io多路复用：
                    selector可以绑定多个channel,实现一个线程监听多个应用。但是这种轮询的方式非常的慢，redis采用了epoll()的方式。添加了红黑树以及回调函数来处理。

              感悟：今天看明白了java的io底层原理，收获还行，但没有实操。还要继续学习，明天继续看javaio和多线程编程。每天早上五点起来看！此外明天可以看一些java8新特性了！

      今日任务：
  mysql，看吧，看吧
      子查询优化：
                1.子查询分为标量子查询，行子查询，列子查询，表子查询。此外还可以分为不相关子查询，相关子查询。
                2.操作数 comparsion_operator 子查询时，子查询只能是标量子查询或行子查询，
                      in/any/some/all子查询时，可以是列子查询和表子查询。比如：操作数 in 子查询，操作数 comparsion_operator any/some/all 子查询 any,some效果是一样的，表示任何一个为true整个为true，all表示必须全部为true，才为true,
                3.子查询执行顺序：子查询分为相关子查询和不相关子查询，in 类型的不相关子查询可以转化为join,或者物化表的方式进行优化。而相关子查询不能优化。优化的方式根据子查询结果是唯一索引，或者有索引，无索引来定。当是唯一索引或者主键时，子查询将直接转化为标准的内连接。
                当查询结果是普通索引时，将采用loosescan的方式，只扫描索引，且只扫描每个值的第一个，反过来去看主表里有没有记录满足，满足就放进，不满足就丢弃；当不是索引的时候，会物化为临时表进行连接或者将主表里的id创建一个临时表，来去重。可以理解为没有索引的时候会创建临时表
                性能损耗比较大。
                          相关子查询的创建条件： in子查询，且in子查询最多只能与与and连接。不能使用union,group by 等数据。不满足该条件时，会先将子查询物化，在建立连接。
                4.对于相关子查询，先取外层查询的一条记录，到子查询表中寻找符合匹配条件的记录，如果能找到，就停止匹配，并将外层查询放入结果集中。知道外层查询全部遍历完。类似于snlj，性能最低，此外还会将相关子查询转换为exists查询，此时可能用到索引。
        连接查询里的控制拒绝：通过在where里设置被驱动表非null的条件可以将外连接转换为内连接，便于优化器优化执行计划。
      分布式事务：
              XA协议的两阶段提交，三阶段提交，基于消息的最终一致性。。前两者遵从强一致性，后者遵循最终一致性。
      看明白分布式事务和spring事务，以及mybatis的执行逻辑。
SPRING事务：
        切面逻辑：
              判断@Transaction注解是否存在->使用spring管理器新建一个数据库连接->set autocommit=false->（mybatis，jdbc拿到上面创建的连接，利用该连接执行sql）->commit或者rollback;
      消息队列的使用：
          下午学一下zookeeper和kafka，下午学会。
      kafka的好处：
        1.同步变异步，微服务远程调用时，如果不是必要，可以采用异步的模式，将参数写入消息队列。增强系统性能
        2.请求缓冲，生产者消费者模型中，双方的性能并不完全一致，此时消息队列可以平衡双方的性能，不至于出现过于忙碌的状态，
        3.数据分发，有的时候，在数据库系统中，可以将Binlog放在消息队列里，不同的canal从消息队列里拿log，这样可以增加主库的写性能。主库不用等所有从库同步完成再回去执行写操作。（类似于第一条，减少远程同步调用的性能损耗）
        实际上网络通信的双方，当双方的性能差距巨大时，或者说一个业务流程里，出现短板，瓶颈时（比如mysql与redis）都可以用消息队列进行平衡，比如秒杀时，通过jvm，大量的请求需要访问redis,mysql，这就出现了极端的差距，就可以用消息队列进行平衡，具体就是redis和mysql之间。通过redis获得了数据，就直接将
        相关信息写进kafka里，然后异步的交给消费者进行消费，执行数据库写操作。
        消息队列的特点：异步，削峰，解耦，
          为什么要使用消息队列，其实是在问，如果不削峰，不异步，不解耦，会有什么问题。
          1.使用消息队列的情况：
        1.执行一堆重量级操作时，如多次远程调用时。
        2.扩展性，增加下游代码的灵活性。让上游的操作与下游完全解耦。比如远程调用的时候，如果之后不需要这个调用了，或者下游的代码出现改动而使得上游出现bug了，那就要去生产者那边改代码。这样肯定不好，因此直接放在消息队列的好处就是让上下游完全解耦。
        3.可用性：消息队列可以保证在部分成功，部分失败的情况下，系统依然可以正常运转，同步代码则只能忍受全部成功的情况。
        4.事件驱动：消息队列里有一股浓浓的命令模式的感觉，即相互调用，比如假如消费者本身也是生产者时，就可以使用消息队列来降低代码的复杂性，因为命令模式，中介模式往往比较复杂。代码不好维护。使用消息队列可以加强程序性能的同时，降低代码的复杂性。
            当一些业务需要多步骤，多机器协同时，就可以使用消息队列了。事件驱动本身也是异步模式。
          远程调用时，谁知道会不会该功能，所以应该统一交给消息队列来处理。通过消息队列来实现观察者模式。
//什么模式呢？策略模式呗。
    感悟：学号kafka,真正开始做项目！
