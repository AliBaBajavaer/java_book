这辈子所有的任务清单都会写在这个文件里面。写多了就分库分表，yes！

今日任务   2024-06-26 更新于 2024-06-27/00:30
  1。mysql:看5篇文章，还要复习mybatis里是如何与mysql交互的。                                          没有搞定，要多实践，多调优。
  2.redis:复习各种数据结构，先学这个。                                                              搞定，还要多复习几遍
  3.java:继续复习八股，练习注解的使用，一定要练熟。练透                                                   完成
          Java里juc并发编程，学透AQS部分，可以自己写一个AQS同步器，弄清楚thread源码                  先看aqs同步器 ，看懂aqs同步器。没有看Thread的源码，按照训练营来。       
          JAVA里的设计模式，学习两个设计模式，尤其是单例模式                                        学了单例模式，工厂模式，抽象工厂模式 
          JAVA集合：搞清楚list，treeset                                                                          没有做
  4.算法：2道算法题，复习一道LRU，还要联系多线程编程，看Thread源码                                  刷了三道题，看了一道单调栈的题。
  5.操作系统：先学前边三个。                        
  6.计算机网络：没想好，先学前边三个，主要是要弄会三个网络模型，

其实今天，再来十几个小时，还是可以学完的。所以要早点休息，早点起床。


今日任务 
  1.mysql: 重点了锁部分的文章。继续看3~5篇文章。     今天就看五篇吧，尤其是要弄明白explain,还有mysql背后的原理，高可用这些不是我们该管的 没怎么看，看了子查询部分，复习了查询方法和explain
  2.redis: 昨天看了数据结构，今天接着看                         今天接着看一些命令，底层原理
  3.java: 并发编程看完了aqs，解这看别的。看3篇文章.   搞明白了threadlocal，map的并发编程。别的啥也没搞懂，下午再看看map,和其他容器；哦对还看了condition对象，LockSupport.HashMap源码没有太看明白，得接着看，要看明白的源码就是java集合，java并发编程。这是7月份必须完全看明白的源码
          设计模式：学习两个设计模式                            
          java基础：继续看javaguide的八股文。             学习了java数组，java集合等         
          集合：理解List集合的源码，写成文章。
          java io好好学。理解nio；           今天必须好好理解java io;完全理解透彻。今天的重点，今天算是理解了nio,bio，以及常用的bio包装类。
  4.算法：刷两道算法，复习以前的算法题         连续的子数组和  刷了两道。自己ac了两道，还有两道是抄的，其中一道太简单了；
  5.操作系统：主要要看io.这个对理解很重要。    
  6.计算机网络：                                  看看tcp/ip 一篇就行。


今日任务
  1.mysql:继续复习锁这一章节。搞清楚explain。         
  2.redis:看文章                                    看了哨兵机制，主从机制。
  3.java:
        java基础：复习知识点，                            
        jvm:虚拟机看50节课                        学习了一些
        java并发编程：接着刷题，每天都要刷题和看源码，尤其是锁       搞清楚了concrrenhashmap,linkedblockingqueue,以及一些阻塞队列。
        java集合：搞清楚hashmap,                   
  4.算法：看零茶山的一课，全部看完。刷两道题。                    刷了两道，感觉一天刷个两小时就行，睡觉吧。明天三点半起来学习。
  5.计算机网络：学习tcp/ip
总结：最近没怎么写项目，但我也觉得，没必要重复这些老项目，没一点意思，有这时间还可以多看看呢.加油陈乾珲，弟弟也不跟你说话的。晚安吧。我买了个长毛毯。看了海贼王路飞去伟大航路一集，太好看了，上次看还是12年左右吧，哈哈哈哈。我人生
最自主的一年半，二年级下学期到初三下学期。之后就是行尸走肉了。哈哈哈。好好学习呀！加油加油陈乾珲，以后就是自己和自己聊天。

  这是写下的日子，这辈子所有的任务清单都会写在这个文件里面。写多了就分库分表，yes！
今日任务：2024-06-28   刚把爹！
  java:
    java并发编程:看明白阻塞队列                 看完了阻塞队列和线程池，延时线程池和futuretask还是没有看明白。
    java集合：继续看源码：                
    java基础：慢慢看八股  
    jvm：看类加载器，这方面一天三个小时吧。也不花太多时间 毕竟东西太多了。
    java设计模式：复习一两个设计模式。
  mysql:看锁这一章。                          开始看锁。基本看完了锁，但是掌握的还不是很透彻，需要接着看。我知道了一点，那就是加锁是给扫描区间里的记录加锁。
  redis:循环看学习指引的文章。                  
  计算机网络：循环看文章，3篇                  看了tcp/ip连接。还要接着看
  操作系统：循环看文章，3篇

  我要专注于每一分钟，没必要思考以后的事情。对于我来说，过好每一个小时自然就能过好每一个月。每一年。其实我也发现了，每天还是要复习，因此以后起来的第一个任务就是复习。感觉明天可以
接着看锁这一部分。以及java的线程池框架。还有redis的数据结构部分。这十天就看redis的数据结构部分。Java基础的话。也是每天轮着来，三个小时吧。算法要多刷。多刷，多刷！jvm就按明天的来。

今日任务：

  redis: 数据结构又过一遍。                  明天可以看看redis的网络模型
  mysql：看了锁这一章，明白了加锁的流程，但是精确匹配还没搞明白，以及锁是怎么退化的
  jvm:就看训练营的那些吧，会就行了。      没有看 
  java设计模式：看一两个；               没有看，看一个吧
  java多线程编程：接着看线程池部分的。   线程池应该说阻塞队列这块不行，还有futuretask这块也不行，得多看几遍。看看阻塞队列吧
  算法：刷了几道简单的滑动窗口。明天接着刷滑动窗口。还要听一听灵神是怎么做的。

    线程池：
            为什么线程池的线程不会回收，很简单，线城池的线程是worker对象，他的run方法是死循环，永远不会回收。除非关闭线程池。
            线程池包括一个list对象存放线程，一个阻塞队列存放任务。因此线程池应该有一个主方法。用来进行判断。阻塞队列里的元素类型必须为runnable,或者Callable,后者相比前者可以返回
            线程的结果。
            线程池的参数：
                          1.核心线程数
                          2.最大线程数，阻塞队列，存活时间，时间单位，拒绝策略。这一切都围绕着线程池的处理流程来展开
            核心流程：当任务被提交时，是通过线程池的execute方法提交的。而不是直接放到阻塞队列里。如果线程池里的线程数并不到核心线程数，那就创建一个线程来处理他。如果达到了。
                      就放到阻塞队列里。如果阻塞队列满了，在线程数没有达到最大线程数的情况下,创建一个线程来执行他。该线程在存活一定时间后会被销毁。如果线程数达到最大线程数。
                      那么任务就会被丢弃。
            提交任务的方法：execute（runnable）,此时没有返回结果，而submit（callable）是有返回结果的。
            关闭线程池：    shutdown,shutdownNow,他们都是调用interrupt方法中断线程，区别在于，shutdown只会中断没有运行的线程。
            查看线程池是否终止：isTerminated();
            线程执行结果的获取：Future或者FutureTask对象来获取执行结果，通过object.get()方法来获取。后者可以对线程进行操控。这里的线程池采用的是JAVA传统的线程方式。
            FutureTask可以用来协调各个线程的工作.
            根据线程池的参数不同，会分为很多不同的线程池。关键在于阻塞队列的选择，拒绝策略，此外可以对线程池进行继承，实现自己的线程池。达到不同的效果。比如延时线程池。固定线程池
          不同的线程池，并发效果是不一样的。比如cachethreadpool的并发能力最大。因为它可以创建无数的线程。
              线程池中线程的个数根据任务类型，cpu数量来定，io密集型应该使用2*Ncpu;cpu密集型应该采用Ncpu+1来做。
            java本身提供了四种线程池，前三种根据线程池线程数量区分，最后一种可以执行延时任务。
  线程池的创建采用了Execute框架。

今日任务：  从今天开始都要写记录。
    mysql:接着看文章，看两三篇吧，看一看mysql的逻辑架构，和引擎；redolog,undolog,          看了redolog,undolog，binlog
    redis:看网络模型那一章，                                                               复习了持久化
    java并发编程：好像都看完了。得复习一下，拔高以下，看看应用                             又看了看线程池，更理解了但是没有看futuretask
    java设计模式：每天两个设计模式                                                        以后每天起来先看设计模式
    java基础：接着看java基础                                                              没有看
    jvm：学习                                                                            学习了类加载机制，双亲委派机制等
    算法：刷双指针，每天刷一些sql题。                                                      刷了两三道滑动窗口的题。
    计算机网络：看tcp/ip一节                                                               看了tcp/ip的拥塞，滑动窗口，以及rpc协议，websocket格式
    操作系统：看小林coding                    没咋看。

    mysql加锁：mysql无论是什么语句，在确定了扫描区间后就会进行加锁。正常的枷锁流程就是确定扫描区间然后逐个加next-key锁。但在一些情况下锁会退化。我们要注意这些特殊情况。
    redis持久化：redis的持久化通过aof,rdb来实现。在这其中要注意写时复制技术。是实现高性能持久化的基础。
    java并发编程：难点在于能不能自己手写一个高效的线程池，应对不同的并发请求。只要涉及到生产者消费者模式的。都可以使用ree锁，其他就用syn锁。书写简单。
    mysql日志：  怎么理解Undo，redo，里面存的是数据，所以其实就是数据页的备份，只不过redolog寸的是多个版本的数据的备份，redolog和undo寸的都是和事务相关的数据。rollback时把undolog拿出来刷盘，恢复时把redolog拿出来刷盘。那怎么主从复制呢，用Binlog，事务是否提交以binlog为主，怎么判断事务是否需要恢复，
  就是看binlog和redolog里的事务id，事务id可以在Binlog里面找到，就恢复，找不到就去undolog里进行回滚。
  ·  一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id：

          通过 trx_id 可以知道该记录是被哪个事务修改的；
          通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链；
    那么undolog如何持久化，undolog本身也是数据页，undolog通过redolog持久化。
  什么是redolog？
    redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。
    当事务提交的时候redoLog一定会刷盘的。事务提交时binlog也会刷盘的。redolog可以实现顺序磁盘写。这一点比随机Io效率要高得多。此外redolog也会有其他的刷盘措施。
  
  我觉得我应该做做项目了。基本都看明白了。以后要猛攻网络，网络是架构的基础。





我感觉主要是白天的运动量太小了。然后天天看短视频，其实我很早就开始睡不着了，主要是独居引起的精神兴奋。然后其实我白天并没有很充实的过着。因为我起的太晚了。就是这样，寒假在家我也睡得很好。长期睡不着就会导致昼夜颠倒。嗯嗯，也不是加油，我觉得我需要做出抉择。技术大咖还是混混。只会扣手机的人
我陈乾珲，于2024年7月三日凌晨1点27分宣布，从今开始不再看贴吧，海贼王，哔哩哔哩，只刷算法，看小林，学Java，跑步，睡觉。除此以外什么也不干。有时候就听听音乐。作为消遣，对我也不看什么房子，什么都不看。只看java，戒掉短视频。




今日任务： 2024-07-03
  java：复习了几种设计模式，学习jvm：待会吃完饭回来学。
  计算机网络：要写文章才行，每天看完要理解一下，这个估计得一个月。慢慢看吧，每天主要看tcp/ip；我们还要看看协议怎么拟定。
  java并发编程：看了cas原子类。以及并发编程常用工具。
  mysql：
      1.看buffer pool 
      mysql的buffer pool用三种链表来管理buffer pool的内存空间。通过后台线程定期的刷新脏页。通过redolog来保证数据宕机也可以恢复。以及mysql的lru算法的实现。
      buffer pool 的内存大小是128mb,可以通过参数调节。
      2.看一看mysql运行时的内存结构，搞明白各个分区都是干什么的。相比于其他技术，mysql是目前进展最好的。要继续努力！
      3.mysql的行记录模式，搞清楚是怎么存储的，一行的最大能占用多少字节。溢出列怎么处理。
      4.看一看buffer pool里的change buffer
        写操作时，如果更新页不在内存里，会将数据写在change buffer里，然后定期的merge，异步的写入到磁盘里，当然，如果访问了数据所在页，也会自动触发merge。处理读大于写的任务时，普通索引加上change buffer可以很好的满足大量
  写入的要求。
      5.select的执行流程
          连接器（连接池，每一个连接都对应一个线程）   解析器（语法，词法解析），优化器（建立执行计划），执行器（与执行引擎交互，单位是记录。执行引擎根据扫描区间拿到记录后返回给执行器，执行器判断是否符合查询条件。如果查询条件就是索引，那么判断会直接在引擎层执行。） 执行引擎
      6.mysql内存结构，看一看吧。
      7.看懂了两阶段提交，和order by的优化方式。
感悟：这些唯一两个狠抓的就是mysql和redis，每天循环的看，好好看！今天mysql没什么长进，我觉得可以一遍遍看吧。


    
今日任务：
    mysql：继续杂七杂八的看，但我觉得还是应该接着搞懂mysql的逻辑结构，再看看B+树。 目前看了为什么存储不能超过2000w,因为层高三层较为合适。四层会有磁盘Io。看了B+树的储存。我觉得我可以看看别的，总结一下sql语句的执行流程。主要就是各个sql语句的执行流程。
          一个知识点就是Mysql的各个查询都是怎么工作的。
                  select ,order by ,group by,distinct,union,子查询，join这几个的流程，会采用什么算法，怎么优化。
            order by 记住filesort_buffer，
            join: nlj,bnl，bka,hash join四种方式，Bka本质上优化的是回表以及对驱动表的扫描次数。被驱动表根据索引获得的id是随机的。那么就可以使用bka，mrr对id进行排序，这样就可以优化了。因此Join查询最好就是bka,其次是inlj，bnl，最差是snlj，记住选择小表作为驱动表。
                  inlj：前提是查询被驱动表用到了索引。那么流程：
                              1.扫描驱动表的记录，获得符合查询条件的记录，
                              2.根据被驱动表的索引来查询。获得被驱动的主键id，回表查询得到记录
                              3.判断记录是否符合要求，符合就组成结果集。返回一条。
                  缺点：访问被驱动表的时候是随机io。
                        假设驱动表记录是100次，那么被驱动表也会被扫描100次。扫描次数是200次。
                  snlj：被驱动表没有索引，那么就会触发被驱动表的全表扫描。过程和inlj一样
                  缺点：假设被驱动表记录100次，被驱动表记录数是10000次，那么扫描次数是100+100*10000次；
                  BNL：使用Join buffer,将驱动表加载进内存里，然后把被驱动表加载进内存里，从被驱动表里取出一条记录。与内存里的驱动表进行比对；
                        假设驱动表内存记录数是100行，被驱动表10000行，那么扫描次数就是100+10000，但是判断次数是100*10000，很占用cpu资源。
                        此外，如果join_buffer撑不下驱动表，被驱动表就要被多次扫描。扫描次数是N+λ*N*M;判断次数仍然是N*M，即100*10000；
                  bak:  bak其实就是snlj，但是bak的前提是访问被驱动表的时候用到了索引。 对于inlj的随机io问题，bak算法会将查询索引得到的被驱动表id值放在mrr里，进行排序，然后顺序Io。
                        1.将驱动表的记录加载进join_buffer,将join_buffer里的记录与被驱动表的索引进行匹配。得到一系列的被驱动表id，
                        2.将被驱动表id放到mrr缓存里。mrr对id进行排序，之后顺序回表，返回结果集。
                  bak算法即优化了inlj的随机io,而且用到了join_buffer，减少了对驱动表的io次数。效果是最好的。假设驱动表有100行，被驱动表扫描后有100行，那么扫描次数就是100+100，而且是顺序io，批量io。有助于增加join语句的执行速率
                        bak缺点：如果被驱动表加大。那么就要维护一个很大的索引树，为了优化。可以创立临时表。对临时表加索引。这样更可以优化了。
                  hash join: 将连接表与被驱动表的记录拿到java里，使用hashmap作为中间点。进行组合。
                  
            group by :
                  1.根据group by的依据，构建临时表。将select里的数据挑出来作为临时表的字段，临时表的主键就是group by的依据
                  2.全部拿出来后，再放到sort_buffer里进行排序。执行聚合函数。返回结果集。
                  怎么优化group by:可以省掉排序，去重的步骤，那就是给group by 创建索引。直接扫描索引就行了。还不用回表。
      
            distinct:
                    1.将distinct字段拿出来作为主键。放到临时表里面，之后sort_buffer里去重。
            union:
                  1.将第一个子查询的全部字段作为主键，
                  2.创建临时表，执行union后的子查询时，在将查询结果放到临时表前，先要判断是否有主键冲突，如果有就会舍弃该条结果。
                  3.查询完毕后，将临时表作为结果集，返回，并删除临时表
            union all :
                  相比于union不会创建临时表，会直接返回每一个子查询的记录。
            
            子查询：
                    
        redis:
            多线程网络模型看的不咋地，得多看几遍。看了过期删除策略和内存淘汰策略。
            看一看二进制协议，看了redis的resp协议，之后再看多线程模型吧，得一遍一遍看，
            我感觉,redis和mysql一天各自最少三小时。
            redis:   
                      缓存，缓存属于临界资源，要修改缓存的话，必须使用分布式锁。而使用分布式锁，就要考虑到多节点的问题，npc问题
                      怎么分片，哈希槽以及缓存一致性问题。
        redis:感觉实操不行，找个时间实操一波。加深印象。
        java设计模式：
              备忘录模式：用于恢复发起人的状态。可以恢复任意时刻的状态。
                  源发器（Originator）：需要保存和恢复状态的对象。它创建一个备忘录对象，用于存储当前对象的状态，也可以使用备忘录对象恢复自身的状态。
                  备忘录（Memento）：存储源发器对象的状态。备忘录对象可以包括一个或多个状态属性，源发器可以根据需要保存和恢复状态。
                  管理者（Caretaker）：负责保存备忘录对象，但不能修改备忘录对象的内容。它可以存储多个备忘录对象，并决定何时将备忘录恢复给源发器。

               访问者模式：
  
         java并发编程：
                  多线程要多看。

            以后每天早上四点起来，看mysql+redis看到11点，之后干别的事情。这俩太重要了.


      今日任务 2024/07/05
          mysql：
            看explain查询计划。分库分表，数据库连接池，别的好像没看
          redis:感觉可以看看高可用部分。然后自己我觉得也没必要非得写个项目，写项目太花时间了，看了集群。这部分挺难理解的。可以实操一下；
          并发编程：aqs
          总结：以后每天都要看这三个，这三个是重点中的重点。
          java设计模式：复习java的设计模式，
          java:背java的八股文了，每天三四个小时。这样两点前解决这四个。
          算法：刷了单调栈，感觉不能贪多。慢慢刷吧.我觉得每天看看灵神的视频是耗时最少的了。不要看多，没用
          
          
          今日任务：2024/07/06
        java：
                明确以后就是上午数据库+并发编程，下午Java+jvm，晚上计算机网络+算法+操作系统
                学完序列化，反射，注解三大块。学习Jvm里的字节码部分

        jdk内置的序列化时两个流。
              jdk序列化的时候不会序列化静态变量，trasient变量，只会序列化成员变量。注意也不会序列化方法这些。
              jdk序列化的时候包括两部分，头部，对象，头部生命版本号，魔数，对象部写成员属性。还包括开始结束符，类名，签名，属性，属性值。
              以及一些特殊分隔符。
            序列化Id:
    如果可序列化类没有显式声明 serialVersionUID，则序列化运行时将基于该类的各个方面计算该类的默认
    serialVersionUID 值。尽管这样，还是建议在每一个序列化的类中显式指定 serialVersionUID 的值。
    因为不同的 jdk 编译很可能会生成不同的 serialVersionUID 默认值，从而导致在反序列化时抛出 InvalidClassExceptions 异常。
    serialVersionUID 字段必须是 static final long 类型。
            序列化要点：
            1.父类如果是se,子类就都可以序列化。而子类是，父类不是，父类不会被序列化，数据丢失
            2.属性是对象类型，属性必须实现序列化，否则会报错
            3.反序列化时，如果对象属性有修改，增加，删减，那么修改部分数据丢失，不会报错
            4.反序列化时Id被修改会序列化失败。
            5.序列化会忽略trasient的属性
      java还指定了Externalizable接口实现自定义的序列化，但他的协议依然是Jdk自身的协议，jdk序列化是无法跨语言，且非常的笨重，不适合在网络中
进行传输。因此应该使用二进制序列化，以及json序列化。对于http应用，使用json序列化，对于微服务相互调用就应该使用二进制序列化，当我们进行传输的时候
      首先应该考虑的就是序列化的协议。一般基于某种编码的字符串是最合适的
      json序列化：  可以用FastJson,Jackson,Gson来进行json序列化，spring默认Jackson进行序列化。从性能上看Fastjson > Jackson > Gson

  序列化要考虑到 安全性（是否存在漏洞，加密。当然使用https时网络报文层面会进行加密，但自定义的rpc是没有加密的），兼容性（跨平台，跨语言），性能（时间，空间），易用性。
    java反射：
        反射广泛应用于注解，框架，动态代理里面。
        反射首先要获取Class文件，class文件的获取方式有四种，常用的是三种，第四种不会对Class对象进行初始化。Class文件本质是Class对象
    通过CLass文件可以获取Method,FIELD对象。通过Method,field对象可以获取Annotation对象，之后可以获取注解的值，进行赋值操作。对私有的成员，方法进行调用时
首先要修改可达性，修改为true;
      反射的缺点：存在安全性问题，反射的性能稍差。
      反射可以去修改成员属性的值，调用成员方法，生成对象。  
      反射的本质就是将JAVA类里的各种成分都映射成为对象，method映射成Method对象，成员就映射成FIELD对象。
      instanceof 关键字比较的就是class对象是否一致。
      动态代理：
      jdk自带的：        实现InvocationHandler的接口必须是代理模式，里面有原目标对象的属性，之后重写invoke方法。然后调用Proxy.newInstance()方法创建代理对象。这样创建的class对象，在使用结束后就会被销毁，减轻内存的负担，但是动态代理只能基于接口进行，对于一些类对象，想实现动态代理只能用cglib动态代理
      cglib:   spring里的aop使用cglib来生成动态对象。代码上和jdk差别不大，但是底层逻辑不一样。在CGLIB中，方法的调用并不是通过反射来完成的，而是直接对方法进行调用
注解：
    定义注解时要用到元注解
    @Documented
    @Target({ElementType.FIELD, ElementType.PARAMETER})   //可以出现在属性，参数上
    @Retention(RetentionPolicy.RUNTIME)                   //可以出现在运行时。即可以通过反射获取他。
    public @interface RegexValid {}
注解属性只能使用 public 或默认访问级别（即不指定访问级别修饰符）修饰。

注解属性的数据类型有限制要求。支持的数据类型如下：

所有基本数据类型（byte、char、short、int、long、float、double、boolean）
String 类型
Class 类
enum 类型
Annotation 类型
以上所有类型的数组
注解属性必须有确定的值，建议指定默认值。注解属性只能通过指定默认值或使用注解时指定属性值，相较之下，指定默认值的方式更为可靠。注解属性如果是引用类型，不可以为 null。这个约束使得注解处理器很难判断注解属性是默认值，或是使用注解时所指定的属性值。为此，我们设置默认值时，一般会定义一些特殊的值，例如空字符串或者负数。

如果注解中只有一个属性值，最好将其命名为 value。因为，指定属性名为 value，在使用注解时，指定 value 的值可以不指定属性名称。
使用注解：
      Method,Field属性都有isAnnotationPresent,和getAnnotation(Class class)属性来获取方法，属性，参数上的注解。进而获取值。
枚举：
          在 enum 中，提供了一些基本方法：

          values()：返回 enum 实例的数组，而且该数组中的元素严格保持在 enum 中声明时的顺序。
          name()：返回实例名。
          ordinal()：返回实例声明时的次序，从 0 开始。
          getDeclaringClass()：返回实例所属的 enum 类型。
          equals() ：判断是否为同一个对象。
          可以使用 == 来比较enum实例。

          此外，java.lang.Enum实现了Comparable和 Serializable 接口，所以也提供 compareTo() 方法。

          枚举也可以实现方法，枚举自身也可以定义方法。也可以调用方法。
          枚举适合实现单例模式。而且是最佳方法。
  netty：
        使用Channel，ByteBuffer读取磁盘文件.
       try(FileChannel channel=new FileInputStream("E:\\project0\\Redis-笔记资料\\03-高级篇\\资料\\juc_learn\\juc1\\eslearn\\src\\test\\resources\\application.yaml").getChannel()) {
            ByteBuffer buffer = ByteBuffer.allocate(10);
           while(channel.read(buffer)!=-1)//read代表的是方向，从磁盘到内存，write代表的是从内存写到磁盘。
           {
               buffer.flip();//切换到buffer的读模式
               while (buffer.hasRemaining())
               {
                   byte b = buffer.get();
                   System.out.print((char)b);
               }
               buffer.clear();//清空buffer，切换为写模式
           }
          Channel是FileInputStream读写文件的关键。ByteBuffer是内存里的区域，用来存储Channel读写的字节。
  mysql:
        子查询执行流程。以及一条普通sql语句的执行流程
  感觉并发编程得一遍一遍地看，一遍一遍的做，刷题。现在首先学会各个api的使用，把leetcode的十道题全部刷完。然后多多看源码，写总结。但我感觉，网络Io加上多线程才是网络编程。redis我觉得明天可以看看Lua,学会搭建redis集群
  mysql的话，我感觉要一遍一遍的看。现在重点看看事务机制。


mysql
    slow_query_log long_query_time 
  set global long_query_time=1 
  log_output=FILE


redis最关键的部分就是数据结构和应用场景，以及高可用部分了。高可用还涉及到了故障恢复。我在想该怎么解释这个东西。
redis学一下lua  学习完毕。
然后看计算机网络。我觉得计网可以先跳过Tcp.看别的，现在感觉关键就是滑动窗口没看明白，把他看明白。今天的任务，然后写一些介绍。
    RTO:重传时间间隔。包发送后达到指定时间没有收到ack时，就会重新发包
    RTT:发送一个包到收到对应的ack,花费的时间。
  窗口：
      窗口分为发送方和接收方，应该说双方都有两个窗口
   
            Tcp报文段里的Window字段，用于告诉接收端自己还有多少缓冲区可以接受数据，发送端会根据这个来发送数据，这样不会导致流量过多。发送方发送的数据不能超过接收方的窗口大小。但是发送方再发送时也不是把大小为窗口的数据发送的数据全部发送过去
  而是一批一批的发送，之后，接收方会返回ack，发送方根据ack移动窗口，确定还可以发送多少数据。
  发送方：
        SND.WND 表示发送窗口的大小（等于对方的Window）
        SND.UNA  指向以发送但未收到ack的第一个字节的序列号。 SND.NXT 可用窗口大小，指向未发送但在可发送范围的第一个字节序列号
        可用窗口大小=SUD.WND-(SUD.NXT-SUD.UNA)
  接收方：
        RCV.WND:接收窗口的大小，等于自己的Window
        RCV.NXT:指向期望从发送方发送来的字节序列号
      发送方会在窗口大小内持续发送窗口，但只有在ack时才会向右延伸窗口，这样就不会导致一次发的太多而造成流量的浪费。  比如收到了ack以后，ack返回的时序列号，SND.UNA和SND.WND会加上这个值。一旦SND.NXT一旦在这个范围内。经过运算，可发送
窗口的大小如果不为0，就会发包，然后移动SND.NXT。直到超过这个大小。
        滑动窗口在操作系统的缓冲区中，缓冲区的大小=滑动窗口的大小+进程未读数据的大小  就是说如果缓冲区为500字节，窗口一开始是500个字节，发送了140个字节，只读了40个字节，那么窗口大小就是400字节了。因此进程阻塞会引起网络传输阻塞。
因此，窗口有可能变成0，引发死锁。这里就出现了窗口探测程序，防止死锁的发生。
  如果窗口大小甚至小于tcp/ip头，那就划不来了。
  利  用窗口可以进行流量控制，拥塞控制
    拥塞窗口 cwnd是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。

我们在前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。
拥塞窗口 cwnd 变化的规则：
只要网络中没有出现拥塞，cwnd 就会增大；
但网络中出现了拥塞，cwnd 就减少；

了解了滑动窗口，拥塞控制，之后应该看看应用层的协议了。这才是根本。

socket函数：
    1.服务端调用Bind函数监听ip,端口，客户端调用connect函数绑定一个Ip，port。服务端之所以也这样做，是因为计算机有多个ip地址。
    2.服务端绑定好后，会调用listen()函数进行监听。此时客户端可以发起连接了
    3.客户端函数调用connect，连接服务端，连接成功后，连接会放在连接队列里。
    （一单connect,listen返回，就代表连接成功。在内核里，会给每个socket建立两个队列，连接队列和半连接队列。队列里的成员是其他客户端的socket）
    4.服务端会调用accept()拿到已经完成的连接进行处理，没有就会阻塞。连接完成后，accept会返回另外一个socket进行处理，所以要注意，监听socket,处理的socket不是一个。连接完成后，双方会通过read,write开始传输数据。类似于文件流
    感受：其实socket就是应用层了。read,write里面会做数据的校验和重发，其实和java是一样的，哈哈。    
在内核里，socket是一个文件，每个进程都有一个task_struct,这个数据结构里，有一个数组包含了该进程所有的文件描述符，文件描述符就是一个整数，是数组的下标，通过文件描述符可以访问到文件的位置，所以socket本身与其说是文件，
    不如说是数据结构。而socket里，包含了一个文件流。而socket的指针，指向了内核中的socket结构，
    socket结构里有两个队列，发送队列，接收队列。队列的成员就是缓存，一块内存区域。保存了对面发送来的tcp包。每一个数据都能看到完整的tcp包。
    所以到此我们可以知道端口的作用了，他只是根据端口来绑定一个socket，我们可以根据prot来确定一个socket而已。实际上在硬件层面根本没有这回事。只有Ip地址，mac地址。通过端口找到socket,通过socket找到连接队列，队列本质上是数组，或者连表
    创建数据结构进行队列的入队出队操作即可。入队的就是tcp包。
总结：socket本质上就是两个连接队列以及半连接队列。位于内核里，通过port来区分不同的socket,通过socket找到连接队列，将网络包去掉ip部分将完整tcp部分放进队列里。发送时将包交给ip即可。这就是所谓的缓冲区。
udp连接：
    udp连接使用sendto,recvfrom来发送，接收包。

我们可以看到，这种基础socket是一对一的。无法多对一。
  因此往往采用io多路复用的方式来维护多个socket,具体实现有select,epoll两种方式，select的复杂度是On,epoll是OlgN，因为采用了红黑树。这样就可以由一个线程处理多个socket连接。没看明白，就这样吧。
dns：
    1.重定向,即改变映射的值
    2.负载均衡，一个域名配置多个ip,然后返回最合适的ip。
cdn：dns的装饰模式
    看用户的 IP 地址，查表得知地理位置，找相对最近的边缘节点；
    看用户所在的运营商网络，找相同网络的边缘节点；
    检查边缘节点的负载情况，找负载较轻的节点；
    其他，比如节点的“健康状况”、服务能力、带宽、响应时间等
    cdn的缓存代理，多级缓存，层次缓存。
如何实现cdn:答案就是nginx+openresty
本质上通过Nginx发送请求，结合vue生成html返回，也很好。
看看jvm

感觉，还是要保证时间，明天要好好看redis了，高可用部分！场景部分！我觉得一天积累一个场景，实现一下。然后天天看高可用！数据结构一天复习一个就行。多线程也是，要多练，明天开始刷题！

今日任务 2024/07/08

今天很不好，手机掉到马桶里了，坏了，他是一个很好的手机，2020年3月的一天中午，妈妈问我要不要手机，就给我买了一个，他真的很好。永远不会发热，不会内存紧张，不会自动关机，我拿着它，其实也玩了很多，不像原来那个，huawei,
我觉得，手机既然作为一个消费产品，我也没必要买太好的。以后我的手机就不超过1000元，我没必要要那么好的。钱应该花在健康，知识上。而不是这些最终伤害自己的地方。钱不花要比花好，我还买了个健身自行车，以后天天骑车，每天两个小时骑，一定可以减肥的


下午开始看redis的高可用部分。全部搞定！熟悉各部分的原理，然后复习上午看过的redis场景；

redis的高可用包括主从复制，哨兵，集群三种方式，集群可以解决海量数据高并发写高可用的问题，哨兵可以解决高并发读高可用的问题，因此数据量很大时就应该选择集群方式了。集群和哨兵都是以主从复制为基础的。
主从复制包括全量复制，增量复制，缓冲区；
  流程：
      1.replicaof ipA  ipB,确认主节点的Ip地址，发起连接。
      2.建立连接收，从服务器发送psync ？ -1申请数据同步。主节点看到-1后开始生成rdb文件，全量同步
      3.同步期间的，直到从服务器发送回数据，主服务器执行的命令都在replication buffer 备份，从服务器返回备份成功后，主服务器会将buffer里的命令全部发送给从服务器。
      4.之后主服务器每执行一条命令，都会将命令发送给从服务器。从服务器每执行一条命令，都会把自己的slave_repl_offset加上命令的字节数，而主服务器每执行一条命令，也会给自己的master_repl_offset加上命令的字节数。
      5.在这期间，如果出现了延迟，主服务器会把命令写在replication buffer里，循环写，主从再次连接后，从服务器再次发送psync runid offset，主服务器根据offset（就是slave_repl_offset）,和replication buffer里的指针replication offset（就是master_repl_offset），计算两个Offset的差值，
      如果超过了replication buffer的大小，就会触发全量复制，如果不超过，就将replication buffer里对应的字节发送给从服务器，也就是增量复制。
      6.因此，为了放置全量复制多次发生，可以调大repl_backlog_buffer的大小，调节公式为：second*write_size_per_Second，second是秒(从服务器断线后重新连接上主服务器所需的平均时间)，后者是主服务器每秒平均产生的写命令数据量的大小。
      具体参数是repl-backlog-size 1mb
  总结：以上就是redis主从复制的流程，可以看到从节点过多也会影响主节点的性能，尤其是网络不好时触发的全量复制，因此在使用主从模式时应该控制从节点的数量，此外，主从复制可以应对从节点挂掉的情况，无法处理主节点挂掉的情况，因此就出现了哨兵机制，哨兵机制里依然是一主多从多哨兵，无法解决海量数据
  高并发写的问题，因此又引入了集群模式。
      如何应对主从不一致？
      redis无法做到强一致性，在主从模式下都做不到强一致性，为了强一致性，往往会在前端采取措施，比如支付后会有一个支付成功页面，点击返回才能返回。这就是通过业务来保证一致性。
      redis里的info replication命令可以查看主节点接收写命令的信息进度和从节点复制写命令的进度，我们可以开发一个监控程序，得到master_repl_offset,slave_repl_offset的差值，差值过大就不会连接相关的从节点。
      redis如何判断某个节点是否正常工作？
      通过ping-pong的心跳检测机制，如果一半以上的节点去ping一个节点都没有pong,就认为这个节点挂掉了
      主节点每10s发送ping命令，判断从节点的存活性
      从节点每一秒 发送 replconf ack offset命令，给主节点上报复制偏移量
      主从切换如何减少数据丢失？
      主节点执行完命令后就断电，会导致命令没有发送给从节点（此时aof还没有刷盘），主节点数据丢失，从节点没有接收到写命令，同样没有数据。
      这个主要会发生在增量复制和全量复制期间，命令无法立刻发送给从服务器。
      min-slaves-max-lag：lag值指的是从节点上一次发送 replconf ack <replication_offset>相对于现在的时间差，主节点发送info replication后每一个从节点都有一个lag值，当所有从节点的lag值超过min-slave-max-lag的值时，主节点就拒绝写入请求，而且还有一个值，min-slaves-to-write，
      指的是从服务器的数量，当数量小于min-slaves-to-write时，主节点也会拒绝写请求
      从服务器每隔1s会向主服务器发送replconf ack <replication_offset>命令，来检测主节点的网络情况，以及自己的复制进度是否有差距。主节点拿到replication_offset后，于自己的master_repolk_offset进行比较，如果有差别，就会把repl_back_log里的差异部分发送给从节点
      主节点每10s也会给从节点发送ping命令，检测从节点的网络连接情况，通过ping,replconf ack <replication_offset>来相互确认对方。
      通过这两个配置数据可以解决哨兵机制时，切换主从节点导致的数据损失。
哨兵
    哨兵主要负责 监控，选主，通知。
    监控：
        每隔1s给所有的主从节点发送Ping命令，当主从节点接收到ping命令后，回复pong,这样来确定各个节点是否运行，如果没有在规定时间内回复，哨兵将标记节点为主观下线。该规定时间由down-after-milliseconds参数设定，单位是ms，
        除了主管下线，还有客观下线，客观下线用于主节点。当一个哨兵发现主节点主管下线后，便会向其他哨兵发起命令，其他哨兵进行赞成或拒绝投票，当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」，判断下线后，哨兵就开始在多个从节点中
    挑选一个从节点作为主节点。那么谁来挑选主节点，多个哨兵会进行投票，每个哨兵一个投票机会，要想成为leader必须拿到半数以上的赞成票，票数还要大于quorum值，而如何成为候选者？那就是判断主节点为客观下线的那个哨兵。6
      执行过程：
            1.发现主管下线，进行投票，超过quroum就是客观下线，确定客观下线的哨兵成为候选者，候选者开始投票，达到quroum以及拿到半数以上的赞成票就成为leader,leader执行主从切换
            2.选出新主节点。
                          1.筛选掉网络不好的从节点，这主要是哨兵的心跳检测，如果断连超过十次，就认为网络状态不好
                          2，根据优先级，复制进度（offset），id号进行最多三轮排查。选择新的主节点。redis中有一个slave-priority，值越小优先级越高；优先级相同时，根据复制进度，最多的作为主节点；复制进度也相同时，根据从服务器的id号。
            3.选出主节点后，哨兵leader向从节点发送 slaveof no one，命令，将从节点变为主节点，发送该命令后，leader每秒向该节点发送info命令，观察该节点的角色信息。当变成master时，就成了主节点。之后哨兵节点向所有从节点发送slaveof ip port，来实现主节点转移。一切做完后，哨兵将新节点的信息返回给
            客户端。
      核心：
            1.ping监听各个节点的状态，主节点客观下线后，通过投票选出leader,leader根据优先级，offset,id来选择主节点，选好后向该节点发送slaveof no one，同时每秒发送一次info指令，观察其角色，转变为master后向其他节点发送slaveof ip port。转换完成。
    哨兵无法解决高并发写的问题。以及海量数据的存储问题，因此redis的最终状态就是集群。  
集群
    得实践一下
redis场景：  网易游戏直播间弹幕系统
      1.一般思路：数据库里维护一个弹幕表，根据直播id,用户id，时间戳可以确认一个弹幕。不存在传递依赖。发弹幕时，直接写进数据库，看弹幕就是轮询，从数据库拿信息
        缺点：扛不住高并发
      2.加入redis:
              先写redis,redis写mq,mq异步写到数据库，查询依然是轮询，看弹幕依然是轮询 ，redis采用的数据结构是zset，按时间排序来拉取（注意List虽然也可以排序，但List无法按照时间来拉取，因此如果我们要根据客观因素，时间，评分来拉取指定范围的数据时，只能用zset）
                mq，redis的消息会丢失，无法保证持久化。可以使用mq/监听redis日志来实现延迟写。
                缺点：mq,redis挂了，消息会丢失。而redis一挂就会导致缓存击穿。打崩数据库。（注意先写redis，再写mysql都有这些问题）
                如果短时间突然有大量的弹幕，可以使用mq来削峰。缓存在mq的硬盘文件里
                对于弹幕读请求，可以使用本地缓存，缓存近5s的数据，过期了就由服务自身回源redis,（数据量太大时会导致gc）
网络模型：
      今天看一看网络模型，感觉一切到最后就是网络模型呀！
      
  看一看抽奖项目；
      1.抽奖的用户要进行鉴权，满足级别的才可以。参与抽奖的用户有一个过滤。可以在网关或者拦截器进行操作。
      2.抽奖的形式：秒杀活动/每天有一定次数的抽奖
            前者存在高并发，后者就是保证奖品不能早早的抢光。
      3.抽奖的概率：要进行控制
      4.风险控制：防止攻击。保证奖品不超发。中奖概率要均衡。
  中奖的流程：
          在此之前有个过滤层，需要对用户进行鉴权，认证。基本逻辑                              基本逻辑-鉴权逻辑
                为了防止用户出现一些多刷行为，对用户进行一些额外逻辑处理，限制他的中奖次数。      业务逻辑-风险控制逻辑
                          java层有一个函数判断是否中奖，中奖就执行数据库操作。创建订单。否则就返回    业务逻辑-核心逻辑  
          由此可见，该项目需要采用责任链模式，代理模式进行处理。以及业务的扩展。
感悟：设计模式yyds呀！代码就是按照设计模式来写的。
设计模式：
      1.命令模式：
              1.抽象命令： 一个抽象接口，定义了执行命令的统一方法
              2.具体命令：  具体命令，拥有接收者对象
              3.接收者：    执行实际命令的类，命令对象调用接受者的方法来执行命令，也就是说，执行命令里写的就是接收者的方法，来执行命令。
              4.调用者：    持有命令对象。比如waiter
              总结：具体命令里包含了接收者。具体命令的实现方法写的就是接受者的方法。然后调用者包含了一个Command对象，调用者执行命令的逻辑里写的就是具体命令的方法。
      举例：服务员点餐，点完报给前台，前台报给厨师，厨师做完交给前台，前台交给服务员，服务员交给顾客。所以是顾客点餐。顾客点餐叫来服务员，服务员接受菜单后交给厨师。在这里一个方法的执行结果交给另一个执行。
              抽象命令：command
              具体命令：点菜，做饭，端饭
              接收者：接收者进行点菜，做饭，端饭  真正命令的执行者是接收者，具体命令包含了接收者而已。但有时候，接收者需要继续分配命令，接收者自己又是调用者。什么是调用者，包含一个command对象，什么是接收者，接收者被放在具体命令对象里。我们可以看到，当一个人要执行命令时，他就是接收者
              ，他要被放在具体命令里，他的逻辑要放在具体命令实现的抽象方法里，当一个对象是调用者，他应该持有一个命令对象，在他的方法里调用命令对象的执行命令方法。
          命令模式见于：一个业务逻辑需要一系列的对象来执行方法，调用方法。那就需要命令模式了。
      2.责任链模式：
                  抽象处理器，
                  具体处理器：
            按照某个条件进行链式调用，就是责任链模式，常见于sevlret的过滤器。
    3.观察者模式：
            常见于发布订阅。
今日感悟：感觉redis看的也差不多了。剩下就是要实操，每天连连redis的api，看看redis的场景题。主要是要实操才行。mysql其实也差不多了，剩下就是也得实操，还得背。我觉得可以看一些进阶内容了。还有，每天还是要看怎样运行的和设计与实现。并发编程也是要实操，加上看源码。网络这块，感觉每天看看netty算了
，一天看个四五节。对于理解网络编程是好的。jvm我觉得不要急。每天看一些。重点是java八股，赶快看完，还有一些场景题目，加密算法。


先学设计模式吧，看看设计模式。
访问者模式：
        抽象访问者：定义了对自身数据结构中各个元素的操作，是接口
        具体访问者：实现了访问者接口中定义的操作
        抽象元素：定义了接受访问者的接口，通常是一个接口或抽象类，其中定义了一个接受访问者的方法，被访问者对象作为方法的参数。
        具体元素：实现了抽象元素接口，它是数据结构中的具体的元素，用于接收具体的访问者并执行相应的操作。每个具体元素都有自己的业务逻辑，并在接收访问者时将具体的操作委托给访问者进行处理。
        对象结构：是一个包含元素角色的容器，提供让访问者对象遍历容器中的所有元素的方法，通常由 List、Set、Map 等聚合类实现。
      对象结构提供了让访问者访问容器每一个元素的方法。元素本身的抽象方法里，参数是访问者，而在里面其实是访问者将元素作为参数进行调用。
        对元素的操作是在访问者里定义的，在具体访问者里实现的。抽象元素里的方法接收抽象访问者作为参数，也就是说，其实元素本身是有操作的，但是里面只是调用了参数的方法，也就相当于把具体操作交给了访问者来实现。
      举例：参观博物馆的时候，游客和考古学家对每一件展品所做的操作是不同的。即对元素的操作是不一样的。那就需要把对展品的操作拿出来，交给这些游客来实现。 
      访问者模式就是将对数据结构的操作与数据结构分离。这样就可以将对数据结构的操作多样化。尤其是这个数据结构并不是数据型数据结构时。
责任链模式：交给一批来处理，包括抽象处理器和具体处理器两个元素
命令模式：存在相互调用的时候用命令模式。
今天应该学习java多线程的应用了。狂刷一天的题。搞明白场景。
Semaphore：
      semaphore底层就是aqs队列，acquire()方法就是将state的值减去一个值，relase()方法是将state值加上一个值。本质他也是用到了state队列。acquire方法则是会加入阻塞队列，
感觉这个月，要加强并发编程的练习。不能光看，还要在写的过程中了解原理。
最朴素的办法就是lock,和syn+volatile，用这两个解决问题。不要去用高阶的，容易死锁。感觉不会的就是并发编程，要多练！数据库这块还行，要实操!       
今天的任务：搞明白并发编程！
    synchronized:
            可以调用wait,notify将当前线程加入阻塞队列，或者唤醒阻塞队列里的线程，返回的前提是获得了锁。调用wait方法时，会将当前线程加入阻塞队列，notify方法会将线程加入同步队列，在同步队列里争抢锁成功后，就可以继续运行。
    线程之前的通信可以使用管道输入输出流：PipedReader,PipedWriter.
jdk本身提供的synchronized,wait,notify,notifyAll,PipedStream,join，volatile提供了基本的线程运行逻辑。volatile是一种轻量级的锁。wait,join都提供了超时模式，syn基本都是阻塞式的获取锁。
如果想恢复线程的执行，可以用LockSupport
lock相比于syn可以多次释放获得锁，且lock提供了condition,非常好；condition必须和lock相关联。此外，lock可以中断的，非阻塞的获取锁。
在java里，实现自定义的同步器，比如锁，阻塞队列，都要采用aqs同步器。
原子更新类：
      原子更新类都无法解决aba问题，可以更新引用，int,boolean,Object类。他们的使用不是很熟悉，但可以用来计算商品的总价格等。不过当商品服务部署了多个服务时，就不太行了。
多线程工具类：
      Semaphore,CountDownLatch,Exchanger,CycliBarrier一共四个。
  CountDownLatch:用于协调主线程和子线程的工作。   适合于传输大文件。再结合操作系统的零拷贝效率会很高。
  CycliBarrier:    cyclibarrirer可以协调所有的线程一起工作。当一个线程到达await方法后，线程就会阻塞，知道所有的线程都到达了await,且变量的值减为0，所有的线程才会被全部唤醒，开始工作。常见的例子就是打游戏，只有所有的玩家都进入游戏，
                  游戏才能开始。
  Semaphore:    信号量，用来控制并发线程的数量
  Exchanger:   用于两个线程之间交互信息。exchange相比于PipedReader的单向传输,使用exchanger的两个线程可以相互传输信息，

  

    CountDownLatch源码：
            使用了aqs同步器，
          public void countDown() { sync.releaseShared(1); }  将state减少1
          public void await() throws InterruptedException {sync.acquireSharedInterruptibly(1);}  state值为0的时候就返回。因为doAcquire返回的条件就是state==0;

          //释放锁
          protected boolean tryReleaseShared(int releases) {
          
            // Decrement count; signal when transition to zero
            for (;;) {
                int c = getState();
                if (c == 0)
                    return false;
                int nextc = c-1;
                if (compareAndSetState(c, nextc))
                    return nextc == 0;
            }
        }
    }
          public final void acquireSharedInterruptibly(int arg)
            throws InterruptedException {
        if (Thread.interrupted())
            throw new InterruptedException();
        if (tryAcquireShared(arg) < 0)
            doAcquireSharedInterruptibly(arg);
    }

 protected int tryAcquireShared(int acquires) {
            return (getState() == 0) ? 1 : -1;
        }
//获取aqs锁，插入到同步队列里，如果之前的节点是头节点，就把自己设置成头节点。然后返回。
 private void doAcquireSharedInterruptibly(int arg)
        throws InterruptedException {
        final Node node = addWaiter(Node.SHARED);
        boolean failed = true;
        try {
            for (;;) {
                final Node p = node.predecessor();
                if (p == head) {
                    int r = tryAcquireShared(arg);//不等于0的时候就代表state还有值。那就继续阻塞，
                    if (r >= 0) {
                        setHeadAndPropagate(node, r);
                        p.next = null; // help GC
                        failed = false;
                        return;
                    }
                }
                if (shouldParkAfterFailedAcquire(p, node) &&
                    parkAndCheckInterrupt())
                    throw new InterruptedException();
            }
        } finally {
            if (failed)
                cancelAcquire(node);
        }
    }
总结：4中多线程工具类的核心依然是aqs，可见aqs是并发编程的核心。除了aqs,还有volatile,synchronized,cas,unsafe，如何基于aqs实现公平锁，非公锁，可重入锁。
    aqs的核心就是阻塞队列，诀窍就是成为头结点的线程获得锁。而释放锁的核心就是要唤醒head.next节点对应的线程。采用的是unpark() ,此外还有可重入锁的设置，都是一样的。Condition是一个封装，它封装了ConditionObject,该类里面有一个阻塞队列。
 public class ConditionObject implements Condition, java.io.Serializable {
        private static final long serialVersionUID = 1173984872572414699L;
        /** First node of condition queue. */    阻塞队列第一个节点
        private transient Node firstWaiter;
        /** Last node of condition queue. */  阻塞队列最后一个节点
        private transient Node lastWaiter;
        public ConditionObject() { }
公平锁，非公平锁的获取.该怎么设置？
阻塞队列：
      插入方法，移除方法，检查方法，这三个方法根据队列为空，满的时候采取什么操作。有以下几种：抛出异常，返回特殊值，阻塞，超时退出。为了避免命名冲突，三种方法应该重载。
阻塞队列的实现也是使用了aqs;阻塞的实现使用了Locksupport.park来实现。
    
 延时阻塞队列：队列里的元素要实现Delayed接口里的getDelay()方法，此外还要有一个AtomicLong 的sequence变量，还要实现compareTo接口，指定比较规则，此外还有一个Time相关的元素，指定时间
            当消费者从队列里拿元素时，拿出后调用getDelay方法计算timer与当前时间的差值，如果小于0，说明timer在当前元素之前，那么就可以获取该元素，否则就阻塞当前线程，当然如果现在有一个线程也在获取头部元素，且在first里标记了，那就阻塞当前线程。线程执行完后调用signal，唤醒其他线程。
适合高并发的队列：SynchronousQueue
  线程之间传递消息的方式：共享内存，以及直接传输两种。Syn和LinkedTransferQueue都是采用了直接传输。
      

一般来说，我们可以设计阻塞队列来实现自己的线程池。实现更高效的线程池
DelayQueue:
      使用了PriorityQueue来实现延时阻塞队列。
计算机网络：
      好好看看。

总结：今天复习了并发编程的知识点。感觉还是缺少实操。弟弟今天说了一个坏消息。我们两个都没有退路，但我们两个前面都不是绝路。如果我俩一直坚持学习，就没事，反之如果贪玩，就会变成绝路，死路。今天看了看并发编程，以后还是要多看MySQL，redis,每天早上起来就看这俩。现在也要包括高可用了。好好看，好好学。今后要注意实操。
        
mysql:今天可以看看高可用.看看高可用该怎么做。redis要看看redisson锁。

mysql:
    读写分离是Mysql应对高并发的第一招，

    mysql的优化参数：
          innodb_buffer_pool_size:   buffer pool的内存大小，可以调节成内存的80%，这样可以减少因为内存不足而刷盘。
          innodb_buffer_pool_instances :buffer pool的实例个数，为了减少并发冲突，当buffer pool大小大于1GB时，innodb会自动将缓冲区划分为多个实例空间。  
          在高并发情况下，这两个参数很有用
          工作线程缓冲区：
            sort_buffer_size: 排序缓冲区，对于优化distinct,group by ,order by 语句很有必要  当排序的结果集大小大于sort_buffer-size时，就需要存到临盘里辅助排序。因此要适当的将该值调大一些，
            join_buffer_size:  对于优化连接查询很有必要。使用bnl算法时，会将驱动表放进join_buffer里，join_buffer放不下驱动表时就要多次扫描被驱动表。
            read_buffer_size:
            read_rnd_buffer_size:用于mrr优化。优化使用索引的语句。
        参数大小如何调整：  对于这些区域，最好根据机器内存来设置为一到两倍MB，啥意思呢？比如4GB的内存，建议将其调整为4/8MB、8GB的内存，建议将其调整为8/16MB.....，但这些区域的大小最好控制在64MB以下，因为线程每次执行完一条SQL后，就会将这些区域释放，所以再调大也没有必要了
          关于排序，还有一个参数：
            max_length_for_sort_data:这个参数关乎着MySQL排序的方式，如果单行的最大长度小于该值，则会将所有要排序的字段值载入内存排序，但如果大于该值时，则只会将排序字段和id放进buffer里，全部排序后，按照id值回表获得完整的数据，并返回。前一种排序称为全字段排序，后一种成为rowid排序。
        调整临时表空间：  
          tmp_table_size:  临时表最大内存大小，超过该值时，临时表会暂存到磁盘里。
            同时还可以调整tmp_table_size、max_heap_table_size两个参数，这两个参数主要是限制临时表可用的内存空间，当创建的临时表空间占用超过tmp_table_size时，就会将其他新创建的临时表转到磁盘中创建，这显然是十分违背临时表的设计初衷，毕竟创建临时表的目的就是用来加快查询速度，结果又最后又把临时表放到磁盘中去了，这反而还多了一步开销。
          调整线程存活时间，和最大连接数：
            max_connections:最大连接数
            wait_timneout:  空闲连接在连接池里的存活时间，默认是8h.
            interactive_timeout:
        join语句该怎么优化？
              算法：bnl, inlj,snlj,bka,其中bnl是对snlj的优化
        二级索引查询该怎么优化：
             read_rnd_buffer_size ：将查询获得的id放到read_rnd_buffer里，排序后回表，将随机io变成顺序io,称之为mrr优化。开启语句是： set optimizer_switch="mrr_cost_based=off"  set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on';
            在join语句的inlj里，就可以采用mrr优化。格式是： set optimizer_switch="mrr_cost_based=off"
            对inlj的查询，也可以用bka进行进一步优化，这样可以对被驱动表进行顺序io。开启方式就是 set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on';此外还要开启join_buffer_pool，mrr排序的字段就是join_buffer_pool里的能容纳的驱动表行的大小，越大，就越可以减少随机io。
            对于snlj，只能由bnl进行查询。
      mrr优化：    
        Multi-Range Read
            Multi-Range Read简称MRR，其目的是尽量使用顺序读盘。
            对于SQLselect * from t1 where a>=1 and a<=100;
            其执行流程是：
            根据索引a，定位到满足条件的记录，将id值放入read_rnd_buffer中。
            将read_rnd_buffer中的id进行递增排序。
            排序后的id数组，依次到主键id索引中查记录，并作为结果返回。
            使用MRR的原因是随着a值的增加，id的值会变成随机的，随机访问的性能较差，但是多少数据是按照递增顺序插入得到的，所以如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。
            read_rnd_buffer的大小是由read_rnd_buffer_size参数控制的。如果步骤1中，read_rnd_buffer放满了，就会先执行完步骤2和3，然后清空read_rnd_buffer。之后继续找索引a的下个记录，并继续循环。这里的过程有点像Block Nested-Loop Join的join_buffer执行过程。
      总结：
            可以看到，mysql的调优主要是内存大小的调优，分为对buffer pool的调优以及针对各个查询的调优，比如sort_buffer_pool,read_rnd_buffer_pool,join_buffer_pool，sort_buffer_pool与group by,order by,distinct有关。

      除了对mysql优化，还有可以对orm框架进行优化，特点就是对orm的连接池进行优化。
        mrr优化，bka优化本质就是代理模式。我们只要开启参数就好。


    主从复制：优化读多写少的场景
          主从复制里的延时问题，
          如果某些业务的一致性要求，延时性要求很高，就必须强制读主库，比如写库后必须实时读数据时，（金融支付后查询支付情况）就要强制读主库。对于一致性必须是强一致性的业务，就要采用读主库的方式。在读主库时，可以采用分库的形式，应对高并发的情况。前边的系统也可以限制操作。比如减少短时间内多次支付。
          主从复制如何避免丢数据：
            可以设置同步方式为半同步复制和全同步复制来避免数据丢失。但这样会导致性能的下降。
          什么时候主从集群？
            应该先考虑sql优化，索引优化，redis优化，最后在考虑读写分离，分库分表。
          大事务：
            大事务的Binlog往往很大，回对主从复制造成影响，影响mysql性能。
          还是要实操
        主从复制的类型：异步复制，同步复制，无损半同步复制，有损半同步复制。建议选择无损半同步复制。此外还有多源复制。多源复制是为了方便统计分析。延迟复制是为了放置误删导致的数据丢失。
        总结：
            1.异步复制用于非核心业务，并发量高
            2.无损半同步复制用于核心业务场景，保证数据的强一致性。
            3.多源复制可以将多个master数据汇总到一个数据库实例进行统计分析
            4.延迟复制可以放置误操作带来的后果。
      
        分库分表：  优化写多读少的场景
              核心：
            分库分表的方式    水平分库分表，垂直分库分表
            分库分表的sharingKey  :即分库分表的依据，由于shardingKey只有一个，当使用别的查询条件查询时就无法获得shardingKey了，那就要将shardingKey映射到其余的字段上，比如根据用户id进行分表，使用订单id查询时，可以将用户id放到订单id最后几位，实现shardingKey.
            分库分表的分片算法： 范围分片，哈希分片，查表法  范围分片容易产生热点问题，哈希分片会影响数据库的伸缩能力。哈希分片也可能出现热点。
            分库分表后的一些查询问题：如count(),就要引入es，为了引入es,还要引入mycat，进行缓存实时更新。
            分库分表后的主键问题：“
                  UUID:首先 UUID 作为数据库主键太长了，会导致比较大的存储开销，另外一个，UUID 是无序的，如果使用 UUID 作为主键，会降低数据库的写入性能。  MySQL InnoDB 引擎支持索引，底层数据结构是 B+ 树，如果主键为自增 ID 的话，那么 MySQL 可以按照磁盘的顺序去写入；如果主键是非自增 ID，在写入时需要增加很多额外的数据移动，将每次插入的数据放到合适的位置上，导致出现页分裂，降低数据写入的性能。
                  雪花算法：Snowflake 算法可以作为一个单独的服务，部署在多台机器上，产生的 ID 是趋势递增的，不需要依赖数据库等第三方系统，并且性能非常高，理论上 409 万的 QPS 是一个非常可观的数字，可以满足大部分业务场景，
                            其中的机器 ID 部分，可以根据业务特点来分配，比较灵活。如果服务器在同步 NTP 时出现不一致，出现时钟回拨，那么 SnowFlake 在计算中可能出现重复 ID。除了 NTP 同步，闰秒也会导致服务器出现时钟回拨，不过时钟回拨是小概率事件，在并发比较低的情况下一般可以忽略
                  此外还有一些分布式Id的解决方案：
    redis看redisson锁：
          redisson在获取锁的时候，如果之前没有获取，就设置锁，锁的次数是1，锁的过期时间，如果锁存在，并且持有者是当前线程，那么增加锁的次数，重置锁的有效期。如果锁已经被别的线程占有，那么返回锁的剩余有效时间。
          redisson在释放锁的时候，如果锁不是自己的，就返回，是，就将锁的次数减1，如果剪完后，锁的次数为0，就删除锁。并使用publish将删除的消息广播给订阅频道的线程。
          tryLock():
                  1.首先会使用lua脚本获取锁，获取锁失败的时候就会返回该锁的剩余有效时间。
                  2.返回后先查看是否已经超过超时时间，如果是就返回false;如果不是，用一个对象来订阅这个锁，这个锁在释放时会publish，
                  3.代码的意思就是在time时间内等待锁的释放，如果time时间内未释放，就会返回false;抢锁失败，如果成功，就可以继续抢锁
                  4.首先计算时间是否过时，过时就返回false;不过时的话，就重新抢锁。但如果抢锁再次失败了，就再次阻塞，这里阻塞的时间根据锁的剩余有效期和自己的朝时期来定，调用aqs进行休眠。
                  5.休眠完成后，继续判断是否到了自己的有效期，到了就退出，不到就计算抢锁。因此用的是循环抢锁。
        watchDog:
     总结：今天刷了一些题，我感觉还是应该按照宫水三叶的刷题单来，而且应该每天中午一道题，一天多个时间段思考它。关于mysql,redis接下来依然要接着读文章，复习之前的知识。开始循环巩固。java也要继续看设计模式，多线程。尤其是多线程，感觉自己多线程这块很薄弱。主要是没有看过实际项目，挺不好的。、
    计算机网络，操作系统，每天看个一两篇文章吧。加油！

今日任务 2024/07/11
    感觉以后早上起来先刷题，看灵神的题解。刷个两个小时。然后是数据库，多线程。

https://leetcode.cn/problems/longest-even-odd-subarray-with-threshold/solutions/2528771/jiao-ni-yi-ci-xing-ba-dai-ma-xie-dui-on-zuspx/  刷题

mysql：不知道该刊什么了，复习吧
redis: 继续看文章！
        关于缓存的问题：关键就是缓存重建（重建时要获取分布式锁，还有未抢到锁的线程怎么办，一种是自旋，查询，还有就是返回旧值。），布隆过滤器，缓存击穿，就用布隆过滤器+设置有效期较短的值，缓存穿透使用重建缓存，缓存雪崩考虑设置Key的有效期。
        缓存的大key怎么解决：采用压缩算法，更压缩的序列化。选择好的数据结构。
        缓存的方式：  更新完数据库要不要写缓存？答案是要么删除缓存（常见做法），要么更新缓存（银行系统常见，但要注意时序性的问题，说白了也是重建缓存，多线程写的问题，可以采用分布式锁+版本号来解决），要么先更新缓存在更新数据库（并发量很大，不是很核心的业务）。
        redLock: 放置主从模式下主节点崩溃导致多个线程都抢到了锁。redLock保证redis加锁时需要争取一半以上的机器同意，才可以加锁，这样即使崩溃了几个机器，也可以保证锁依然存在。
              NPC:  N：network,网络延迟，获取锁后返回时，网络延迟过大，导致锁的释放，可以用锁的过期时间来处理
                    P：进程暂停，发生了GC，GC后锁已经过时了，导致多个进程获得了同一把锁。redLock无法解决。
                    C：时钟漂移，redis的服务器发生了时钟漂移，即key过期了，被删除，使得锁瞬间过期。这点redLock也无法解决。
        综上redLock相较于单一的redisson，（redisson解决了服务崩溃锁无法释放，不可重入的问题。）解决了分布式锁主节点崩溃时多机抢锁的问题，但没有完全解决，在P,C状态下，redLock无法解决。

        redis限流：限制用户访问。
              计数器：
              滑动窗口：
        感觉限流还挺难的。感觉redis这边就是细水长流了，现在要多看看网络，操作系统,jvm的文章。但是redis还是搞的不太懂，我觉得要实操！一边看一边实操。
                缓存相关问题： 已经解决。
                集群相关问题：比较初级
                分布式锁：感觉也可以了，就一个redisson就行
                持久化：也还行，复习一下参数
                场景：不行，其实就是这块不行，这块要学会实操！明天开始实操一下。
          mysql：每天复习一下就行了。还有也是看看orm的框架了，学会实操
          算法：感觉每天刷着，看零茶山的课。不会就过。一天看一个新题就行
          八股：背一背计算机网络了，java基础了。

    感悟：感觉只能早起，五点起来干活，每天可以多8个小时的学习时间。基本可以学完数据库相关的。

  redis:完全掌握基本命令和数据结构，主从集群复制！才能去看场景题。
    redis：根据需求，和内存，选择正确的数据结构。· 
          redis的数据结构会根据类型大小做出不同的实现；redis的value对应的是redisObject结构
        redisObject{
                unsigned  type;  通过 type 查看
                unsigned  encoding; 通过object encoding查看
                void* ptr;   指向底层的数据结构
        }
          type 只有5个：string,set hset,list zset.
          编码类型就是底层的数据结构实现：int,embstr,raw,  linkedlist,ziplist,intset,HT,skiplist分别是整数，简单字符串，动态字符串，双向链表，压缩列表（存在级联更新，但内存小）整数集合（set里面全是整数的时候）HT （hset的底层）skiplist(zset的底层)
      字符串对象：  type位string
              如果字符串长度是39字节，那么用embstr保存这个值，特点是。redisObject和sds将分配一个连续的空间；如果是大于39空间，那么就是用raw格式，特点是sds,和redisObject不在一块了。释放内存时需要释放两次，如果字符串保存的是整数，在Int范围内，也是用int来保存。如果是小数，就用embstr或raw来保存
            命令：set,get,append,incrbyfloat,incrby,decrby,strlen,sterange getrange
      set对象：    编码为int set,hashtable
              如果全是整数，就用int set来保存，int set是升序的，查找时使用二分查找的方式。集合元素对象保存的所有元素都是整数值且数量不超过512个用int set,超过就用hashtable.
                调节参数：
                        set-max-ziplist-entries:
          命令：sadd,scard,sismember,smembers,srandmember,spop,srem,sinter,sunion,sdiff
      list:    编码为ziplist,linkedlist
              元素的字符串长度小于64字节，元素数量小于512；
              调节参数：  list-max-ziplist-value:
                          list-max-ziplist-entries:
              命令： lpush,rpush,lpop,rpop,lindex,llen,linsert,lrem,ltrim,lset
              可以修改指定索引处的值，可以在某个节点前后插入值，可以修剪链表的长度，可以返回指定索引处的值
      hash对象：ziplist,linkedlist
                键，值字符串的长度都小于64字节，键值对数量小于512个
                调节参数： hash-max-ziplist-value
                          hash-max-ziplist-entries
            命令：hset,hget,hexists,hdel,hlen,hgetall
            解析：只能对hash进行增加删除，统计长度的操作；
      zset对象：ziplist,skiplist
                元素数量小于128个，长度小于64字节
                调节参数：
                          zset-max-ziplist-value
                          zset-max-ziplist-entries
                命令：zadd,zcard,zcount,zrange,zrevrange,zrank,zrevrank,zrem,zscore
      就是这些个命令，所有的场景都是用这些个命令来实现的。
        剩下几种数据结构：
                    bitmap,geo hyperlog    bitmap用于海量数据打卡，签名等，geo用于地理纬度查询，hyperlog用于粗略估计总量
              bitmap:    setbit key offset value
                          getbit key offset  
              hyperlog：pfadd key value  pfsountc key  
              geo: geoadd  geosearch fgeoradius\
                              geoadd city 121.47 31.23 "上海" 116.41 39.90 "北京"
                              geosearch key 
                             geopos city 北京
                             geodist city 北京 上海 km
                            georadius city 121.48 31.23 10 km withdist withcoord withhash count 10 desc
                             georadiusbymember city "上海" 10 km withdist withcoord withhash count 10 desc 
        redis数据类型就这么多。所有场景都根据他们来实现。
        redis本身数据结构：
          redisServer{
            redisDb* db;
          }
          redisDb{
          dict*dict;
          }
          dict{
        dictht ht[2];
          }
          dictht{
          dictEntry * *table;
          unsigned long size;
          unsigned long sizemask;
          unsigned long used;
          }
        dictEntry{
          void* key;
          union{
          void* val;
          uint64_t u64;
          }
        dictEntry *next;
        }
        //骑手抢单
              有一个单，扫数据库订单表，找到，设置。自己的名字。修改配送状态。可以使用拉模式。骑手的app定时查询数据库，获得到后，使用乐观锁修改。这样性能不好；
            写完数据库后，同步到redis,这里会有时序的问题。骑手自己的程序轮询，但这样访问量很大，因此应该采用推模式。有一个系统定时轮询。

      接着看redis的高性能，高可用，今天彻底结束。
        主从复制，哨兵，集群，现在主要看集群。
          集群： cluster meet ip port， 会将ip port对应的节点加入到nodes所在的集群里
          启动集群时需要更改配置文件里的cluster-enabled，改为yes，否则无法使用集群模式。集群根据槽来确定数据是否要存入自己的节点，reids采用hash槽的形式相对改善了
          集群的伸缩能力，因为他会自动的进行数据迁移。
          槽的使用：redis服务器接收到key后，使用hash算法转换为hash值，然后hash%16384得到数据所在的槽，接着查询自己的char slots[16384]。看是否为1，1说明节点在自己的
          处理范围内，为0就要查询clusterNodes *slots[16384]，找到索引对应的主节点值，再次查询。接着返回结果。
          redis今天就到这里吧，看看别的，http吧。http感觉也没啥用呀，就把那些文章看完就行。感觉redis的集群，还是很难判断的。
      现总结主从复制和哨兵，我们要明白他们会出什么问题，他俩都是应对读多写少的请求的。集群是应对海量数据写多读多的请求的。
    
    http:
          http将数据类型分为八类，八类下边由很多子类格式：type/subtype。如text/html,就表示是文本类型里的html文件。image/png就表示是图片类型里的png格式的图片。
        对于未知数据类型，采用application,如application/octet-stream就表示是二进制数据。此外对于大文件，还需要进行压缩；指出采用了何种压缩算法。即encoding type;
        常见的有gzip,deflate,br等
            http里的字段： accept:  accept-encoding: content-type:
          语言类型： accept-language:   content-language:     accept-charset:  语言的编码格式放在content-type里。
          Connection:表示了希望之后的连接该怎么处理，keep-alive表示之后应该继续保持连接；close表示关闭连接。
          服务器主动断开连接的方式： keepalive_timeout,keepalive_requests，前者表示一段时间内没有数据收发就主动断开连接，后者表示长连接上的最大请求次数。 这是在nging里边的设置；
          其实tcp本身就是可以主动断开连接的。但在http里服务器只能在自己的配置文件里面进行设置。
          重定向：http服务器的响应报文的headers里出现了Location:的字段，它的内容就是客户端应该重定向的uri,这个uri既可以是相对uri,也可以是绝对uri,相对uri省略了scheme,host:port,只有
        path,query。
              区别：站内条转需要使用相对uri,站外跳转需要使用绝对uri
          重定向的问题：性能损耗和循环跳转。
        Cookie:    
                请求头里由Cookie,响应头里有set-cookie;set-cookie的字段是：key=value; 请求头里的字段是 cookie:key=value结构。 cookie由浏览器存储，服务器和客户端浏览器个有一份
        cookie存在生存周期，他的有效期由expires,max-age两个属性来设置。“Expires”俗称“过期时间”，用的是绝对时间点，可以理解为“截止日期”（deadline）。“Max-Age”用的是相对时间，单位是秒
浏览器用收到报文的时间点再加上 Max-Age，就可以得到失效的绝对时间。Expires 和 Max-Age 可以同时出现，两者的失效时间可以一致，也可以不一致，但浏览器会优先采用 Max-Age 计算失效期。
        其次cookie有作用域，domain,path属性制定了cookie所属的域名和路径，浏览器在发送cookie前会从uri里取出host,port对比cookie的属性，不满足条件时，就不会再请求头里发送cookie.
        此外cookie还有安全性，httponly告诉浏览器cookie只能通过http协议传输，secure表示cookie只能用https加密传输。
        http的缓存：cache-control字段，属性有max-age;max-age=30表示相应的数据将缓存30s。时间的计算起点是响应报文的创建时刻即离开服务器的时刻。
                    此外还有 no_cache字段，表示可以缓存但使用前必须去服务器验证是否过期（即还要发送http请求），no_store表示不允许缓存。 must_revalidate表示过期后如果还想用就要去服务器
      验证。
              对于缓存，客户端浏览器也可以在请求里添加缓存相关的请求，比如cache-control:no-cache,表示不希望缓存。而查看是否使用缓存需要根据请求头里的status-code，如果后面有from disk cache；
      就表示是缓存。
              条件缓存：  if-modified-since,if-none-match  ,需要使用上一次服务器响应时的数据 last-modified,和ETag,
      代理服务器：服务器会有一个代理，缓存静态文件，常用的就是nginx,代理可以解决跨域的问题。以及安全地问题。还有负载均衡的问题。常用的字段就是Via字段。通常情况下服务端需要知道用户的ip地址，
      因此http里面出现了 X-forwarded-for ,x-real-ip两个字段。后者就是客户的真实ip，此外还有origin,via字段
        感悟：还需要了解加密算法，但我觉得一天时间花在数据库上是没问题的。
      如何伪装redis的客户端，执行info replication;可以减少主从节点的数据不一致情况，
      redis主从复制的缺点，没有复杂均衡和轮询的中间件，可能出现数据不一致的情况。哨兵机制，如果哨兵挂了怎么办

    感悟：明天早上继续看集群。要彻底看明白，然后过几天实操一波，继续实验mysql的加锁。我觉得明天可以把集群结束了。然后每天复习。java就可以复习一些基础和框架就行了。除此之外呢，还有一些，但主要就是这个了
  ，也就是redis，剩下还有并发编程。但可以接着练练redis的数据结构。不同数据结构主要怕是
    今日任务： 2024/07/13 
    redis:看明白集群。
    创建集群：     redis-cli --cluster
    访问节点：      redis-cli -c -p port 
    查看集群：     redis-cli -p cluster nodes
    添加集群节点： redis-cli --cluster add-node new_host:new_port existing_host: existing_port --cluster-slave  --cluster-master-id <arg>  默认是主节点
    插槽分配：      redis-cli --cluster reshard ip:port         x(移动的插槽的数量)      id(接受插槽的id)        source(槽的来源id)  done(结束)     yes（是否要移动插槽）
    集群的数据迁移类似哨兵，不过是自动完成的。
    故障转移命令:cluster failover  在从节点执行，执行后会直接将从节点变成主节点，主节点变成从节点，数据迁移工作
    ask错误：发生在槽迁移期间的错误，是一种临时方案
    moved错误：发生在正常运行期间
    选举新节点：当某个主节点状态时fail时，发现fail的主节点会在集群里广播，收到消息的fail节点的从节点，就会进行广播，要求处理槽的节点给自己投票。票数超过N/2+1时，该从节点就成为主节点。
    PING:集群里的节点每隔1s就从集群中随机挑出5个节点，对这五个节点中最长时间没有发送过ping消息的节点发送ping消息。以此检验该节点是否在线。当然一个节点一个可以直接广播pong消息，来更新时间。
    aqs看源码
        REENTRABNTLOCK:
              非公平锁：只是用cas,因此锁的性能要好一些，此外，非公平锁是非阻塞的。
              公平锁：加入了同步队列，
  jvm:
      查看字节码文件： javap -verbose PCRegisterTest.class
      设置栈的大小 -Xss1m  -Xss256k 虚拟机的大小是1m;  设置前需要先添加 ADD VMoptions
      每个方法都对应着一个栈帧。栈帧里面包括局部变量表，操作数栈，动态链接，方法返回地址。
        局部变量表是一个数字数组，最基本的存储单元是Slot(变量槽)，局部变量表存储的是各种基本数据类型，对象引用，returnAddress类型，局部变量表所需的栈帧大小是
        局部变量表里，32位以内的类型占一个slot,64位占2个slot,
        编译器已知的，保存在maximum local variables数据项里。也可以称为locals，stacks代表了栈的深度；
        LineNumberTable: 行号指的是代码里的行号，起始PC指的是该行代码在字节码里的起始行号。
        LocalVariableTable: start pc:变量作用域的起始字节码，length:该变量在多少行字节码内是有效的。两个参数一起限定了变量的作用域。
        在栈帧中，与性能调优最为密切的就是局部变量表，局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中变量直接或间接引用的对象都不会被回收。

    设置堆内存：  -Xms20m -=Xmx20m 最小内存20m,最大内存20m
          内存监视：jvisualvm.exe，位于jdk的文件内。
          堆内存里新生代和老年代的分配比例： -XX:NewRatio=2 表示新生代占1，老年代占2，新生代整个为1/3；它代表的是老年代是新生代的多少倍。
                          新生代里伊甸园区和两个幸存者区的比例：默认为8：1：1，参数为 -XX:SurvivorRatio=8，表示伊甸园区占比为80%

    感悟：每天都要看设计模式，写设计模式！这个太重要了。

    总结：明天要接着看Mysql,redis，redis尤其要做好命令的记忆和熟悉，然后多练。多看配置文件。并发编程继续看源码，看aqs和多线程的源码，mysql要加强实操。jvm发狠的看，看完jvm基本就
    大功告成了。redis要多看场景的使用，redsiion的源码。

今日任务
    jvm：继续听课，看书，尤其是记住参数，感觉其实也不难，就是毕竟比较多
          方法区大小设置参数：-XX:MetaspaceSize  -XX:MaxMetaspaceSize,在Windows里前者是21M，后者是-1表示没有大小限制，在运行时，-XX:MetaspaceSize会随着gc反复变化
          赋值：-XX:MetaspaceSize=100m
          查看运行时方法区的大小：jinfo -flag MetaspaceSize pid
          方法区主要包括类信息和运行时常量池，静态变量，即时编译器编译后的代码缓存，field信息，Method信息；使用cglib,proxy时，会产生大量的临时类，即动态生成的类，在没有用的时候应该予以回收。  
          类信息包括：类型完整的有效名称，直接父类的完整有效名，修饰符，这个类型直接接口的一个有序列表。
          field信息：field名称，类型，修饰符，field信息的存储按照生命顺序
          methdo信息：名称，返回类型，修饰符，参数的数量类型，字节码，操作数栈，局部变量表大小等。
  感悟:今天就当放假吧，早期比什么都重要！以后要接着看并发编程，数据库，设计模式，Jvm，加油！

今日任务：
    jvm:听方法区的知识
    java并发编程：看unsafe魔法类；
    java设计模式：复习两个设计模式
    数据库：
            mysql:看分布式id的方案；
            redis：看数据结构和场景。
今日任务：
      1.juc：继续学习；包括设计模式，查漏补缺。看了看unsafe类，没啥感觉，还不如看看阻塞队列，异步执行。
      2.io：看小林哥推荐的
      3.juc:
            Future:future接口时阻塞式的，也可以叫做观察者模式，当被观察者状态发生变化时，主动通知观察者，回调，AIO皆是基于此创建的。被观察者要包含观察者属性。
      CompletableFuture可以进行异步任务的编排组合，而获取结果的get方法依然是阻塞调用的。因此他并不是观察者模式。我们可以用外观模式来形容completableFuture接口
      那么该怎么做呢，就是说可以任意安排顺序，这又可以叫做桥接模式。我们可以在里面形容为观察者模式，即一个线程的状态改变后，就通知另一个线程，或者可以用中介者模式
      感觉复杂的相互协同工作，应该是中介者模式。同时每一个执行方法的方法体都有一个任务，应该是模板方法模式。
          1.如何传递结果。传递结果时，需要用到抽象工厂模式。
          2.
          /**
           *      subMit，execute
           *      Async的区别就是执行方法的线程，Async时可以自己制定或者ComeplatableFuture内定线程，否则就是主线程
           *      thenApply Async    Function  有参数有返回值
           *      thenAccept Async  Consumer      有参数，无返回值
           *      thenRun  Async     Runnable         无参数，无返回值
           *      thenCompose Async          对传入的参数有类型限制
           *
           *      Consumer可以接收前一个任务返回的结果，但本身没有返回值，Function则可以接收参数，也可以返回，Runnable就是纯的执行逻辑。，Callable则是没有参数，有返回值。BiConsumer可以接受两个参数，但是没有返回值。
           *
           *      任务组合：
           *      thenCombine Async   对任务的结果进行整合。也是异步的。
           *
           *      runAfterEither,  两个任务任意一个执行完了就开始执行第三段逻辑
           *      runAfterBoth,  两个任务任意都执行完了就开始执行第三段逻辑
           *      allOF(...).join():所有任务都执行完了才会往下执行，
           *      anyOF(...).join(),任意一个任务执行完了，就会往下走
           *
           *      getNow();可以指定默认结果；
           *      whenComplete(): 给每个任务注册回调逻辑
           *      execeptionally() :注册出现异常时的逻辑；参数是一个异常。
           *      complete():直接让任务完成。
           *      cancel:取消任务。如果任务已经结束了，那就没有用，如果正在执行，未执行，会在获取结果时抛出异常。
           */
              看一看completableFuture的源码，弄明白了他采取了策略模式和观察者模式，明天可以继续看，晚上看些什么好呢？晚上看java io吧，
              今天可以看一看kafka的源码！
            今天晚上看java的io，学会使用kafka；
            java io:
              操作系统的io：操作系统在进行Io时，堆内存进行了划分，在硬盘和内存见还有一层内存，他和硬盘间的读取是以块为单位的（使用dma）。在Io时，数据从硬盘以块的形式放到这个内存里，再以另一种速度放到
            用户空间中。bio的read,writ都是一次读写一个字节，buffer则是一次一个缓冲区。bio使用的是cpu里的寄存器进行读写操作，即数据先到cpu,再到内存，因此一次只能是32字节。
            此外当需要大批量操作时，可以使用dma，dma以缓冲区为单位进行数据拷贝。将数据直接拷贝到用户空间里。这样一可以减少cpu的使用，二可以加快数据传输的速度。此外操作系统还提供了map接口；将缓冲内存的
            地址直接提供给用户空间，用户修改后直接刷入磁盘，这显然更快，但也非常危险。因此dma也是nio高效的基础。在传输数据的时候，还可以使用dma进行零拷贝。全程都以块为核心进行传输。
              此外，对于java来说，数据拷贝流程是：disk-内核缓冲区-native buffer-heap buffer，可以看到相同的数据传输了三次；使用dma后，还是要拷贝三次，但这时是以dma，以块为单位进行拷贝
              ，但是当使用map时，不需要拷贝，直接修改kernel buffer里的数据。使用零拷贝的时候，也是通过两次dma，将kernel buffer里的数据直接放到硬盘里。这也是为什么buffer比directbuffer慢，
            directbuffer比map()慢，而零拷贝是最快的。
                因此nio相比于bio，更多的使用了操作系统提供的api,而异步Io，则在api的基础上增加了设计模式里的命令模式，来进行回调操作。
                        在java里，通道就是对dma。cpu传输数据的抽象。
                  只分为5种,在单线程模式下是一样的，但是在多线程模式下，这是很有意义的，可以用多个线程相互协作。我们在思考io的时候，就看他属于哪一种，除此之外
            还有它对系统调用的使用次数，因为每一次使用都会导致一次上下文切换。
                  1.io类型。 2.系统调用次数（对应上下文切换和系统调用次数，系统调用次数还受到该Io是字节类型的还是块类型的传输数据方式）3.内存拷贝次数。4io的两端
            bio：
                bio往往比较低效，首先他会阻塞主线程，其次他是一个字节一个字节的读写。因此系统调用次数很多，但bio可以根据两端分为:
                字节流：一次读写一个字节，内存io(不用序列化，ByteArrayInputStream同一个线程内通信,PipedOutPutStream两个不同的线程通信,)，网络io（socket不同的线程，进程通信）文件（需要序列化，FileInputStream,BufferedInpurtStream，ObjectInputStream，DataOutPutStream）,
                字符流:  一次系统调用读写一个字符。
            NIO:
                    NIO采用缓冲区，选择器，通道三种改良措施，通道可以双写，且通道不必拘泥于两端，Buffer的存在可以将数据直接读取的缓冲区内，BIO中是先调用read()方法,再将read方法的结果写到目的地中，因此Buffer可以减少内存拷贝的次数。
            选择器可以同时监听多个Channel,因此BIO主要用于网络io里。此外Nio还是用了零拷贝的技术。
              在nio里，可以划分的只有Channel,它分为四类：
               FileChannel：从文件中读写数据；
                DatagramChannel：通过 UDP 读写网络中数据；
                SocketChannel：通过 TCP 读写网络中数据；
                ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。   
              Buffer: Buffer的存在可以减少数据拷贝次数，比如DirectBuffer直接将数据从内核空间复制到设备，除此之外，Channel还实现了基于块的拷贝，这样也可以加快拷贝速度。DirectBuffer相当于直接使用内核空间，它是将
              用户空间与内核空间进行了映射。
              io多路复用：
                    selector可以绑定多个channel,实现一个线程监听多个应用。但是这种轮询的方式非常的慢，redis采用了epoll()的方式。添加了红黑树以及回调函数来处理。

              感悟：今天看明白了java的io底层原理，收获还行，但没有实操。还要继续学习，明天继续看javaio和多线程编程。每天早上五点起来看！此外明天可以看一些java8新特性了！

      今日任务：
  mysql，看吧，看吧
      子查询优化：
                1.子查询分为标量子查询，行子查询，列子查询，表子查询。此外还可以分为不相关子查询，相关子查询。
                2.操作数 comparsion_operator 子查询时，子查询只能是标量子查询或行子查询，
                      in/any/some/all子查询时，可以是列子查询和表子查询。比如：操作数 in 子查询，操作数 comparsion_operator any/some/all 子查询 any,some效果是一样的，表示任何一个为true整个为true，all表示必须全部为true，才为true,
                3.子查询执行顺序：子查询分为相关子查询和不相关子查询，in 类型的不相关子查询可以转化为join,或者物化表的方式进行优化。而相关子查询不能优化。优化的方式根据子查询结果是唯一索引，或者有索引，无索引来定。当是唯一索引或者主键时，子查询将直接转化为标准的内连接。
                当查询结果是普通索引时，将采用loosescan的方式，只扫描索引，且只扫描每个值的第一个，反过来去看主表里有没有记录满足，满足就放进，不满足就丢弃；当不是索引的时候，会物化为临时表进行连接或者将主表里的id创建一个临时表，来去重。可以理解为没有索引的时候会创建临时表
                性能损耗比较大。
                          相关子查询的创建条件： in子查询，且in子查询最多只能与与and连接。不能使用union,group by 等数据。不满足该条件时，会先将子查询物化，在建立连接。
                4.对于相关子查询，先取外层查询的一条记录，到子查询表中寻找符合匹配条件的记录，如果能找到，就停止匹配，并将外层查询放入结果集中。知道外层查询全部遍历完。类似于snlj，性能最低，此外还会将相关子查询转换为exists查询，此时可能用到索引。
        连接查询里的控制拒绝：通过在where里设置被驱动表非null的条件可以将外连接转换为内连接，便于优化器优化执行计划。
      分布式事务：
              XA协议的两阶段提交，三阶段提交，基于消息的最终一致性。。前两者遵从强一致性，后者遵循最终一致性。
      看明白分布式事务和spring事务，以及mybatis的执行逻辑。
SPRING事务：
        切面逻辑：
              判断@Transaction注解是否存在->使用spring管理器新建一个数据库连接->set autocommit=false->（mybatis，jdbc拿到上面创建的连接，利用该连接执行sql）->commit或者rollback;
      消息队列的使用：
          下午学一下zookeeper和kafka，下午学会。
      kafka的好处：
        1.同步变异步，微服务远程调用时，如果不是必要，可以采用异步的模式，将参数写入消息队列。增强系统性能
        2.请求缓冲，生产者消费者模型中，双方的性能并不完全一致，此时消息队列可以平衡双方的性能，不至于出现过于忙碌的状态，
        3.数据分发，有的时候，在数据库系统中，可以将Binlog放在消息队列里，不同的canal从消息队列里拿log，这样可以增加主库的写性能。主库不用等所有从库同步完成再回去执行写操作。（类似于第一条，减少远程同步调用的性能损耗）
        实际上网络通信的双方，当双方的性能差距巨大时，或者说一个业务流程里，出现短板，瓶颈时（比如mysql与redis）都可以用消息队列进行平衡，比如秒杀时，通过jvm，大量的请求需要访问redis,mysql，这就出现了极端的差距，就可以用消息队列进行平衡，具体就是redis和mysql之间。通过redis获得了数据，就直接将
        相关信息写进kafka里，然后异步的交给消费者进行消费，执行数据库写操作。
        消息队列的特点：异步，削峰，解耦，
          为什么要使用消息队列，其实是在问，如果不削峰，不异步，不解耦，会有什么问题。
          1.使用消息队列的情况：
        1.执行一堆重量级操作时，如多次远程调用时。
        2.扩展性，增加下游代码的灵活性。让上游的操作与下游完全解耦。比如远程调用的时候，如果之后不需要这个调用了，或者下游的代码出现改动而使得上游出现bug了，那就要去生产者那边改代码。这样肯定不好，因此直接放在消息队列的好处就是让上下游完全解耦。
        3.可用性：消息队列可以保证在部分成功，部分失败的情况下，系统依然可以正常运转，同步代码则只能忍受全部成功的情况。
        4.事件驱动：消息队列里有一股浓浓的命令模式的感觉，即相互调用，比如假如消费者本身也是生产者时，就可以使用消息队列来降低代码的复杂性，因为命令模式，中介模式往往比较复杂。代码不好维护。使用消息队列可以加强程序性能的同时，降低代码的复杂性。
            当一些业务需要多步骤，多机器协同时，就可以使用消息队列了。事件驱动本身也是异步模式。
          远程调用时，谁知道会不会该功能，所以应该统一交给消息队列来处理。通过消息队列来实现观察者模式。
今日任务：kafka学习。设计模式学习，晚上学习数据库和redis，
//什么模式呢？策略模式呗。
    kafka真的牛逼呀，吞吐量在几十万几百万几千万都行，还能持久化。
    感悟：学号kafka,真正开始做项目！
    拦截器：使用的是代理模式，但可以配置多个拦截器，也就是责任链模式。责任链模式，可以使用在线修改的方式，进行修改，添加职责。使用享元模式进行动态的添加。但是要鉴权，那就再加一层责任链模式进行鉴权。
    指定拦截器的时候要指定全限定名。 
    感觉刚刚学了学，还是得实操呀。一边学一边用。今天看两个设计模式。
  设计模式：访问者模式
        抽象访问者（Visitor）：定义了对自身数据结构中各个元素的操作，为每个具体元素类对应一个访问操作，该操作中的参数标识了被访问的具体元素。
        具体访问者（Concrete Visitor）：实现了访问者接口中定义的具体操作，确定访问者访问一个元素时该做什么。
        抽象元素（Element）：定义了接受访问者的接口，通常是一个接口或抽象类，其中定义了一个接受访问者的方法，被访问者对象作为方法的参数。
        具体元素（Concrete Element）：实现了抽象元素接口，它是数据结构中的具体的元素，用于接收具体的访问者并执行相应的操作。每个具体元素都有自己的业务逻辑，并在接收访问者时将具体的操作委托给访问者进行处理。
        对象结构（Object Structure）：是一个包含元素角色的容器，提供让访问者对象遍历容器中的所有元素的方法，通常由 List、Set、Map 等聚合类实现。
  抽象访问者采用方法重载的方式，方法名都是相同的，但是参数不一样；所谓的参数就是元素；元素本身也是有方法的。
  具体访问者实现了抽象访问者的方法，但他其实是调用了参数的方法，以及自己自定义的逻辑，这样就可以做到，针对同一个参数，但是实现效果是不一样的。
  抽象元素：定义了统一实现方法，方法的参数就是抽象访问者，各个元素的不同就是根据实现方法不同来区别的。
  具体元素：实现了抽象方法，里面只有一行；访问者的visit方法参数就是this,这样就可以做到一个方法不同实现效果了。
  对象结构：保存了元素的集合。定义了添加元素和接收访问者的方法，最后由对象结构来调用方法，实现不同的访问者不同的实现效果，
  也就是说，真正的逻辑写在访问者里面，但是访问者的方法需要传递参数，因此元素里也要包含访问者，且访问者所有方法的方法名必须一样，因为从逻辑上讲，访问者的动作都是访问，因此应该定义一样的动作。在元素眼中，
  访问者利用自己做了一些事情，这些事情也是独立的，抽象的，都是需要她来完成，所以也可以定义成一个接口，而且里面的逻辑也就是访问。因此元素本身需要定义接口来接收访问者，里面的逻辑也只有一行。区分不同访问者的就是访问逻辑
  实际应用：如果我们有一个集合，希望每个对象访问集合时，都可以出现不同的效果，一千个人心里有一千个哈姆雷特，那我们就可以用访问者模式。典型的比如消息队列，我们希望每个访问者做的事情不一样，

继续kafka：
kafka查找数据：
      由于kafka是直接落盘的，所以消费者使用一条数据后，kafka不会删除数据，但是kafka会维护所有消费者的off_set,然后取其中的最小值，删除最小值以前的log文件。log文件的命名就是根据offset来命名的。offset定位文件时，也是使用
    二分查找，找到第一个大于他的offset,然后回退到上一个index,seek（hash索引）得到自己的索引，就得到了数据的偏移地址，再加上所在index文件本身的大小（因为文件名称就是offset），就得到了数据的真实地址，访问该地址即可；
  全程的时间复杂度基本是O1的。因为index文件不会很多，而在index里面又是hash索引，索引查询速度很快。这也就是分段的好处。
      kafka后台会有一个定时任务，查看各个leader partition的Min(offset),然后删除之前的index,log文件，这样就可以保证不会占用太多磁盘空间。
避免reblance:
      reblance多为Consumer实例退出导致的。这有三个原因：
        1.consumer长时间没有给Coordinator发送数据
        2.consumer的poll方法执行时间过长
        3.consumer的jvm发生gc;
      可以调节 broker 端的  session.timeout.ms  如果超过该时间没有给coordiator发送心跳请求，就会将consumer实例标记为下线，引发reblance
                            heartbeat.interval.ms:表示consumer多久给broker发送一次心跳请求。因此 h>=3*s；
             以及消费者端的  max.poll.interval.ms：consumer执行poll的最长时间。超过该时间就认为consumer实例下线。可以考虑给该值设计的大一些。
                             max.poll.records:poll方法一次返回的最大记录数。   
      实际生产中，最容易造成问题的就是gc，导致consumer长时间不能发送心跳，因此一定要做好垃圾回收。
kafka位移提交：
    参数：enable.auto.commit 默认为True.即自动提交。
          auto.commit.interval.ms: 默认值5s,即5s进行一次自动提交。
     主动提交：
          被动提交是每次提交上一次poll的位移值。poll方法会先提交上一批处理的消息，在处理下一批。但是在Reblance的情况下，会出现重复消费，因为poll方法是源源不断的获取与处理任务的。假设提交后3s，发生了
      Reblance,但是这3s处理的消息都没有提交，Reblance后就会重新调用poll方法，那么这3s的消息就会被重复消费。
      手动提交：阻塞提交可以确保不会重复消费，但是他会影响tps，异步提交时，如果提交失败，就无法重试了，也就是说offset值不会变。那么下一次仍然会重复消费。
      细粒度提交：
            实际上offset值存在每一个ConsumerRecord里,也存在OffsetAndMetadata里,实现细粒度的提交时提交即可。他们里面都有一个属性叫做long offset，就是偏移量。
              private Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
                  int count = 0;
                  ……
                  while (true) {
                              ConsumerRecords<String, String> records = 
                  	consumer.poll(Duration.ofSeconds(1));
                              for (ConsumerRecord<String, String> record: records) {
                                          process(record);  // 处理消息
                                          offsets.put(new TopicPartition(record.topic(), record.partition()),//创建一个Map,来存储每一个消息，最后提交即可。
                                                     new OffsetAndMetadata(record.offset() + 1)；//下一次传输的起点。
                                         if（count % 100 == 0）
                                                      consumer.commitAsync(offsets, null); // 回调处理逻辑是null
                                          count++;
                  	}
                  }    
          
  多线程消费者：
                消费者可以单线程的接收消息，但是用线程池处理消息，这样就可以加快处理速度。或者多线程的接收消息，多线程的处理任务。这样都可以提高系统的吞吐量。这里可以使用中介者模式来处理任务。
                缺陷在于无法保证同一分区内，消息的处理顺序，以及正确提交移位。就比如先消费了消息2，然后提交，但消息1消费失败，就会导致丢消息。这里我们就可以引入阻塞队列，让各个线程去阻塞队列里获取消息。
  Kafka消费者双线程设计：
        Kafka的消费者端有两个线程，处理任务线程和心跳线程，心跳线程专门向broker发送心跳检测。任务线程用于处理任务。
   producer的tcp连接：
          producer需要像send的对象建立tcp连接，因此最好不要创建太多的主题，这样会导致创建过多的tcp连接，同样消费者也不要消费太多的分区，也会导致创建tcp连接。
  消费者组进度监控：
    lag:消费者落实broker的程度，lag越大越危险。这时就需要主动的去建空消费者，动态的调整了。而这也就是线程池的意义所在。动态的适应项目的需求。
  分区leader与fllower:
        Broker端参数replica.lag.time.max.ms参数值。这个参数的含义是Follower副本能够落后Leader副本的最长时间间隔，当前默认值是10秒。
        这就是说，只要一个Follower副本落后Leader副本的时间不连续超过10秒，那么Kafka就认为该Follower副本与Leader是同步的，即使此时Follower副本中保存的消息明显少于Leader副本中的消息。
        我们在前面说过，Follower副本唯一的工作就是不断地从Leader副本拉取消息，然后写入到自己的提交日志中。如果这个同步过程的速度持续慢于Leader副本的消息写入速度，
        那么在replica.lag.time.max.ms时间后，此Follower副本就会被认为是与Leader副本不同步的，因此不能再放入ISR中。此时，Kafka会自动收缩ISR集合，将该副本“踢出”ISR。
        值得注意的是，倘若该副本后面慢慢地追上了Leader的进度，那么它是能够重新被加回ISR的。这也表明，ISR是一个动态调整的集合，而非静态不变的。
    unclean领导者选举：
          此时会从不在isr里的分区里选取一个leader，这样会牺牲数据一致性，设置参数为Broker端参数unclean.leader.election.enable，为了数据一致性，不建议开启。
    避免消息重复消费：
          消费者端增加幂等性逻辑。可以利用数据库实现幂等，常见的是查询，setnx等，还可以设置前置条件，也就是查询，以及版本号。  第三种类似于全局数据库，给每个消息分配一个全局唯一id，执行前先检查这条Id对应的
    数据是否被消费过。
          幂等性本质也是锁，也是共享数据的消费问题
          幂等性保证：使用数据库作为兜底，使用布隆过滤器或bitmap，redis进行过滤，最后使用定时任务的异步线程兜底数据库的操作;
          幂等性主要用来过滤无效数据，因此主要应该用Bitmap进行操作。其实就相当于加锁。但是成功后不会释放。之后需要插入唯一索引。唯一索引是可以兜底的。之后可以判断插入数据的状态，也就是多次插入数据，判断
          其实，通过前边的Bitmap,唯一索引已经可以过滤到大部分数据了。
          更新不成功，就会抛出异常，然后提交事务即可，比如说，我就是失败了，那就有可能丢消息。这个可以采用命令模式，来处理，消息失败后，该怎么办。那就是重新投递啦。这么简单的事情。成功就什么也不管。也可以把消息放到一个地方
          来保存消息队列。
          本质就是通过加锁操作，来阻塞访问，但这里的阻塞就是直接失败。
  kafka参数：都不要使用默认值。
     Broker端：以下参数都是不能使用默认值的参数。一共11个
        存储参数：
            log.dirs:指定Broker需要使用的若干个文件目录路径，需要亲自指定，参数为csv，比如：/home/kafka1,/home/kafka2,/home/kafka3，一个broker对应一个leader分区，多个follwer分区，写多个磁盘实际上是多个镜像。每个磁盘里的数据是一样的
	    	这些个目录存储的文件是相同的，便于当一个硬盘坏了时，可以在另一个硬盘上恢复。    
	    log.dir:上一个的补充，也就是一个路径，
            设置log.dirs时可以同时读写多个物理磁盘，吞吐量，安全性都更好。当某个磁盘坏掉时，该磁盘上的数据都会转移到其他正常的磁盘上。
         连接参数：
            zookeeper.connect:zookeeper集群的ip:port集合，也是csv格式的，
            listeners:监听器，  监听器的格式为<协议名称，ip（主机名），port> 告诉外部连接者要通过什么协议访问指定主机名和端口开放的Kafka服务。简单说就是告诉别的客户端，服务器该如何访问。
            advertised.listeners: 对外开饭店个监听器，
                  这两个其实都可以写自己的ip:port即可，但Ip也可以写成主机名，以放置ip改变。协议名如ssl,tls(加密传输),plaintext（明文传输）
                  举例：docker run -it --name kafka01 -p 19092:9092 -d -e KAFKA_BROKER_ID=0 -e KAFKA_ZOOKEEPER_CONNECT=192.168.233.129:12181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.233.129:19092 -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 wurstmeister/kafka:latest
                      这个命令的意思是docker里运行了一个kafka实例，对外是19092端口，advertised_listeners告诉所有人，可以在192.168.132.200:19092上通过plaintext访问这个kafka服务，他需要连接地址为192.168.132.200：12181的zookeeper服务，他的服务器id时0（不可重复）
        Topic管理三兄弟：全部是false;
            auto.create.topics.enable：是否允许自动创建Topic。          最好不要，应该让使用kafka脚本创建topic，
                          什么时候自动创建：当生产者/消费者向broker的某个topic进行交互时，不小心打错了topic的名字，kafka就会自动创建该topic，实际上不能发生这种情况。
            unclean.leader.election.enable：是否允许Unclean Leader选举。最好不要，这会导致数据丢失
            auto.leader.rebalance.enable：是否允许定期进行Leader选举。  设置为true时会强行修改Leader,在rebalance.enable=true的情况下，会引起reblance，非常影响性能
                  上面三个参数应该都是false;
        数据保存三个参数：根据实际情景来设置，尤其是最后一个和第一个
            log.retention.{hours|minutes|ms}：这是个“三兄弟”，都是控制一条消息数据被保存多长时间。从优先级上来说ms设置最高、minutes次之、hours最低。一般使用log.retention.hours，
            log.retention.bytes：这是指定Broker为消息保存的总磁盘容量大小。默认值是-1，即保存多少都可以。
            message.max.bytes：控制Broker能够接收的最大消息大小。指broker可以接收的单个消息的最大字节数。默认1mb,可以根据实际情景调大一些；
            compression.type:broker端存储数据采用的压缩算法，最好与生产者压缩算法保持一致，默认值是producer,表示使用producer端的压缩算法，即不对数据做处理。如果不同，就会重新解压，压缩，引起cpu使用率飙升。还会使得kafka无法使用零拷贝，降低消息发送速度。
    Topic参数：同样是broker端的，只有三个，要根据不同的topic来修改
          retention.ms:规定了该Topic消息被保存的时长。默认是7天，即该Topic只保存最近7天的消息。
          retention.bytes：规定了要为该Topic预留多大的磁盘空间。默认值-1，表示无限使用磁盘空间。
          max.message.bytes：topic可以接收的单个消息最大大小。
        命令举例：创建时设置参数：bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic transaction --partitions 1 --replication-factor 1 --config retention.ms=15552000000 --config max.message.bytes=5242880
                    这个命令在localhost:9092的broker上创建了一个transaction的topic,设定了分区数为1，副本数为1。总的分区只有一个。 修改了两个参数(--config后面的连个参数)
                  修改参数：bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name transaction --alter --add-config max.message.bytes=10485760
                      这个命令请求zookeeper，修改transaction的config参数，使用 --alter  --add-config来实现
                两种命令建议使用第二个。
    JVM参数：主要是垃圾收集算法和堆大小，也是broker端的。broker毕竟也是jvm程序，因此应该设置jvm参数
          KAFKA_HEAP_OPTS：指定堆大小。
          KAFKA_JVM_PERFORMANCE_OPTS：指定GC参数。
          如果Broker所在机器的CPU资源非常充裕，建议使用CMS收集器。启用方法是指定-XX:+UseCurrentMarkSweepGC。否则，使用吞吐量收集器。开启方法是指定-XX:+UseParallelGC。
    生产者端：
          enable.idempotence:true,表示开启生产者端的消息发送幂等性，但是只能保证单会话，单分区的幂等性。
  总体上来说参数集中在broker端和消费者端，消费者端参数的目的是防止gc而造成reblance;broker端需要管理数据存储位置（多个磁盘存储），zookeeper连接，监听器（自己的注解，port,以及传输协议），数据方面的三个（数据保存时间，数据最大字节数，broker可以保存的总数据大小），topic相关的(topic的消息保存时间，topci可以保存的最大字节数，以及单个消息的最大大小)
分区策略指定：生产者端
    常见分区策略：轮询，随机，按消息键存储。默认是按消息键存储，如果没有消息键就采用轮询的方式
    缺点：无法实现更多样的消息投递，这时可以实现org.apache.kafka.clients.producer.Partitioner，实现close,partition方法，其中的partition方法就是分区的策略。返回值就是分区的索引。
      int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);返回值就是分区的索引。
压缩策略：Producer端压缩、Broker端保持、Consumer端解压缩。producer,broker端的compression.type需要保持一致。避免重新解压，压缩。broker端的消息在解压缩时，是以消息集合为单位的。
          压缩算法的选择也会影响到broker端消息存储的大小，消息传输的速度。
无消息丢失配置：生产者端，broker端，消费者端都要配置，生产者，broker的参数较为固定，consumer需要自己写复杂的代码逻辑。
      生产者端：使用producer.send(msg,callback)，callback回调会告诉结果。失败时可以在里面写入处理逻辑；
      broker端：设置参数ack=all.一个partition的所有follower都将消息写入磁盘时，在回复producer. 
                此外设置unclean.leader.election.enable = false;
                replication.factor >= 3,即多设置几个follower.
                retries:生产者重试次数。
                min.insync.replicas > 1控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于1可以提升消息持久性。在实际环境中千万不要使用默认值1。
                确保replication.factor > min.insync.replicas。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，
                还要在不降低可用性的基础上完成。推荐设置成replication.factor = min.insync.replicas + 1。
      consumer端：多线程异步处理消息时，如果使用自动提交位移，poll方法会在拉取消息前将上一次的offset自动提交。因此应该使用手动提交的方式。
                enable.auto.commit：false。表示consumer取消自动提交。
kafka拦截器：
      producer端：会在send方法调用前，返回ack前调用onSend,onAcknowledgement，这里可以使用代理模式进行一些额外的处理；
      consumer端：onConsume,onCommit，在poll方法前，commit提交位移后调用，同样也用的是代理模式，可以进行额外的处理；
      当前Kafka拦截器的设置方法是通过参数配置完成的。生产者和消费者两端有一个相同的参数，名字叫interceptor.classes，它指定的是一组类的列表，每个类就是特定逻辑的拦截器实现类。拿上面的例子来说，
      假设第一个拦截器的完整类路径是com.yourcompany.kafkaproject.interceptors.AddTimeStampInterceptor，第二个类是com.yourcompany.kafkaproject.interceptors.UpdateCounterInterceptor，
      Properties props = new Properties();
      List<String> interceptors = new ArrayList<>();
      interceptors.add("com.yourcompany.kafkaproject.interceptors.AddTimestampInterceptor"); // 拦截器1
      interceptors.add("com.yourcompany.kafkaproject.interceptors.UpdateCounterInterceptor"); // 拦截器2
      props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);

 tcp连接：     
      KafkaProducer实例创建时启动Sender线程，从而创建与bootstrap.servers中所有Broker的TCP连接。
      KafkaProducer实例首次更新元数据信息之后，还会再次创建与集群中所有Broker的TCP连接。
      如果Producer端发送消息到某台Broker时发现没有与该Broker的TCP连接，那么也会立即创建连接。
      如果设置Producer端connections.max.idle.ms参数大于0，则步骤1中创建的TCP连接会被自动关闭；如果设置该参数=-1，那么步骤1中创建的TCP连接将无法被关闭，从而成为“僵尸”连接。
producer幂等性:防止消息被多次发送
      开启enable.idempotence=true,之后broker端会保留一些参数，用于校验。这种处理只能保证单会话，单分区的幂等性。多分区时就要使用事务。
      事务：使用事务型producer.
          这里的事务借鉴了数据库的redolog。可以落盘，但必须追加标志才算是提交成功。事务消息的类型应该是这样的： begin1  x1 x2 end1 begin1 x2 y2 end2 ,我们可以看到，这就是一个redolog型的消息。没有end时，就不读取。
          使用步骤：
                  1.开启enable.idemponent=trrue,
                  2.调整代码逻辑：    
                               producer.initTransactions();
                                try {
                                            producer.beginTransaction();
                                            producer.send(record1);
                                            producer.send(record2);
                                            producer.commitTransaction();
                                } catch (KafkaException e) {
                                            producer.abortTransaction();
                                } 
                  3.调整消费者端的isolation.level为read_committed,表示只会读取事务型producer成功提交事务写入的信息
      总结：事务型producer相比于幂等性producer要慢很多;
      

   防止reblance的参数： 都是consumer端的参数 
       session.timeout.ms 调大一些
        heartbeat.interval.ms 调小一些
        max.poll.interval.ms：调大一些
        GC参数   
  Kafka提交位移：
          可以使用countdownlatch,cyclibarrier来进行提交；
  commitfailedException:
        poll方法调用间隔超过了max.poll.interval.ms的值，引发了reblance;
        措施：增大max.poll.interval.ms或者减少poll方法拉取的消息数量，或者使用多线程加快消息的处理速度。
  多线程方案：
        针对reblance，可以使用消费者生产者模式，进一步去缓冲它。poll线程获取消息，提交给阻塞队列，worker线程从阻塞队列获取元素，进行处理，这样就可以防止poll的发生了。
  使用动态线程池，consumer lag，监控，调整消费者的消费速度：
        
   kafka控制器：
          第一个创建/controller的broker会成为控制器。控制器可以管理主题，分区重分配，领导者选举，集群成员管理，数据服务






多线程：
      Future相比于FutureTask的区别： 
      FutureTask可以单独使用，也是一个线程。
CompletableFuture:
  看一看jvm:
感悟：最近比较乱，因为没有做项目，好多东西都没有落实下来，可以写写项目了。asynflow的本质就是reactor模式，不过这里是拉模式，并发量比较小。
    就写asynflow这个项目；
    任务上下文：下一个阶段任务处理需要的参数；可以用json字符串，url，总之是下一个阶段处理所需要的信息。 一般来讲任务是多变的。要根据任务来调用，也就是中介者模式，或者类似于reactor模式，并发量较大时，也可以完全做成reactor模式。启用多线程和代理模式来处理任务。
    order_time:自定义的排序逻辑，使用时间和优先级这些单一的排序规则可能会导致饿死。因此order_time应该是包含多种排序规则的。那么可以用多元函数的方式对这几种规则的优先级进行映射 order_time=k1*l1+k2*l2+k3*l3+k4*l4;k代表的就是影响程度。最终order_time就是一个整数类型；
          排序策略使用java的接口，策略模式进行安排。本质上就是我们制定了Java优先级的策略。在里面实现。并且将优先级编程可修改的。就是这样。仍然是按照asc来排序的。
    最大重试次数：若重试次数过多，就不能再处理了。因此使用最大重试次数作为限制。
    当前重试次数：记录当前已经重试的次数
 分表：分表是一个专门的服务，而且会变化，因此应该专门抽出来，做成一个服务。毕竟它完全就是一个后台服务而已。且分表的策略也比较复杂。最好不要放到一个jvm来做。
        分表：任务开始表：worker从哪拉取任务，begin
              任务插入表：flowsvr将任务放到哪个表。 end
     这里可以借鉴mysql的redolog里checklsn的做法。此外，为了减少连接的创建于销毁，与数据库的操作的连接应该保持稳定，因此worker服务都通过flowsvr对数据库进行读写。后台线程因为逻辑可能会变，可以采用自己的连接来与数据库交互。
          整体上flowsvr将执行所有与数据库相关的操作。任务治理将会对flowsvr,worker进行管理。worker从flowsvr对数据库进行操作；
          flowsvr的功能：创建任务，更新任务；占据任务。
  感悟：leetcode只能是积少成多，但是多线程是可以好好练练的！每天都要练习多线程

多线程设计模式：
        1.immutable object 所有的对象都是不可变的。如果修改，就直接使用新的变量。这样可以保证，所有的线程访问该变量时，都不会出现线程安全问题。
                  适用于变量被多个线程频繁访问，且更改概率不大时。此外还可以针对是否频繁修改，创建两个类，例如String,StringBuffer类，前者不会被改变，多个线程访问它时，不加锁，效率高，如果要频繁修改
                就要使用StringBuffer类了，他是single thread execution模式的。
        2.single thread execution:某个类，方法某一时刻只能由一个线程访问，可以加synchronized,或者双重检验的方式，这样的方法往往涉及到对共享变量的操作。需要进行加锁，比如阻塞队列，线程安全队列里面的添加，删除，修改操作。
        这时应该注意锁的粒度。对于set类尽量使用分段锁，对于queue，可以采用乐观锁。这样可以减少阻塞的线程数。
          场景：对共享数据的修改就要涉及到single thread execution模式，该模式保证一瞬间只有一个线程可以执行完该方法。
            该对象的角色：
                    sharedresource 共享资源   safemethod unsafemethod 不安全方法需要使用锁来保护。
                还要思考该保护哪些字段，确定以后要看一看有没有漏的地方，以及用什么单位进行保护。
      3.guarded suspension:保护性暂停。  程序的执行需要根据某个状态来，状态不行时，就要保护性暂停，悬挂起来，等待可以了再次使用。
                          被守护方法执行时需要先满足守护条件。不满足则要悬挂起来，等待通知。代码逻辑为：
              while(守护条件的逻辑非)        
            {
              wait();
            }
            执行目标条件处理;
        此外还要有改变守护状态的方法。两个方法分开，就是生产者消费者模式，或者说生产者消费者模式本身就是guarded suspension模式。
        类对象组成：
                guardedObject{
              guardedMethod();
              stateChangingMethod();  
                }  有些时候两个方法是在一块的。即状态的改变和获取在一个方法内；
          guardedMethod(){
          while(守护条件的逻辑非){ wait()/Thread.yield();  这里可以处理成自旋，阻塞，yield三种，自旋不放弃锁，阻塞放弃锁进入阻塞态，yield放弃锁，但并不阻塞}
            process();
          stateChangeMethod();   //
          }
        guarded suspension可能比较耗费性能。balking是前者的高性能版本。
        4.Balking模式： balking模式相比于Guarded suspension，也存在守护条件，但是balking模式一但守护条件不成立，就进行中断处理，结束方法。因此实际应用中可以使用balking给guarded suspension进行兜底。
                代码模板：
                    if(守护条件的逻辑非) return ;
                      process();
        5.guarded timed: 介于balking,guarded suspension之间。再守护条件成立前先等待一段时间，到时候条件不成立，直接balking,比如redisson锁。在获取锁失败后，会先休眠一段时间。
        6.生产者，消费者模式：
                  由生产者，消费者，channel角色组成，channel需要提供wait,notify方法，channel本身必须是线程安全的，生产者和消费者以channel作为中介。当两者在处理速度上存在较大差异时，就引入channel进行平衡
        7.Thread-Per-Message模式： 某个线程将某个任务委托给另一个线程，扩展后就会形成forkandjoin,这样可以做到不影响主线程的性能。可以大大的提高并发量。 
                  模式成员：
                              Client: 比如主线程，发起请求。但是他不必知道Host如何处理请求。
                              Host: 调用request方法创建新的线程。在县城里使用Helper来处理请求。
                              Helper:在新的线程终，调用helper执行任务，Helper为Host提供请求处理的功能
                        关键在于Helper和host怎么连接。MVC里使用了hashmap以及线程池，kafka和redis则是固定下来。不进行连接，使用消息队列进行解耦。
                  感觉：比Thread-per-Message更高并发量的就是生产者消费者模式。可以在每秒接收更多的请求。而且更加稳定，kafka用的就是这种模式，redis用的也是这种模式。Thread-per-Meassage常见的使用就是
                    tomcat服务器，MVC框架。这也就是为什么，相比于redis,kafka这些使用了队列的服务器，并发量低的原因。
        8.Work Thread模式，也被称为线程池模式,没有工作就等，工作来了就干活。角色包括：
                    Client,Request,Channel,Worker Thread角色四个。核心就是调用和执行的分离；
        9.Future模式：
        10：active object模式：
        11.Master-Slave:前边的reactor,Thread-per-Message的场景是任务数目多，而Master-Slave则是任务很大，需要拆分成子任务进行；典型实现就是forkandjoin;
                    Master线程：（创建slave线程，提交任务时才执行这一步），将任务进行拆分，等待处理结果。返回处理结果。
                    Slave线程：内部通过BlockingQueue 保存task,这时候如果没有任务就会阻塞。常见的模式也可以采用forkandjion模式。
        Fork/Join:
        总结：并发量大的时候用Thread-per-message,很大时用消息队列，任务很大时可以用master-slave,具体实现为fork/join;。但是如果想实现任务编排，需要采用completableFuture;最好可以了解一下他们的原理。
        感悟：多线程真的是门学问呀！要好好学多线程。
        但感觉，多线程就是架构的基础，因此每天都要看多线程源码。然后记录，更新。
    感悟：要多加练习多线程题目，对自己爹项目帮助很大，还有，感觉还是得写一些项目。还有，不能再看消息队列了。要多看一些软的东西，设计模式，连多线程题目，看redis场景，看Jvm；可以多练习多线程。多线程其实就是场景题，记住，多线程的本质就是加锁。

    同步：亦称为直接制约关系，指的是为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调他们的工作次序而产生的制约关系。进程间的直接关系源于他们的相互合作。
    互斥：并发的进程需要共享一些系统资源，分为互斥共享和同时共享。前者成为临界资源。访问需要互斥的进行。
      互斥资源代码逻辑：
                        进入区(上锁)  临界区 退出区(解锁) 剩余区  进入和退出区负责实现互斥的代码段。如果进程不能进入临界区，该怎么处理？占有cpu,阻塞。
              空闲让进 忙则等待 有限等待 让权等待
    信号量：用于实现进程同步的最好措施。但如果各个进程间不是同步关系，仅仅是互斥关系，那就不能用信号量了。但有时候某些个线程在互斥的同时存在着一些同步关系。因此，锁分为两类，凡是需要进行线程同步的，都可以用信号量，减少资源消耗。否则就是互斥锁了。但有时会混杂着同步与互斥，这时互斥锁与信号量都需要。
          如果使用锁，就要满足 空闲让进，忙则等待，有限等待，让权等待。
    生产者消费者：  所有线程对消息队列的访问是互斥的。 但是必须生产者生产->(缓冲区不为空)消费者消费，消费者消费->(缓冲区没有满)生产者生产。因此要用到信号量，但也要用到锁。这就是ReentrantLock了;
                       
                      得到信号量许可  s.acquire(1)    错了，这样会死锁。当要加两个锁时，要格外注意死锁的问题。
                          进入缓冲区  lock.lock();
                      释放许可        x.release(1);
                  离开缓冲区          lock.unlock();
    读者写者问题：      不属于生产者消费者问题；
    多线程常见模式： 信号量，互斥锁。  同步机制，互斥机制， 生产者消费者模式， 多生产者多消费者模式， 吸烟者模式，读者写者模式，哲学家就餐模式。 中间两种都是信号量+互斥锁的操作方式，吸烟者是信号量的同步机制，互斥机制就是纯的互斥锁。读者写者模式也可以使用信号量。
        但要注意，一切多线程，都可以用信号量解决。但信号量可能存在代码拓展性差的问题。但如果不会，就用信号量，1的信号量就是互斥锁。


  //在redis里面，hash结构就是信号量的典型代表。
    消息队列的学习：暂时停止，学学技术就行了。感觉redis学的还是不行。并发量很大时才会用到消息队列；
  redis zset查询：zset,set本身也是hashmap,如果想优化查询，可以使用分段锁的机制。对于集合类，list类，如果想优化查询，提高并发量，可以将它们做成集群。

  可以把redis当作一个在内存的数据结构，去看待它。去思考怎么优化他。
    当前无法上手项目的原因就是redis不行，要学习redis的场景，页包括集群场景。
缓存：存放热点数据。
    缓存模式：
              旁路缓存： 更新后删除，适合数据一致性要求高或者缓存数据更新比较复杂的业务。
                        或者可以有一个专门的服务来更新缓存，其他服务查不到就返回，服务本身接收数据库的更新信息，然后更新缓存。但这样性能会有问题，而且数据一致性也会有问题。因此还是建议采用传统的旁路缓存模式；
              读穿透：写请求时，cache里没请求就直接写db,有就先写cache,再写db；读请求时，cache里没有就读db,然后更新缓存。
                      举例：一些粉丝较少且不活跃的用户发表微博后，Vector 服务会首先查询 Vector Cache，如果 cache 中没有该用户的 Outbox 记录，则不写该用户的 cache 数据，直接更新 DB 后就返回，只有 cache 中存在才会通过 CAS 指令进行更新。
              写穿透：与读穿透类似，区别是写穿透会异步的更新db，存在消息丢失的风险，在并发量很高时适合采用。
          从区别来看，关键是怎么处理写请求：安全，与效率选一个，  旁路缓存选择安全，有较好的数据一致性。写穿透选择效率，读穿透位于中间，cache里存在数据就更新，没有就直接更新db.
                                  怎么处理读请求，尤其是cache没数据时，旁路缓存使用分布式锁，加db更新。 读穿透与旁路缓存一致，写穿透也一致。
                                  可以说最主要的区别就是怎么处理写请求。旁路缓存最安全，效率最低，写穿透效率最高，但不安全。
    缓存层次：
            本地缓存：进程内缓存
            进程间缓存：与远程缓存一样，不过一个和业务进程在一个Host里，另一个在另一台机器上,如redis.pika
            远程缓存：如redis,pika.
    存储介质：
              内存型：redis,读写性能高，缓存数据少
              持久化型：Pika,缓存的内容多。读写性能低1~2个数量级。
  总结：可以看看pika.
缓存设计因素：
            1.缓存数据结构设计
            2.缓存分布式算法的设计：取模，哈希一致，
            3.分布式读写如何实施：代理模式，或者client直接读写
            4.数据如何迁移
在使用redis集群时，只需思考第一步，其余三个已经实现了。
缓存部署：
        1.核心，高并发访问的不同数据需要分拆到独立的缓存池里，防止相互影响。而访问量小的，非核心业务数据可以混存
        2.对海量数据，访问超过10~100万的数据，要考虑分层访问，建立多层次缓存。怎么建立多层次缓存呢？nginx,redis,lua，jvm这些都是分布式缓存。其实就是这几个角色.怎么去发http请求,lua请求.   
        3.对缓存集群管理。
总结：消息队列不能解决全部问题，只能是一个补充。所有的问题都要在redis解决。即redis写完就要可见。无论多么大的数据。可以将大数据拆分，hot key不同部署。
  缓存设计考虑因素：
        1.读写方式（使用hyperlog,bitmap进行大量统计），以此传输时是否要全部传输，还是部分传输。
        2. kvsize  大Key要分开存储（尤其是集合类型） ,只读hot key要分版本存储（比如微博的热搜。    ） 
        4.key的数量（少的时候直接全量存储数据库，多的时候只能存储热点数据），
        5.读写峰值（跟qps，来构建缓存结构，采用集群，主从，哨兵，多层次缓存等结构），
        6.命中率（保持较高的命中率）。
        7.过期策略。：设计较短的过期策略。过期策略是数据一致性，命中率的平衡。命中率背后是缓存穿透，雪崩，击穿。要详细的评估数据一致性和命中率哪一个更重要。根据这个来选择过期时间。如果会造成雪崩，击穿，穿透，就必须延长缓存时间。比如秒杀业务里设置逻辑过期。 否则就根据业务来。
        8.根据读写qps选择数据库： 5000以下：mysql 10w以下，单节点redis,主从模式， 10w以上，redis集群+本地缓存
        9.缓存命中率：持续对缓存命中率进行监控，及时进行故障转移，故障处理，以及多层次数据备份。  原因：命中率下降就会导致缓存穿透，缓存雪崩等情况的发生。实际中应该使用布隆过滤器，bitmap,redis,local cache进行多层次缓存处理，使用canal 更新缓存，减少无效请求。
        11.平均缓存穿透加载时间，过大时就要考虑如何加速了。毕竟缓存穿透对应的是hot key。
        12.缓存可运维性（伸缩扩容，转移数据，监控报警），
        13.缓存安全性（限制来源ip)
        redis的内存删除策略：redis会删除一些长时间没有被访问的键。redis还会定期的删除过期键。此外，客户端在更新缓存时需要使用分布式锁。分布式锁使用信号量实现。具体使用hash结构。
  缓存击穿：  对请求做严格的合法性校验，此外，使用布隆过滤器，bitmap,过滤无效请求。数据库里不存在时，就在缓存里设置一个过期时间较短的临时值。
  缓存穿透：redis里的热点缓存失效，大量请求打到数据库，可以使用分布式锁更新策略，以及逻辑过期策略。
  缓存雪崩：合理调整各个key的过期时间。过期时间=base时间+随机时间。
  缓存数据不一致解决方案：
      缓存更新失败时：重试，重试失败就将失败的key写入队列，缓存访问恢复后，将这些key删除。让key再次被查询时，从db加载。
                      或者，缓存时间适当缩短，让缓存数据及早过期。在重新从db加载
                      又或者，不采用rehash漂移策略。
  hot key:导致某个缓存节点出现过载，卡顿现象。大量的缓存会碰到网卡，带宽，cpu的上限。导致缓存访问变慢，卡顿。尤其是在集群模式下，会出现这种情况。引发hot key的业务场景如微博热搜，秒杀活动等。
          解决方案：
                  将热点key做分散处理，比如Key名为Hot,可以被分散成hotkey1,hotkey2,然后放在不同的机器里。客户端请求随机访问一个节点即可。 这只适合于只读的key,比如微博热搜。但是如果hot key是一个高并发写的key，比如秒杀场景，这种策略就会失效。这个时候，可以用消息队列，缓存请求。
                总结:hot key为只读key,不会被修改时：将hot key做成多个副本，分散在不同节点里；
                      hot key需要高并发写时：使用消息队列进行流浪削峰。
  BigKey:  大key查询慢，由于每一个Key都有过期时间，如果过期时间很短，大key就会被反复的创建与更新，反复访问数据库。导致查询变慢。
              此外，大key字段较多，每一处改变都会导致大key被删除，引起缓存失效。
          大key场景：微博需要保存每个用户的个人信息，这些加在一起就是一个大key,
          解决方案：将大key变为多个小key，分界点存储。同时设置较长的过期时间。
  场景题：66
  cap:只有cp,ap,cp的意思是，出现异常时，为了牺牲a,持续阻塞，向用户返回旧数据；ap是指，出现异常时，返回旧数据，体验感比较好。

感悟：果然，redis,和juc还是有好处的。每天都要看！都要做题，关于redis和数据库的，目前看redis主要是数据结构怎么用，多线程，要多多做题。学会灵活运用，以及看源码。
 redis网络模型：
          基于epoll的网络模型
 感悟：练习基于信号量的编程题目，还要想一想，java这些锁相比于信号量都做了哪些优化。还有，多看看Jvm的内容，要学会jvm调优。redis也要天天看，不想看分布式了，太多了，可以先看看基础的计算机知识，比如王道的考研课。看有没有分布式的东西。然后再看分布式的内容。

  感悟：核心就是mysql,redis,多线程编程，网络编程。    再加上jvm，
分布式技术：
      cap：p是存在的，c是强一致性，a是即时可用性，当下分布式系统要么是cp,要么是ap，在cap基础上提出了base，保证基本可用性，最终一致性，来实现一致性和可用性的平衡。
      基本可用性常见措施有：流量削峰，延迟响应，体验降级，过载保护。这四个措施可以保护系统的核心功能正常运行。
      为了实现cp,ap还出现了  Qurum： 系统一共N个副本，每次成功同步W个副本，系统每次读取R个副本，为了保证C，需要满足：N-W<R,这样每次读取时都能读取到最新的数据，只有N-W<R时，才代表
      写入成功，典型如Kafka,以及mysql主从模式下事务成功的标志是，至少一个从节点将binlog落到了磁盘，这样就可以保证数据库主节点挂掉后，从节点依然能够具有最新的 
      redis本质上是ap结构，无法提供一致性，毕竟只是强一致性，他的最终一致性也是要靠Mysql兜底的，因为redis的事务不保证acid，但是zookeeper,kafka都是可以的。因此如果想要高可用，也可以选择zookeeper,
 为了保证分布式事务，起码都是两阶段提交：
      prepare 事务信息 commit.  就和redolog一样，要有一个结束标志。
      两阶段提交：
                  1.leader接收到事务，将请求保证成Proposal请求，添加一个递增的事务id，通过消息队列广播给每一个follower,follower执行
                  请求，持久化到磁盘后，向leader返回ack,当leader收到超过半数的ack时，自己提交事务，并开始广播commit,follower收到commit后
                  完成事务提交；
          对比mysql:mysql只要收到至少一个节点relay log完成，就提交事务，也不通知。
      崩溃恢复：选举产生的leader会与过半的follower进行同步，使得数据一致，当与过半的机器同步完成后，就会退出恢复模式，进入消息广播模式。
juc:
    怎么加锁，使用信号量机制，首先分析程序中有哪些进程/线程，他们相互间的关系（只有同步，互斥，前继三个），确定信号量PV顺序，编写伪代码；
    锁该怎么设计：为了保证效率和公平性，还要注意锁的设计，公平锁，非公平锁，偏向锁，重量级锁，分段锁，细粒度锁，都会影响程序的性能，分布式锁还要考虑网络连接以及锁的失效和永不释放。

cap：每一次get不一定拿到最近一次put的值，
aqs详解：


当多个线程读取集合的时候，应该尽可能减少锁的粒度。例如mysql的间隙锁，记录锁；redis也可以直接分片。这样可以增加并发度。当集合非常大的时候就需要考虑分段锁。以及查询的性能。

目前的重点就是多线程，分布式，数据库，计算机基础。
gfs学习：

​	gfs由存储服务，客户端，网关组成。gfs架构中最大的特点是没有袁术服务器组件，这有助于提升整个烯烃的性能，可靠性和稳定性。元数据值文件的属性信息，理由，元数据服务器的单点故障率较高。一旦元数据服务器崩溃，即使节点的冗余能力再强，系统也不可用了。GFS通过扩展可以支持数PB存储以及处理数千客户端的请求。gfs借助tcp/ip或者infiniBandRdna网络将物理分散分布的存储资源汇聚在一起。统一提供存储服务。使用同意全局命名空间来管理数据。

​	采用弹性哈希算法在存储池里面定位数据，取代了传统的元数据服务器定位数据，可以真正的实现并行化访问。改善了单点故障和性能瓶颈。

​	高可用性，通过配置某些类型的数据卷，对文件进行自动复制，即使单个节点出现故障，也不影响数据的访问。当数据出现不一致的时候，自动修复功能能够把数据恢复到正确的状态，数据的修复一增量的方式在后台进行。GFS采用标准的磁盘文件系统存储文件，数据可以使用传统访问磁盘的方式被访问。

​	如何同时保证高可用高性能：使用数据卷进行自动复制，使用弹性hash自动映射，这样就可以做到真正的并行化访问了。

​	全局统一命名空间可以基于不同节点进行负载均衡，同时动态的扩展，收缩存储资源，大大提高了存取效率和系统的可用性。
	弹性卷管理，类似于redis里的hash slot，将数据存储在逻辑卷里，逻辑卷从逻辑存储池进行独立逻辑划分。逻辑存储池可以在线的增加和移除。不会导致业务中断，逻辑卷可以在线增长和缩减，并可以在多个节点复杂均衡。不同的是gfs的逻辑卷可以在线增加减少，此外采用hash算法并行化确定访问节点与访问。通过自动复制客服redis的非高可用性，实现消息的原子保留。
 	基于标准协议 可以支持http,ftp即cluster原生协议，现有应用程序不需要做任何修改就可以对gfs里的数据进行访问。也可以使用专用api访问。
  	这些的基础都是磁盘的顺序写。
   操作系统进行进程管理的银行家算法可以解决死锁的问题。但其实不太能用到现实中。
   为什么要保证基本可用，正是因为无法完全保证强一致性的同时保证高可用，才出现的基本可用，以及弱一致性。比如1s只有200qps，那当然可以做到强一致性和可用，也就是acid。几乎所有的互联网系统采用的都是最终一致性，只有在实在无法使用最终一致性，才使用强一致性或事务，比如，对于决定系统运行的敏感元数据，需要考虑采用强一致性，对于与钱有关的支付系统或金融系统的数据，需要考虑采用事务。所有都采用的是最终一致性，但是，怎么实现最终一致性？目前的做法只有轮询。强一致性属于不存在延迟的最终一致性。如果业务无法容忍短暂的延迟，那就是用强一致性，如果可以容忍（比如qq状态）那就用最终一致性；
   最终一致性的核心就是以什么为准？以最新写入的数据为准或者第一次写入的数据为准。
   数据的一致性指的是那些支持事务的数据库即本身在qps低的时候可以做到cap的数据库，而不是本身就是ap的数据库，他们怎么也做不到最终一致性的。
   对于mysql:不同的表最终实现一致性，该怎么做？启用后台线程进行轮询；
   定时任务的完成：怎么实现最终一致性。比如中间失败了，那就要重试，重试后还要通过幂等性来保证最终一致性，即幂等就是为了最终一致性而存在的。即最终只会有一个状态。幂等正是最终一致性程序中的一个必须措施。而布隆过滤器，bitmap,唯一索引，只是为了达到幂等性的工具，他们最终都是为了最终一致性而服务的。这就是通过锁机制来实现的。也就是兜底机制。
   感悟：其实一个项目，不同进程之间的项目，就是分布式系统，考虑到就好了。
   最终一致性适合于高性能，该并发场景，且具备可用性。
   实现最终一致性的措施：版本向量，分布式事务，事件日志，消息队列，时间戳，状态机复制，冲突解决策略，复制策略，弱一致性。
   最终一致性一般采用消息队列实现。消息队列顺序消费，但这样，不好吧。
   最终一致性定义：对于同一个数据对象，如果没有更多的更新产生，最终所有的访问都会返回最新更新的数据(版本)
   最终一致性最常规的实现方式：
   	R>N-W;比如5个副本，以此写入两个，那就要以此至少读4次，其中N>1,否则就无法实现最终一致性。因此为了实现最终一致性，需要写两次，而对于redis，就是消息队列。
    事务：N个副本要么全更新，要么全不更新。
    可以采用两阶段提交，或者三阶段提交，消息队列+补偿机制，异步回调。
    两阶段提交：以事务协调者发出的指令位区分，prepare就是第一阶段，global commit是第二阶段。
    a.准备阶段
    	1.事务协调者向参与者a,b发起事务预处理请求，
	2.a,b收到预处理请求后，开始执行，写redolog,unlog,刷盘,但之后并不会commit,而是返回给coordinator返回vote commit
 	3.事务协调者收到所有的vote commit后，就进入提交阶段，如果没有收到任何一个，就回滚所有的事务。
  b.提交阶段
   	1.如果所有参与者都返回vote commit,那么协调者向所有参与者发送全局提交确认通知(global commit)，参与者会完成自身事务的提交，然后回复ack,
    	2.如果有任何一个参与者返回失败，就回滚事务。
    因此redis丢了怎么办？那就是说，使用两个以上副本，一次至少写入两个，另外一个是消息队列，重新生成主节点的时候，从消息队列里拿到数据，进行更新。
    2pc存在的问题：
    	1.2pc属于cp类型，非常耗费性能，如果其中一个参与者通信超时，其他参与者都会阻塞的占用资源，这个资源或者是数据库连接，或者是其他。第二，2pc特别依赖与事务协调者，如果事务协调者出现单点故障，整个系统将无法提供服务，虽然可以重新选举事务协调者，但也会造成之前的事务参与者无法释放资源。第三，如果发送commit后出现了网络抖动，一部分参与者没有收到commit，自然也就不会提交事务，返回ack,那么整个数据库系统就会出现不一致，即不能保证最终一致性。
     三阶段提交：
     a.cancommit阶段 验证阶段
     	1.检测各个参与者网络通信健康程度，事务协调者向所有参与者发送cancommit,所有参与者返回yes后，才进入下一个阶段。如果任何一个参与者反馈No,整个分布式事务就会中断。
      b.precommit阶段 执行阶段，不commit
    	1.阶段一中，所有事务参与者返回yes时，就会进入precommit阶段，事务协调者向所有参与者发送precommit请求，参与者收到后开始进行事务操作，并将undolog,redolog记录到事务日志里，执行完后，不提交commit，而是向协调者返回ack.所有参与者都返回ack时，才进入下一阶段，如果任何一个不返回，就回滚整个事务，协调者向参与者发送abort请求。
     c.docommit commit阶段
     	1.协调者向所有参与者发送docommit请求，参与者收到请求后，执行commit操作，并向协调者返回ack消息。协调者收到所有的ack后，提交事务。如果有一个没有返回ack或者返回超时，协调者就会向所有参与者发送abort请求，
回滚整个事务。
      3pc设置了超时时间，解决了参与者长时间无法与协调者通信的情况下，无法释放资源的问题。但如果，参与者实际上提交了事务，但协调者回滚了其他地区的事务时，就会造成数据不一致。因此3pc是努力向cp，但实际也会导致数据不一致性。或者说，为了cp不仅仅是3pc这么简单。
      感受：2pc,3pc都是cp系统（强一致系统），不具备高可用性。
      TCC:
      分为try,confime,cancel。
      try阶段：对业务系统做检测，以及资源预留
      confirm:确认执行业务操作
      cancel:取消业务执行。
tcc开源框架由hmily,seta.
分布式事务执行注意：
	可以将分布式事务变为异步任务，先写一次数据库，保证数据库成功，之后采用异步任务执行框架，保证最终一致性。其实也类似于消息队列了。理解来是这样的：
原来：
 事务A：
 	子事务a...
  	子事务b...
   子事务c...
   事务完成...
现在：
	   完成事务A所需参数写入数据库，提交....   
	   异步线程a执行子事务a...
	   异步线程b执行子事务b...
	   异步线程c执行子事务c...
	   事务完成.
    异步线程执行时允许失败，可以多次重复执行，重试超过一定次数需要额外处理。这样就可以保证最终一致性了。数据可也可以使用消息队列来代替。
    第一步举例：举例比如将购物时支付成功后。将商品订单号，收货地址，收货人姓名以任务上下文context的形式写在数据库表的一个字段里，只写一次数据库，就代表执行成功了。
    一般情况下，异步事务已经足够解决所有问题了。
    因此无论是nacos，或者是别的，本质上我们都讨论的是他们是什么数据系统，cp,还是ap，或者base。
    此外当前一些系统，由于raft,paxos的使用，还出现了cp+ha的方案，比如kafka，可以保证完全满足cp，就是每一个Get，可以拿到最新一个put,但同时，也保证了高可用，但这只是针对一条消息而言，多事务仍要用base,此外。
	

感悟：今天看了看分布式，了解了分布式基本的一些知识，这对我构思项目的帮助是很大的！要持续搞懂分布式，多线程编程，以后才能走得更远。

计算机网络：看完小林coding里关于三握四挥的问题。
tcp可靠传输：接收方进程从缓冲区读出的字节流与发送方发出的字节流是完全一样的。
	如何确保可靠：校验 序号 确认 重传
		增加伪首部
		序号：seq
		确认：ack
	单一包的正确传输，不丢包。
滑动窗口实现流量控制：在通信过程中，接收方根据自己接收缓存的大小，动态的调整发送方的窗口大小，即接收窗口rwnd，发送方的发送窗口取接收窗口和拥塞窗口的最小值。滑动窗口可以一直向前发送，对于中间没有发送成功的数据包，使用超时重传机制来兜底。这样就可以一次发送多个数据包了。这样就是滑动窗口机制。这样接收方也可以少发些报文，而采用统一的格式发送。
拥塞控制：慢启动，慢恢复，动态的调整网速，在整个系统中进行平衡。
为什么会有应用层？不同的程序为了高效，安全需要不同的协议，因此还需要应用层的协议，比如服务之间要使用rpc协议，cs可以采用http协议，redis采用resp协议，都是为了程序更高效的进行数据处理与更新。协议大部分包括两部分：消息头，消息体。抓住这个进行设计，就可以大差不差。
应用层的协议开发，要以应用层的功能为依据，比如mysql，redis的应用层协议就是为了传输数据库语句以及修改结果的，内容都是字符串。而http则需要传输超文本，图片，流媒体视频，json等数据类型。rpc协议传输的是远程调用方法的参数，方法名等。要先明确功能要干什么，再来确定协议的报文，主体结构，最后在考虑额外的为了速度，安全来做的措施。这就是应用层协议开发的原则。
	规定报文的语法，报文里各个字段的详细描述，
	每个字段的语义，以及字段里信息的含义。
	进程何时，如何发送报文，即如何交给传输层，以及对报文响应的规则。
如果基于Java开始传输，就要使用netty了。作为网络通信的框架。
常见的应用层协议：
	文件传输的协议： ftp
	电子邮件协议：	smtp pop3
	虚拟终端：      http
	远程调用：	rpc
	音视频：..
	....
应用层常见使用模型：cs模型，p2p模型。
	服务器：1.永久提供服务。2.永久性访问地址/域名。目前大多采用永久域名，毕竟可以分散布置。
	客户端：1.只和服务器通信，使用服务器提供的服务。2.间歇性接入网络，3.可能使用动态ip地址。4.不与其他客户机直接通信。
应用层模型：cs,p2p
服务器网络模型：reactor,epoll,等。

p2p:
	1.不存在永远在线的主机.
	2.每个主机既可以提供服务，也可以请求服务.
	3.任意端节点可以直接通信.
	4.节点间歇性接入网络，可以改变ip地址.
	5.可扩展性好，主机接入的越多，功能越强大.
	6.网络健壮性强.
dns:	通过递归查询，进行查询。

感悟：学习已经全面铺开了，要先把操作系统放一放，理解网络和jvm，同时加强redis场景，数据库的复习，多线程的持续学习！

gfs将数据分为master,cs,master用于存储元数据信息，表的位置，查询时：gfs里面维持了两个相当重要的表，我么可以理解为redis里的hash slot，
	file name->array of chunk handles  handle->list of chunk servers  这样就得到了一个文件的位置 version number，primary,lease expiration,这些个chunks会有一个主节点。
	数据写入方式：主节点在磁盘上维护一个日志，每当数据发生变动时，就在日志上追加一个checkpoint,使用日志可以非常高效的向磁盘追加数据。
	主节点有时会将完整的系统状态保存到磁盘上，这一过程需要一定的时间，也就是rdb,

	cs：数以千计的客户端
	master：存在副本，主节点维护着从文件名到数据所在位置得映射，实现这种映射，需要两张映射表。 
	chunk server采用wal技术，每次请求过来都是先写日志，并标记checkpoint，等到因为故障恢复数据的时候，从最后一次checkpoint开始恢复数据就行了。

	paxos：提议者，接收者，学习者。提议者。
	
	分布式总结：看完不现实，每天看一两篇，剩下都要看jvm,计算机网络了，消息队列，数据库，多线程。毕竟这才是基础。今天看懂paxos算法吧。
	强一致性实现：	多数派，主从赋值，
		缺点：无法保证系统的正确性。比如说，不同的命令写入顺序不一致，也依然无法保证最终一致性。
	分布式，并发环境下的实现：paxos算法。basic paxos multi paxos , fast paxos
	因此，在分布式环境下，需要同时写入多次。对于kafka，因为只起到
		其实也就是应用层怎么保证安全，tcp是可以保证安全的，顶多泄露ip地址，这没什么，不开防火墙就行了。但是应用层会携带参数，就有问题了。


http协议：
	tcp可能出现的粘包，http擦用回车符，换行符，content-length作为边界。connection用于表示连接的情况，短链接，长连接，还是其他。根据connction，双方决定是断开连接还是继续链接，只要任意一端没有在connection里写出断连字段，tcp连接就将保持下去。
	content-type表示了数据的类型，accept用于表示可以接受的数据类型，content-type后还有字符集charset，如果是二进制格式，还有一个content-encoding 表示编码格式，如Gzip，请求端也有一个字段accept-encoding，可以接收的编码格式，
	http可以一次发送多个请求，然后分别回应。这里就是利用了tcp的滑动窗口，可以看到，要想高性能的发送数据，还得是tcp呀。tcp的核心就是基于字节流，因此还是要看tcp呀。尤其是利用到tcp的高级性能，毫无疑问网络协议，多线程，是分布式的基础，而那些算法，可以一点一点看。
	get语义：从服务器获取指定的资源。如静态文本，页面，图片视频。ascII字符，get请求的参数只能是ascII字符集，且浏览器对URL长度有限制。
	post:对指定的资源根据请求体的内容进行处理。post的数据一般写在body里，浏览器不会对body大小做出限制。
	幂等性：get方法是幂等的，post并不是。get请求可以被放到nginx代理商，用于缓存。
 http缓存技术：
	强制缓存：在请求头添加cache-control,expires字段，Cache-Control: max-age=30, must-revalidate\r\n 最大30s，到期后必须重新验证，可见，一个协议的多功能，就是取决于方法的多功能。
	协商缓存：if-modified-since ,l;ast-modified ；etag,in-none-match 
	servlet容器：解耦合。将服务器与执行的方法进行解耦合，可以看作中介者模式。中介者模式本身说的是各个同事相互调用，这里我们可以简单一些，就是一个同事要调用其他同事。在处理时呢，使用了策略模式。毕竟都是抽象的角度看，都是处理请求，但是他是依据请求方法，来调取服务，还是属于中介者模式，这样可以进一步的处理更多的请求，因此，多层的中介者，可以处理很多很多很丰富的请求。服务器只管调用，这里是通过反射来实现的。
	这里有个问题，为什么tomcat是非阻塞的，正是由于，tomcat使用的service方法将会异步的执行，所以才会是异步的，而异步使用方式，只有这一种。为了高效一些，tomcat使用了线程池，这样tomcat采用模板方法模式，将流程确定下来，在模板方法里提供可变性，而模板方法里，由于方法类型被限定死，可以加强程序的严谨性，如果不想，也可以使用反射自主调用，
	为什么不能使用线程池直接提交，因为方法类型被限定死了，而执行对象和执行方法是绑定的，因此不能直接提交到线程池，这里也就使用了中介者模式，也即是说，是servlet容器调用service方法，这样就根据方法请求，url路径确定了具体的servlet,然后就可以在map里找到这个servlet以及对应的方法，这也就是中介者模式以及模板方法模式，尤其是执行逻辑很多样，很多元时，可以使用中介者模式；本质上也算是模板方法模式。
	只不过这里，没有同事。该同事也不会在中介者里面罢了。这里的模板方法有两层，外层的总逻辑，以及servlet容器的service方法两个都是模板方法。
	此外，为了做一些统一的过滤，还采用了代理模式，考虑到代理的功能比较多样化，因此代理模式有使用了责任链模式。之所以可以这么做，都是因为模板方法得到缘故。实际上，应该尽可能把那些执行过程难以预测的方法，放在模板方法里，不要干预主逻辑的执行速度，这样才能尽可能提高并发量。也就是说，要把业务相关的方法都放在模板方法里，而且都要异步的执行，
	免得影响主逻辑，而正是基于此，才使用了线程池。异步可以让主线程的速度可以预测，可以估计。因为他的每一个操作的结果都是已知的。而出Bug的方法都放在另一个线程里。

	感受：每天都要读源码，看源码，了解这些设计思想。
	从上面我们可以看到，servlet要考虑两件事：接收连接，处理请求，这对应的着connector,与servlet容器Container,连接器负责对外交流，容器负责内部处理。在这里，是两层的消费者生产者。我们可以认为，流水线抽的越长，并行性就越高。因为他将本来同步执行的代码，不断地缩短与并行化。就可以极大的增加容器的处理速度。为什么会采用
	多生产者消费者，就是这样。流水线拉的越长，每一个线程可以执行的代码也就越短，并行化就越多。因此为了极大的加快生产速率。可以采用拉长生产线的方式。
	处理对外请求时，首先就要考虑网络Io模型，该使用哪种才尽可能加快速度。
	Connector的设计使用了外观模式，将具体处理逻辑封装在抽象接口里，实际上外观模式+模板方法模式可以实现很多元的程序。外观模式可以固定下来流程，将流程里的每个方法顾定成模板方法。通过实现不同的实现对象，实现代码的多功能性。

	不积跬步无以至千里。每天都要做。不能不坚持。
	感悟：别学分布式了，看看tomcat,kafka源码就行了，然后每天学学分布式，大头都在基础这。

	1.分析并发进程的关键活动，划定临界区
	2.分析不同进程之间的关系，分为同步，互斥，前驱。同步指的是两个进程为了共同完成某个任务，在某些位置上协调他们的工作次序而产生制约关系，而相互间属于合作关系。
		互斥：两个进程需要访问某个共享数据区；
		前驱：一个进程的运行需要另一个进程释放相关条件。
	分析哪些地方才要实现同步关系，可见同步关系就是一段代码一段代码的来看。
	对不同的临界资源要设置不同的互斥信号量。一个同步关系对应一个信号量。只有互斥才是一个信号量。同步机制就是一前一后关系，需要分析那些地方需要一前一后，然后就可以归纳同步关系了。



 jvm：
	不应该将大文件上传进堆里，尤其是martipartfile类型的文件。这应该由前端自己上传。此外，对于转码类的操作，也要慎重考虑。考虑并发度以及老年代的大小。大部分请求的都是会话级别的。
 
今日目标：系统总结各个垃圾回收算法。

计算机网络：安全，https,
并发编程：看源码，做多线程题
	
jvm:垃圾回收算法
	标记清除算法；
	1.标记：从根节点开始，所有被引用的对象在header里添加标记；
	2.清除，对堆空间进行线性遍历，对于没有标记的对象进行清除。
	缺点：gc的时候会导致整个程序停止，清理出来的空闲内存不连续，产生内存碎片。
	复制算法:
	效率非常高，年轻代就是采用了复制算法。但是老年代不能使用复制算法，因为大部分对象都会存活下来。
	标记压缩算法：应用于老年代，老年代的垃圾收集算法就是在标记压缩算法的基础上改进的。标记后，将对象复制到一端。

	虚拟机一般采用分代收集算法，即不同的代使用不同的垃圾收集算法，新生代使用复制算法，老年代使用标记整理算法。针对对象的生命周期，来选择垃圾收集。String适合垃圾整理算法。

	避免full gc:分代收集，增量收集，分区收集
	增量收集：让垃圾收集线程和应用线程交替的进行，这样每次只需要收集一小片区域的内存空间即可。
		缺点：线程切换和上下文切花的消耗，使得垃圾回收的总成本上升，造成系统吞吐量下降。服务器的核心标准延迟，吞吐量。
	分区算法：一般情况下，对空间越大，gc时间越长，因此可以将堆划分为若干个小块，每次回收若干个区间。
	分区算法和增量收集都是为了降低延迟。而非吞吐量。
垃圾回收器种类：一般一种垃圾收集器对应一中算法，一个代。
	并发：垃圾回收线程与用户线程并发的运行
	独占式：
		并行：多条垃圾回收线程并行的工作，用户线程处于等待状态
		串行：一条垃圾回收线程工作，用户线程处于等待状态。
	三个种类的垃圾回收器将会影响服务器的吞吐量，延迟。
Gc性能指标：
	吞吐量   	运行用户代码的时间/总运行时间
	暂停时间	执行垃圾收集时，程序的工作线程被暂停的时间。
	内存占用	java堆区所占的内存大小
	垃圾收集开销	吞吐量的补数（1-吞吐量）
	收集频率	相对于应用程序的执行，收集操作发生的频率。
	快速：		一个对象从诞生到被回收所经历的时间。
吞吐量和暂停时间是相互矛盾。核心指标就是吞吐量，内存占用，暂停时间。这三者会构成不可能三角，任意垃圾收集器只能满足两个。因此应该根据具体场景来选择垃圾收集器。
目前最重要的是暂停时间和吞吐量，其中暂停时间最重要。低暂停时间可以减少程序的卡顿现象。
标准：在最大吞吐量优先的情况下，降低停顿时间。
serial:	新生代 独占式 单线程 标记复制
	新生代采取标记复制算法，老年代采取标记整理算法（serial old）。属于独占式的垃圾收集器，stw;由于只有一个工作线程，适合于内存资源紧张的服务器。对于新生代内存很小的服务器，stw的时间很少，完全可以接受。
	因此serial适用于内存受限，新生代只有200M左右的服务器程序。很多Java桌面应用比较适合。但是一旦内存较大，就不能采用了。
ParNew:	新生代 独占式，多线程，标记复制
	serial的多线程版本，也是独占式的。用于新生代垃圾收集，可以与cms老年代收集器配合使用。在多核服务器里，性能优于parnew.
Parellel Scavenge: 新生代 吞吐量优先
	
serial old：老年代，标记整理，抢占式，单线程
Parallel Old:老年代，标记整理，抢占式，多线程 吞吐量优先

总结：多线程版本的都可以降低延迟。
G1：新生代，老年代都可以收集。


操作系统：文件系统
	文件系统和cpu,内存一样，也是需要管理的资源。

操作系统里的文件分为数据项文件和文本文件。比较常用的就是数据项文件。
	顺序文件分为链式存储，顺序存储。这里可以参考mysql。
文件存储：分为实际文件和索引文件，同时，再写入的时候，使用日志来实现快速写入。因此数据项文件基本组成就是 索引文件，日志文件，实际文件三类。日志文件可以再细化为 redolog，undolog。根据这三个部分
数据项文件才可以实现快速写入删除。而这也及时mysiam的做法。innodb实现了更厉害的B+树，可以方便范围查询。而单纯的hash索引文件，无法实现范围查询。而索引文件，也是分布式数据库需要考虑的。
比如redis本质上就是hash索引。innodb就是B+树查询；
数据库设计三要素：索引数据结构，日志写入写出。实际文件结构。这三者共同决定了数据库的性能，持久性。这里优化的瓶颈及时尽量减少随机Io,
对多个文件进行管理的时候，就需要元数据区了。即查找文件的时候，现根据元数据区，找到文件的路径，索引文件的路径。这个元数据区，也是一个索引文件。当我们设计分布式系统时，也要这么考虑。当然了，分布式系统还使用了hash一致性算法，就可以省略掉原空间了。
文件目录就是一个树形结构。但是这里还用了一个inode，文件目录记载的就是文件再inode的位置，inode里记载了文件的实际位置。inode是线性表，可以快速地增删改查，最好。
文件目录->inode里的fcb起始地址->得到文件的访问权限，真实物理地址，上一次修改时间等。
树形结构不便于实现文件的共享。因此提出了无环图目录结构。便于文件共享，有向无环图可以用不同的文件名实现文件的共享。文件内还实现了索引表，来方便获取部分文件。
因此，查询本身也是使用索引了。


Scalable I/O in Java
	1.将程序逻辑每一步写清楚。
	2.将一个完整的处理过程分解成一个个细小的任务
	3.每个任务执行相关的动作且不产生阻塞。
	4.在任务执行状态被触发时才执行，例如有连接到达时，才处理连接。   注：为了进一步提升性能，还可以再不同步骤之间引入缓冲区暂存数据以及负载均衡算法，缓冲区采用链表存储，
	这样可以进一步提高处理速度，可以说，最高效的网络处理模型也就是这样了。常见的就是kafka,redis,tomcat。同时优化底层io。这样就可以实现一个超高性能的网络服务器了。目前，唯一不懂得点就是
	多路复用，要好好理解。其实本质上，这个步骤也可以用于业务逻辑里。asynflow也是基于此来做的。最好的Io服务器也就是这样了。此外还可以考虑细粒度锁，forkjoin来加速处理。所以本质就是多线程。
	本质上分成了5步， accept select read process send，每一个对应一个线程，因为他们都有可能阻塞，导致对应的线程无法执行，期间还可以使用阻塞队列进行速度的缓冲。read,send都是线程池的实现。
	而process也是线程池的实现，selector也是多个selector,是复杂均衡的形式。accept是一个。另外，由于锁的缘故，就要看看他们怎么减少锁的开销。再kafka里，send线程每个都有一个独立的队列，这样可以
	减少锁带来的消耗。而redis里，采用批处理的形式，取消了锁的使用。此外还有tomcat,zookeeper,netty,我们看看他们都是怎么实现的
任务：看懂kafka,redis,zookeeper，netty，多线程的源码。了解性能差异

大概弄明白了epoll，所有的服务器还是要看read,proecss,send这三部是怎么处理的，尤其是process端。read，send端kafka采用了消费者模型，tomcat采用了异步，但并没与采用缓冲区来缓冲,并没有采用，redis则是一起发送，因此tomcat的性能
是最差的，就是因为没有采用缓冲区，redis是在主函数里面执行完所有的流程，但redis敢这么做的原因是他每一部都很快。所以不同的网络模型，优化的重点就是process本身，以及read,send的方式，如果在epoll_wait和三者间设置三个缓冲区，那么想能就会非常快。毕竟主要是网卡这部分很快。很多时候，process本身就可以添加缓冲区，
这样才能加快处理速度。


多线程要赶快练题了，不能只在这做信号量同步题。


秒杀系统：
	怎么做呢？首先想一想qps，针对qps解决问题，3000左右mysql, 10w以下redis, 10w以上，消息队列。就这么来。
	查询是请求级的，一个页面只请求一次。并发量大的抽奖，应该是等个几秒后统一处理，并发量小的时候，10w以下，就直接返回结果了。否则就处理一段时间。



什么是rpc调用：
	简单讲就是发送应用层协议，应用层接受协议执行，然后回包。不过还有一个情况，我们把参数传过去，在将函数名传过去，就直接调用了。这样其实比发送请求更简单，协议也非常简单。类似于 set x y; 此外还要针对cap做出设计，此外，服务通过服务中西来发现，注册，负载均衡。

redis 的proxy如何构建。
为了实现redis的高可用，可以用redis proxy对redis集群进行管控。比如主节点与从节点连接较差时主动切换，以及停止写入。实时反馈
感悟：无论是动态线程池，还是redis proxy，都是为了实现集群的容灾能力和高可用，高效性。因此当redis不行时，就再加一个中间层。
计算机网络中，代理是加快速度，提高可用性，安全性的关键技术。如消息队列，nginx,redisproxy都实现了高可用，高并发。
redis主从的解决主要还是靠外部监控来做一个复杂均衡。即先过它外部监控，再去服务器。比如使用netty来开发一个外部程序，来保证并发性，tomcat，springmvc本身其实是http服务器。因此不能用mvc.而是要考虑dubbo。
实际上从节点对于主节点发过来的命令并不会同步执行，而是要先执行别的任务，比如查询任务，其他同步命令，这种情况下就会造成主从不一致。这个时候就可以通过info replication命令，来查看主从节点的同步情况，然后决定请求可以发给哪个节点。
如果从库的进度差值大于预设的阈值，就不让该从库访问。实际上还可以通过info replication来监控复制情况，以决定要不要写入redis，如果此时主节点与各个节点都存在延迟过大的情况，那么就应该拒绝写入，防止丢数据。 
读取过期的数据：
	实际上，读从库的时候还可能会读到过期的数据。redis采用了过期删除的策略。在 3.2 版本后，Redis 做了改进，如果读取的数据已经过期了，从库虽然不会删除，但是会返回空值，这就避免了客户端读到过期数据。protected mode 
	protected-mode:是否允许远程访问，哨兵实例如果设置为no,则相互的哨兵无法沟通，即使redis挂掉了，也无法进行主从切换。
	bind:表示允许那些ip上的redis客户端访问，设置哨兵模式的时候，要填上其他哨兵实例的地址。
	cluster-node-timeout: 集群心跳时间。
感悟：实际上为了可以远程监控redis,还是应该使用元数据区域，将所有配置文件保存在元数据区，重启redis的时候直接重启即可，还可以统一在线修改配置文件。
感悟：redis需要根据内存的大小选择合适的键，尤其是批量保存大量类似的value时，要注意采用集合键，而不是使用string，使用了集合键，还要避免大key,热key,要对集合进行分片管理，防止数据倾斜。
比如玩家账号，如果有上万个玩家，那就应该使用hash结构，而不是string,同时还要分片存储在不同的节点上，然后根据分片原则访问不同的节点。最好是hash一致性算法。这里就可以在redis上再加一层代理，再代理层里计算出存储在哪个节点。
然后进行访问。其实也就是分库分表了。此外，涉及到排序的时候要用zset,涉及到交集，并集采用set,涉及到很多存储的时候使用hset，涉及到单个值的就是string,但一般情况下每个用户的信息都是相同的。所以可以使用hset进行统一管理。对于List,对象
大部分情况下可以用zset代替。而zset查询某对象在不在是O1级别的,插入删除是Ologn级别的但是string对象是可以单独设置过期时间的。集合对象由于存在更新，不太行。
感悟：实际上，我们思索数据结构的时候，考虑的还是时间复杂度和空间复杂度。在大数据量下，某个数据结构占据的内存空间大小，以及删除的大小。
redis的查询是o1级别的。增删改，list,set,hash,string都是o1级别的。zset是olgn级别的。根据这个进行思考。
此外list不支持范围查询，而zset可以。这就是区别，因此一旦涉及到排名的功能，都可以使用zset。
一旦只涉及到有/无，就可以使用set,bitmap.set表示需要存储信息，Bitmap不用，只是查看状态，比如签没签到
hash：分布式锁。
大数据统计：hyperlog.
大量使用string对象会导致元数据过多占用空间。
比如怎么进行限流？
限流本身就是生产者消费者模式,使用一个可以先进先出的集合保持即可。用什么呢？zset,list两个都行。但是zset可以自定义排序规则，所以更好。
一亿个key要统计，用什么集合。统计什么？所以就要深入了解各个数据结构了。
如何优化两阶段提交：我们可以认为一定数目的follower返回成功的标志就代表消息提交成功，而不要求所有节点。

感悟：每天都要先学redis,mysql,之后看多线程，然后看kafka,zookeeper,redis源码。这三个一定要弄懂！



每天早上起来 mysql,redis,多线程！然后是kafka,五点起来学习！




juc多线程模型：	
	
	多线程-进程，并发度不高，cpu调度的基本单位是进程，当一个线程阻塞的时候，无法切换到其他线程，但是线程管理的开销比较小。多线程也不能在多核处理机上运行，
	内核级线程：内核级线程由操作系统控制，当一个线程被阻塞以后，别的线程也可以继续进行。用户级线程是代码逻辑的载体，内核级线程是运行机会的载体。线程是cpu调度
	的基本单位。












Layouts:
怎么实现日志快速查找，实际上就是用内存映射，在缓存区进行内存操作。这样就非常的快。
wal技术可以保证写磁盘和写硬件一样快。因此日志的记录和输出才是最关键的。
Log4j
Log4j的使用需要appender，类似于handler,layouts等同于formatter,
handler可以根据channel的方向分为控制台，文件，网络分为三类，其中文件又细分为两类。因此handler总分为5类
日志系统应该搭配finally,代理模式使用。
日志级别：
  fatal:严重错误，造成系统崩溃并终止运行的信息，比如线程被销毁
  error:错误信息，不会影响系统运行
  warn:警告信息，可能会发生问题
  info:运行信息，如数据连接，网络连接，io等
  debug：记录程序的变量参数。

  trace:追踪信息，记录程序的所有流程信息


```

```

学Java虚拟机。学习zookeeper源码。理解分布式的第一步。
zookeeper的每个节点相比于redis存储了许多版本信息，这样他就可以在分布式的环境下进行读写了。当集群进行崩溃恢复的时候，可以利用这一点来做。每个节点的数据
是1mb，节点分为四大类。

[]
cZxid = 0x4
ctime = Sun Aug 04 17:40:32 CST 2024
mZxid = 0x6
mtime = Sun Aug 04 17:48:11 CST 2024
pZxid = 0x7
cversion = 2
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 3
numChildren = 0


发送分为5步：
      1.确认发送的topic是可用的，  需要获取metadata
      2.序列化record的key,value;
      3.通过负载均衡，获取partition。
      4.向缓冲区追加数据
      5.追加完后如果缓冲区达到了bacth.size大小，就唤醒sender线程
  可以看到这一步就是异步的,send只管把数据放到缓冲区就行，但也有一个问题，元数据是怎么获取的。
    1.通过waitOnMetadata()来获取元数据，确保topic可用
    2.序列化k,v。这一步可以自定义序列化实现。但kafka的序列化也可以了。这里如果选取了string的序列化器，就是用toString().getBytes(Charsets.UTF-8);
    3.获取partition,这里涉及到负载均衡机制，经过计算后，会确定消息将要发送到哪一个broker,也就是partition.  
    4.向accumulator写数据：
        accumulator的核心存储结构是 ConcurrentMap<TopicPartition,Deque<RecordeBatch>> 每一个TopicPartition会对应一个Deque，而且在写入的时候
      是向topicPartition对应的Deque里的RecordBatch来写数据的。然后一次发送一个RecordBatch.这里的好处在于。发送者和生产者就不会抢锁了。也就是说。RecordBatch
      数据不满时，消费者线程不启动，发送者一直写。满了的时候，唤醒消费者线程发送，因此两者可以减少等待时间。






kafka节点启动时，就会在Broker上注册一个临时节点，值为自己的Id，路径为 /brokers/ids并watch/brokers/ids,当节点出现新的broker时，broker就会获得通知。
在broker出现停机，网络分区或长时间垃圾回收停顿时，Broker会从zookeeper上断开连接。此时Broker上的临时节点自动从zookeeper移除，监听Broker列表的kafka组件会被告知broker被移除
控制器：第一个启动broker会成为控制器，coordinator.并在zookeeper里创建一个临时节点，路径是/controller.其他broker注册的时候也会尝试创建/controller节点。但是会失败，并监听
该节点。当该节点与zookeeper断开连接时，临时节点就会消失。其他broker就会得到消息，并重新注册/controller。每个控制器通过zookeeper的条件递增获得一个全新的数值更大的
controller_epoch，其他broker直到当前epoch后，会拒绝旧的epoch消息。
当控制器发现broker离开集群时，他就需要给失去leader的partition选择一个新的leader,控制器会遍历这些follower,选择一个新的leader。然后向所有包含新首领或跟随者的broker发送请求；
该请求消息包含了谁是新首领以及谁是follower的信息。然后，新首领开始处理来自生产者和消费者的请求。follower开始同步消息。
因此Kafka的架构是通过zookeeper作为元数据区。通过controller作为协调者。控制器使用epoch来避免脑裂。即两个节点同时认为自己是当前的控制器。可以看到这里各个节点都使用了集群，来保证个整个
系统的可用性。controller通过zookeeper来获取每个broker的信息。并与之通信，确定offset，根据offset来选取leader。
以上也就是Kafka的管理机制
kafka本身将自己定义为分布式的，可分区的可复制的提交日志服务。保证kafka的持久性和可用性。

folloewer:follower同步消息时是顺序的，也就是在2没有同步前不会同步3，leader也会根据follower的同步偏移量来得知复制进度。如果follower在10s内没有请求任何消息
或者10s内没有请求最新数据，follower就会认为是不同步的，在重新选举时就不会包含全部的消息。 只有同步副本才有可能被选举为新的首领。此外控制器通过监听每一个broker在zookeeper上注册的节点，当broker崩溃宕机时，
他们就会与zookeeper断连，协调者就会得到消息。对broker相关的Broker发送消息。

kafka处理请求时主要处理客户端，分区副本，控制器的请求。kafka提出的基于tcpip的协议确保了如何处理请求以及响应。
所有的请求消息都包含了一个消息头。
kafka broker工作原理：
	在每一个监听端口上运行一个acceptor线程，accepter线程 负责建立连接，然后将连接交给processor线程，该线程读取网络请求并交给请求队列。之后从自己的响应队列里获取请求发给客户端。这里使用了多个生产者消费者模型，以及Io多路复用
尽可能保证了并发量。可以看到总体上分为acceptor线程
	这里为什么是每一个端口一个acceptor线程，因为broker需要监听客户端，以及其他broker的请求以及zookeeper的请求。因此是多个acceptor。其实相应的。他们不会在一个流程里。防止相互影响。
	
	常见请求：
	生产请求，获取请求。这两个请求都需要从leader里获取。因此在获取请求前先要知道leader,那么就需要先发送元数据请求。服务器端的响应消息里指明了这些主题
	所包含的分区以及每个分区有哪些副本。这些元数据都存储在zookeeper里面。元数据请求可以发给任何一个Broker，因为每一个Broker里都存储了这些信息；
	而此外，客户端需要定时发送元数据请求。来更新元数据。如果客户端使用了旧的元数据请求，就会收到非首领错误。客户端就是主动发送元数据请求更新元数据；之后
	再次发送请求。
	生产请求：
		ack,参数，kafka并不会立刻将消息刷到磁盘，而是通过复制功能来保证消息的持久性。就是说kafka的持久性是通过多个Page cache来保证的。并不是说非要落磁盘；
	如果ack=all, 请求会保存在一个被称为炼狱的缓冲区里，直到所有副本都同步完成，才会返回响应。
	此外，ack=all时，kafka需要确保确实有足够的副本。副本数量不足时也会拒绝写入。而kafka只会对已经回复ack的消息保证持久化。因此ack这个参数相当重要。
	获取请求：
		客户端发送请求向broker请求主题分区里具有特定偏移量的消息。客户端可以指定broker最多可以从一个分区里返回多少数据。因为客户端需要为这些数据分配足够多的内存。也就是说。这个是在消费者端进行配置的。
	如果客户端请求的时已经被删除的数据，将返回错误。如果存在，broker按照客户端指定的数量上限从分区里读取消息。再把消息返回给客户端。kafka使用零拷贝技术向客户端发送消息，kafka直接将文件缓存里的消息发送到网络通道里。避免了字节复制。
	核心：
		kafka之所以高效的原因正是在此：
			1.发送消息时，kafka通过多副本机制，在不写到硬盘的前提下，即保证了可用性又保证了可靠。
			2.再发送消息时，kafka将消息从磁盘里放到文件缓存里，通过零拷贝的方式拷贝数据，加快了速度。
		以上两点彻底避免了随机读写带来的随机Io,以及复制带来的速度上的损耗。又通过副本机制保证消息的可靠性。因此是最高效的！
			如果follower无法与leader连接怎么办？如果某个follower无法与leader连接，超过10s便会被踢出isr。这样就可以保证可用性。只有follower重新追上Leader后，才会加入isr.
	客户端只能读取已经写入所有同步副本的消息。分区首领知道每个消息会被复制到哪个副本上，在消息没有被写入到所有同步副本前，是不会发送给消费者的。
kafka的协议是二进制协议。这也确保了kafka可以跨平台，跨语言。

kafka的其他请求：
	主题创建请求， 这个请求会直接更新zookeeper里的主题列表。broker会监听这些列表，当有新的主题加入时他们会得到通知。
	偏移量请求：之前偏移量保存在zookeeper里，消费者在启动的时候，需要先查询zookeeper来获得偏移量，而现在偏移量都保存在kafka的broker里
感悟：请求的处理是kafka高性能高可用的关键，kafka通过复制，写pageCache保证生产者的告诉写，通过零拷贝保证消费者的高速读。

分区：同一个topic的同一个partition只能被同一个消费者组的一个消费者消费。
分区分配:
	假设在6台broker上创建一个10分区的主题，复制系数为3，意思就是每一个Partition都会对应着3个副本，一个Leader,两个follower.kafka则会有30个分区副本。kafka会保证每个分区的副本不会在同一个
broker上。
	选择好broker后，就需要决定分区该使用哪个目录。规则是：新的分区总是被添加到数量最小的那个目录里。这里是为了解决负载不均衡的问题。这里的好处是，不同的分区挂载到不同的磁盘上，还可以减少随机读写。
	提高读写效率。这些目录需要尽可能挂载到不同的磁盘下。

保留数据：
	kafka会给每个主题配置数据保留期限，规定数据被删除之前可以保存多长时间。或者清理数据前可以保留的数据量大小。这是因为在磁盘里寻找数据是非常耗时的，
	kafka将分区分成若干个片段，每个片段包含1GB或者1周的数据，kafka往片段里写数据时，如果达到片段上限，就会再开一个片段。此外正在写入的片段一定不会被删除。即使里面的数据过期了。
	broker会给每个片段打开一个句柄。如果句柄过多，操作系统会进行调优。
文件格式：
	消息的格式，包括若干条。主要就是在消息前声明消息的长度。以及压缩算法，时间戳，偏移量。其中偏移量可以方便后续消费者消费完成后提交偏移量。而偏移量也是broker构造的。此外
	消息也可以批量发送。
	偏移量 魔数 压缩和解压 时间戳 键的大小 键 值的大小 值 
索引：
	kafka维护了索引文件，方便快速定位偏移量。kafka给每个分区都维护了一个索引。索引也会分成片段，在删除消息的时候，会删除相应的索引。
清理：
	kafka里存在清理线程。
	
可靠的消息传递：	
	follower需要保证过去10s内从leader同步消息，与zookeeper之间有一个活跃的会话。即过去6s内向zookeeper发送过心跳。过去10s内从首领那获取过最新的消息。
follower不满足以上任何一点，就会从isr移除。
	一般来讲网络并不是问题，造成不同步的原因往往是jvm发生了垃圾回收。


broker配置：三个参数会影响数据的可靠性
	1.复制系数：default.replication.factor 副本个数，3代表一共三个副本，1个leader,2个follower.
	2.不完全的首领选举： unclean.leader.election 只能在broker级别进行配置。默认是true.
	3.最少同步副本：min.insync.replicas 比如值为2 就代表着isr里至少存在2个副本，才能执行写请求。否则就会报异常。
生产者：
	1.ack 设置为all
	2.重试参数
	3.额外错误处理，即回调接口
消费者：

tcp/ip:
	tcp在发送的时候会设置一个定时器，如果定时器溢出还是没有收到确认，就会重传数据。
	
tcp是全双工的，但是在每一个传输过程中，是半双工的。既一方发送，一方回复ack，这是通过应用层来表示的。应用层总要知道全部内容再去回复。因此tcp保证一个连接是全双工的，但是每一次是半双工的。既一方发送，以方发ack.除了连接建立和结束时。双方会主动通信。
这有什么区别呢，因为ack报文不需要重发，而另一个报文则需要定时重发。ack丢了会触发对方的定时重发，自己不用重发ack.

kafka作为数据管道：
mysql连接池：
	mysql连接池其实是线程池，并不是说socket放在里面，而是执行mysql的线程，mysql会给每一个连接的客户端创建一个连接，将该连接放进一个线程进行执行。只有当连接断开后，线程才会被放到线程池也就是连接池里面。
因此如果不断开连接也是不会放回线程池的。为了防止长期站着影响数据库性能，mybatis等框架也规定了超时时间。
mysql的连接分为长连接和短连接，长连接在操作完毕后，不会关掉，而短连接操作完成后会立马关闭。但是长连接超过一定时间后，也会断开连接。这个时间默认是8h
	show global variables like 'wait_timeout'; -- 非交互式超时时间，如JDBC 程序
	show global variables like 'interactive_timeout'; -- 交互式超时时间，如数据库工具
这里的问题就在于mysql没有使用io多路复用。mysql使用的是bio模式，因此无法处理高并发请求。此外，每个线程内部都有本地内存：包括：
	thread_stack：线程堆栈，主要用于暂时存储运行的SQL语句及运算数据，和Java虚拟机栈类似。
	sort_buffer：排序缓冲区，执行排序SQL时，用于存放排序后数据的临时缓冲区。
	join_buffer：连接缓冲区，做连表查询时，存放符合连表查询条件的数据临时缓冲区。
	read_buffer：顺序读缓冲区，MySQL磁盘IO一次读一页数据，这个是顺序IO的数据临时缓冲区。
	read_rnd_buffer：随机读缓冲区，当基于无序字段查询数据时，这里存放随机读到的数据。
	net_buffer：网络连接缓冲区，这里主要是存放当前线程对应的客户端连接信息。
	tmp_table：内存临时表，当SQL中用到了临时表时，这里存放临时表的结构及数据。
	bulk_insert_buffer：MyISAM批量插入缓冲区，批量insert时，存放临时数据的缓冲区。
	bin_log_buffer：bin-log日志缓冲区，《日志篇》提到过的，bin-log的缓冲区被设计在工作线程的本

mysql在启动时，会将当前库里已存在的索引的根节点放入内存缓冲区里，因为索引的根节点只有16Kb,只占据一个页。就算当前库里创建了1000个索引，也不过占据了15mb的空间。
mysql会有一个index page专门存储索引页。而不是和其他数据页放在一块。
mysql里的日志缓冲区是为了优化写，写数据时先写到日志缓冲区，然后刷盘。

Raft算法：
	每个节点等待领导者心跳信息的超时时间是随机的。超时后会发生领导者选举。但其实，redis的实现里还包含了下线投票这一步。
基于消息补偿的最终一致性：
	每一个分支事务执行完成后，会将任务上下文放到本地消息表以及消息队列里。将本地消息表的状态设置为未完成。下游服务收到消息后进行处理；
	处理完成后通过消息队列向上游返回处理结果。上游的异步线程接收到数据后更新人物的状态为已经完成，此时将会删除对应的上下文。
	优点：相比于单纯的消息队列，存在重试机制。并且将分布式事务拆分成了本地事务。异步事务。
2pc,3pc,tcc都是cp模型，实际不可用。
提高并行的策略：
	对于实际开发，更多的是基于消息队列，数据库，将process过程拆解成并发的或者基于消息队列拆解成并行的任务来处理。这样效率更高。但是要注意事务机制。
	将大事务拆解成小事务时更要增加兜底机制。以及确保acid.
分布式处理策略: 高性能的前提：弄明白事务，确保只要做到哪一步就可以保证事务就行了。比如mysql保证了binlog写好就能保证事务。实际开发中也要思考，事务的最短点是哪个。只要做到哪一步就一定可以确保事务成功。典型做法就是redolog,undolog。
	重试：重试时要确保幂等性；
	限流：限制访问。也是保证基本可用性的体现。
	降级：某些功能进行降级
	异地多活：使用dns域名解析，分配不同的ip给用户。保证复杂均衡；
	异步，以来组件就是消息队列，但消息队列的特点是无法重复消费，或者说重复消费影响很大，这里可以使用本地消息表进行状态更新。同时将消费者消费事务与返回消息队列看作是一个事务，同时成功，同时失败。
反应式编程：
限流算法：
	滑动窗口：维持一个队列，不断地执行pop,push操作，维持队列元素的数目在固定值。
	固定窗口：
	漏斗限流：类似于消息队列。每次弹出来固定个。可以确保系统应对突发流量，确保系统在稳定的负载下进行。
保证可用性的前提就是限流，因此限流是十分必要的。需要既保证服务可靠性，又保证服务质量。
限流的另一种做法：分布式限流。


项目：
	定时任务，不能使用消息队列，消息队列延时很高。消息队列是用来保证异步的，不是做定时任务的。
	定时任务数量很大：使用分布式定时任务时，会存在延时，且查询也需要网络开销。当任务量很大的时候，也需要多个线程。但如果任务量特别大
	比如1w个任务，又该怎么办。显然，不能用一两个线程池来解决问题，而是应该布置多个服务，来解决问题。此外，要有一个数据存储系统可以支持
	范围查询，等值查询，同时还要满足高性能，毫无疑问，redis。
	利用redis的zset，同时分片查询。就可以保证了。此外，任务表还要做到冷热数据区分，缓存里存放的应该是最近需要执行的任务。
	查询任务  执行任务  解耦  其中查询任务分为 唤醒 查询任务
   任务异常处理：
	任务失败了需要保证重试。
	常见异常：
		任务触发阶段异常：既业务方提交业务时异常
		任务流转异常：	数据库异常，中间件存取异常。	
		新建，激活接口异常：	
	解决机制：
		重试，失败兜底 ， 失败报警，人工干预
我们需要三个线程池，并行的解决问题。
此外这个项目是一个拉模式。主动地去轮询，而不是等数据库主动的推过来。因此为了防止没有被执行，要反复的拉取任务。但是也要防止任务重复执行和不被执行。对于重复执行，可以设置一个分布式锁；
对于没有执行。兜底机制就是，每次执行前，都会执行上一个分片的任务，执行完成后拉取本分片的任务；这个到时候看看源码。然后想一想自己能不能改一下。重复执行的任务可以通过幂等性来解决。怎么做呢？信号量。只有投放任务了，才能消费该任务。否则不能。

我觉得可以把我做的这几个都整合到一块进行。比如整合

感悟：今天听懂了核心流程，以及托底机制。晚上可以学一学操作系统。和计算机网络以及Kafka。以后每天下午实操。早上晚上看知识点。
操作系统：
	kill -l
1~31是普通信号，34~62是实时信号。
action指的是信号被发送到晋城市，默认操作系统采取的动作。 term表示结束进程，core表示终止进程，生成核心转储文件，ign忽略信号。stop停止进程执行。
cont表示继续执行被停止的进程。信号是可以被注册的。但是9，19信号是不可以修改的。因为进程终止和停止的权力必须有操作系统掌握。

操作系统内置了函数，可以进行信号的处理：
	signal(int signum,sighandler_t handler) ;比如：
	void fun(int sig) {cout<<"get sig"<<sig<<endl; }
	int main(){
	signal(1,fun);
	while(1)
		{
		cout<<"hello"<<endl;
		sleep(1);
		}
	}

而实际上，信号可以用来模拟中断行为。信号的产生方式有终端按键产生，系统调用产生，硬件异常产生，软件条件产生。实际上，再linux操作系统里，
操作系统提供了系统接口，来实现进程之间的传递信号，同时由进程自己指定收到信号后的执行函数。信号函数执行后，主函数会从信号函数下继续执行。
此外，空指针异常时，MMU内存管理单元也会产生信号。软件一场也会产生信号。不过，这些信号的值都是固定的。因此，对于socket产生的信号，进程也可以进行处理。

实际上到这里我也明白了，进程之间的随机通信是需要用到信号的。这样也就可以很好的解释Io多路复用，信号模型，异步Io的形式了。这里很奇怪。这三种其实没什么区别。
实际上信号驱动io就是单纯的用信号驱动函数，在信号驱动函数里进行读写。读写后，进程继续操作。而异步Io则是将数据读取好后在使用信号量，找到回调函数。进行处理。4，5其实很类似；
但是这两种不如io多路复用简单。io多路复用是阻塞的。并且。后两种不太可控。

scala可以随时随地的定义函数，当然java也可以。通过匿名内部类；
	kafka里每一个分区对应一个Log对象，每一个Log对象对应于文件里夹里的一个目录，Log对象由LogSegment对象组成。LogSegment对象由log,index,timeindex,txnindex四个文件组成。那么每一个都对应一个文件地址。
	实际上我也认为，java里缺少对于文件的抽象。
	LogSegment是保存文日志的最小载体。
那么kafka如何进行日志写呢？

日志对象Log:
	LogAppendInfo:保存了一组带写入消息的各种元数据信息。比如第一条消息，最后一条消息的位移值。
	LogAppendInfo（O）:LogAppendInfo对应的工厂方法，用于创建特定的LogAppendInfo实例
	RollParams:定义日志段是否切分的数据结构。
	LogMetricNames：定义了Log对象的监控指标。
	LogOffsetSnapshot：封装分区所有位移元数据的容器类。
	LogReadInfo：封装读取日志返回的数据及其元数据。
	CompletedTxn：记录已完成事务的元数据，主要用于构建事务索引。
感觉：读一读网络部分收发消息怎么做的就行了。broker端的那些，先不看了。没啥用。
然后看一看zookeeper的源码；
然后感觉自己需要一个综合性的后端分布式项目，业务方面不要太多，就是用kafka，redis,zookeeper,mysql，多线程来做。别涉及其他的中间件。主要练的就是高性能处理消息。分布式事务。

信号是一种简单的，轻量级的进程间通信方式，重量级的就是socket。linux支持33种不同的实时信号，这些信号的编号为32~64，实时信号由应用程序定义。

jvm:
	直接内存：可以直接使用jvm进行管控，可以使用参数来限制： -XX:MaxDirectMemorySize 来控制它的大小。
	本地内存：native函数使用的是本地内存，jvm无法限制；
gcroots:
	gc的速度与堆的大小无关，只与对内存活对象的多少有关。
gcroots包括：Java县城里所有被调用的方法的引用类型参数，局部变量，临时值，静态变量，常量池里的引用类型常量，入口说白了就是线程，静态变量，JNI引用。
G1的配置参数：
	MaxGCPauseMills:设置最大停顿的预定目标；
	G1HeapRegionSize:设置小堆区的大小
	InitiatingHeapOccupancyPercent:整个堆内存使用达到一定比例，并发标记阶段就会启动。
感觉jvm最重要的就是把G1搞明白：
	垃圾回收器组合：Serial Serial Old    ParNew CMS           Parallel Scavenge  Parallel Old   G1
	第一个组合：Serial:穿行回收年轻代，使用复制算法。  在但吞吐量下性能非常出色。多cpu下吞吐量不如其他垃圾回收器。。 适用于cpu较少，内存较小的情况。
			Serial Old:单线程针对老年代的回收。    采用标记整理算法。既一个区里解决问题；
		开启参数：-XX:+UserSerialGC
	第二个组合：ParNew CMS
		ParNew:   多线程进行回收，回收年轻代。       缺点就是回收期间用户线程被暂停。其停顿时间和吞吐量都不行
			参数：-XX:+UseParNewGC  新生代使用ParNew,老年代使用SerialOld.  官方不太建议使用。
		CMS: 关注的是系统的暂停时间。既低延时。它允许用户线程和垃圾回收线程并发执行。
			参数：XX:+UseConcMarkSweepGC;    采用标记清除算法。延时低，但是会出现内存碎片，退化问题，以及浮动垃圾问题。  
	第三个组合： PS PO
		PS:默认年轻代垃圾回收器。多线程并行回收，关注的是系统的吞吐量。可以自动调整堆内存的大小。对于吞吐量来说，可能会导致暂停时间过长。适用于后台任务。
		比如大数据处理，大文件导出/
		PO: 同样也是多线程垃圾回收器。采用标记整理算法。暂停时间比较长。
		jdk默认ps+po。
		ps是唯一一个可以自动调整堆内存大小的垃圾回收器。
实战：jvm调优
	内存泄漏：某对象已经不被使用，但仍然在gcroots上，就是内存泄漏；工作中可以自己主动的销毁这些对象。
	常见内存泄漏：
			查完数据库的pojo对象，可能会引起内存泄露。

项目：
	1.用户添加标签，标签的分类
	2.主动搜索.
	3.组队
	4.允许用户修改标签
	5.推荐：相似度计算算法+本地分布式计算。
	因此标签该怎么管理？	因此标签表，应该是这样的。此外标签表还应该。有相关的统计。	
	计算机类————	java,cpp,python.  
	还有，如何根据标签大家进行相互聊天。大家在一个标签里进行相互沟通，这里可以使用pub sub.推模式，拉模式。允许用户修改标签。
	感觉比较类似于百度贴吧。
	这里会有高并发吗？我倒是觉得可以根据这个做一个消息论坛。来推送这个东西。
	感觉这个还挺好扩展的。看看怎么做吧。
	本质上，若干个人可以进行组队。感觉很像虎扑呀。每个人都有一个标签。标签会存在里面。然后如果有个标签变成热度很高的标签。需要进行扩散处理。这里可能会用到redis集群。
	我觉得。这里的关键就是和redis的交互。这个该怎么处理。
	这里我想实现一个目标，对于redis的高并发读写。对于热帖的处理。对于大key的存储。
产品：
	0.高并发读写。我们该怎么处理这个视频
	1.首页：后端在用户登陆的时候，从用户订阅的标签里，以及热帖种随机选取最好的几个。推送给用户。
	2.用户的扫描页。
	3.用户在写的时候要考虑到分库分表。写完以后呢。可以从redis里查询。写完数据库以后，就要进行查询了。
	怎么发帖子？帖子可以这么做：用户发布帖子，发布后进行审核，然后发布到标签里。为了速度。可以先进行一个粗粒度的发布。然后进行进一步的审核。
	用户组队后，相互之间进行通信。有的时候，
	三大功能：
		标签还有一个感觉，怎么组织标签。这个基本上是一种树型结构。一级标签，二级标签，多级标签。这些标签应该使用一致性hash，这样就知道他们在哪了。
		组队，加入小组。给自己加标签，修改自己的标签。 根据标签进行搜索。进行组队。
		还有，标签的存放应该是数据库字段。这里需要使用json的方式。	
		此外，这里该怎么搜索。改以什么样的方式进行搜索，那就是redis的set并集
		但同样有个问题：分库分表怎么办。那就是用es了，这种搜索，redis做不到。
		发帖子，看帖子，给帖子评论，相互之间进行评论。 redis解决问题。高并发读写，大key，热key.
		相互发消息。
	//根据标签搜索用户。redis,like,正则表达式。
		还是用redis吧，redis的可靠性怎么保证。丢数据了怎么办。比如同步过去了。但是没有了。反熵，命令追加。谣言。/wral, 目前来看都不行。因为redis是一个ap系统，不能保证。因此必须提供数据库兜底机制
	那么数据库怎么查标签。
		很简单，求并集。求交集。而不是一次查询。我还是觉得应该放到另外一个表里，做关联查询。
		查询该怎么查询呢？redis可以做成cp的。无非是加一个代理层而已。一次写N个，大于N/2+1就算写入成功。但是选举也需要自己来选了。这个时候可以借鉴kafka，使用zookeeper;
		每个中间件维持一个元数据区。客户端查询的时候，到达每一个，都会转发给领导者，领导者来协调请求。这样就降低效率了。但是呢，总比redis那样要好，毕竟redis并不可靠。或者不应该把redis看做cp数据库；
晚上看一看zookeeper源码：
感觉还是看一看redis的源码吧，太难了。
	
